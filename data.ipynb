{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from argparse import ArgumentParser\n",
    "from typing import Dict, List, Tuple, Union\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from atom3d.datasets import deserialize\n",
    "from Bio.PDB import PDBParser\n",
    "import re\n",
    "import pickle\n",
    "import io\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import lmdb\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import openbabel\n",
    "from openbabel import pybel\n",
    "\n",
    "from utils import MyData, prot_graph_transform, hetero_graph_transform\n",
    "\n",
    "pybel.ob.obErrorLog.SetOutputLevel(0)\n",
    "atomic_num_dict = lambda x: {1: 'H', 2: 'HE', 3: 'LI', 4: 'BE', 5: 'B', 6: 'C', 7: 'N', 8: 'O', 9: 'F', 11: 'NA',\n",
    "                   15: 'P', 16: 'S', 17: 'Cl', 20:'Ca', 25: 'MN', 26: 'FE', 30: 'ZN', 35: 'Br', 53: 'I', 80: 'Hg'}.get(x, 'Others')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class GNNTransformAffinity(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cutoff: float = 4.5,\n",
    "        remove_hydrogens: bool = True,\n",
    "        max_num_neighbors: int = 32,\n",
    "        supernode: bool = False,\n",
    "        offset_strategy: int = 0,\n",
    "        task='affinity', #lba/ppi\n",
    "        gearnet=False\n",
    "    ):\n",
    "        self.cutoff = cutoff\n",
    "        self.remove_hydrogens = remove_hydrogens\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        self.supernode = supernode\n",
    "        self.offset_strategy = offset_strategy\n",
    "        self.task = task\n",
    "        self.gearnet = gearnet\n",
    "\n",
    "    def __call__(self, item: Dict) -> MyData:\n",
    "        # print(\"Using Transform Affinity\")\n",
    "        ligand_df = item[\"atoms_ligand\"]\n",
    "        protein_df = item[\"atoms_protein\"]\n",
    "\n",
    "        if isinstance(ligand_df, pd.DataFrame):\n",
    "            atom_df = pd.concat([protein_df, ligand_df], axis=0)\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "            lig_flag[-len(ligand_df):] = 0\n",
    "        else:\n",
    "            atom_df = protein_df\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "        chain_ids = list(set(protein_df['chain']))\n",
    "        for i, id in enumerate(chain_ids):\n",
    "            lig_flag[torch.tensor(list(atom_df['chain'] == id))] = i + 1\n",
    "        labels = item[\"labels\"]\n",
    "        #目前是按照肽链来区分不同的蛋白，为了便于Unprot分类\n",
    "        if self.gearnet:\n",
    "            graph = hetero_graph_transform(\n",
    "                atom_df=atom_df, super_node=self.supernode, flag=lig_flag, protein_seq=item['protein_seq']\n",
    "            )\n",
    "        else:\n",
    "            graph = prot_graph_transform(\n",
    "                atom_df=atom_df, cutoff=self.cutoff, max_num_neighbors=self.max_num_neighbors, flag=lig_flag, super_node=self.supernode, offset_strategy=self.offset_strategy\n",
    "            )\n",
    "        \n",
    "        lba = labels['lba']\n",
    "        ppi = labels['ppi']\n",
    "        if lba != -1:\n",
    "            affinity = lba\n",
    "            graph.affinity_mask = torch.ones(1)\n",
    "            graph.y = torch.FloatTensor([affinity])\n",
    "        elif ppi != -1:\n",
    "            affinity = ppi\n",
    "            graph.affinity_mask = torch.ones(1)\n",
    "            graph.y = torch.FloatTensor([affinity])\n",
    "        else:\n",
    "            graph.y = torch.FloatTensor([0])\n",
    "            graph.affinity_mask = torch.zeros(1)\n",
    "        graph.chains = lig_flag[lig_flag!=0]\n",
    "\n",
    "        graph.lig_flag = lig_flag\n",
    "        graph.prot_id = item[\"complex_id\"]\n",
    "        graph.type = self.task\n",
    "        return graph\n",
    "    \n",
    "    \n",
    "class GNNTransformEC(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cutoff: float = 4.5,\n",
    "        remove_hydrogens: bool = True,\n",
    "        max_num_neighbors: int = 32,\n",
    "        supernode: bool = False,\n",
    "        offset_strategy: int = 0,\n",
    "        task='ec', #ec\n",
    "        gearnet=False\n",
    "    ):\n",
    "        self.cutoff = cutoff\n",
    "        self.remove_hydrogens = remove_hydrogens\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        self.supernode = supernode\n",
    "        self.offset_strategy = offset_strategy\n",
    "        self.task = task\n",
    "        self.gearnet = gearnet\n",
    "\n",
    "    def __call__(self, item: Dict) -> MyData:\n",
    "        # print(\"Using Transform EC\")\n",
    "        info_root = './output_info/uniprot_dict_all.json'\n",
    "        with open(info_root, 'r') as f:\n",
    "            chain_uniprot_info = json.load(f)\n",
    "        ligand_df = item[\"atoms_ligand\"]\n",
    "        protein_df = item[\"atoms_protein\"]\n",
    "        atom_df = protein_df\n",
    "        if isinstance(ligand_df, pd.DataFrame):\n",
    "            atom_df = pd.concat([protein_df, ligand_df], axis=0)\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "            lig_flag[-len(ligand_df):] = 0\n",
    "        else:\n",
    "            atom_df = protein_df\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "        chain_ids = list(set(protein_df['chain']))\n",
    "        uniprot_ids = []\n",
    "        labels = item[\"labels\"]\n",
    "        pf_ids = []\n",
    "        #目前是按照肽链来区分不同的蛋白，为了便于Unprot分类\n",
    "        for i, id in enumerate(chain_ids):\n",
    "            lig_flag[torch.tensor(list(atom_df['chain'] == id))] = i + 1\n",
    "            if '-' in item['complex_id']:\n",
    "                pf_ids.append(0)\n",
    "                break\n",
    "            if id in chain_uniprot_info[item['complex_id']]:\n",
    "                uniprot_id = chain_uniprot_info[item['complex_id']][id]\n",
    "                uniprot_ids.append(uniprot_id)\n",
    "                labels_uniprot = labels['uniprots']\n",
    "                if uniprot_id in labels_uniprot:\n",
    "                    for idx, u in enumerate(labels_uniprot):\n",
    "                        if uniprot_id == u:\n",
    "                            pf_ids.append(idx)\n",
    "                            break\n",
    "                else:\n",
    "                    pf_ids.append(-1)\n",
    "                    print(\"Error, you shouldn't come here!\")\n",
    "            else:\n",
    "                pf_ids.append(-1)\n",
    "        num_classes =538\n",
    "        if self.gearnet:\n",
    "            graph = hetero_graph_transform(\n",
    "                atom_df=atom_df, super_node=self.supernode, flag=lig_flag, protein_seq=item['protein_seq']\n",
    "            )\n",
    "        else:\n",
    "            graph = prot_graph_transform(\n",
    "                atom_df=atom_df, cutoff=self.cutoff, max_num_neighbors=self.max_num_neighbors, flag=lig_flag, super_node=self.supernode, offset_strategy=self.offset_strategy\n",
    "            )\n",
    "        ec = labels['ec']\n",
    "        graph.functions = []\n",
    "        graph.valid_masks = []\n",
    "        for i, pf_id in enumerate(pf_ids):\n",
    "            if pf_id == -1:\n",
    "                valid_mask = torch.zeros(num_classes)\n",
    "                prop = torch.zeros(num_classes)\n",
    "                graph.functions.append(prop)\n",
    "                graph.valid_masks.append(valid_mask)\n",
    "                continue\n",
    "            valid_mask = torch.ones(num_classes)\n",
    "            annotations = []\n",
    "            ec_annot = ec[pf_id]\n",
    "            if ec_annot == -1:\n",
    "                valid_mask[:] = 0\n",
    "            else:\n",
    "                annotations = ec_annot\n",
    "                \n",
    "            prop = torch.zeros(num_classes).scatter_(0,torch.tensor(annotations),1)\n",
    "            graph.functions.append(prop)\n",
    "            graph.valid_masks.append(valid_mask)\n",
    "        try:\n",
    "            graph.functions = torch.vstack(graph.functions)\n",
    "            graph.valid_masks = torch.vstack(graph.valid_masks)\n",
    "        except:\n",
    "            print(\"PF ids:\", pf_ids)\n",
    "            print(item['complex_id'], chain_ids, labels)\n",
    "            print(len(graph.functions))\n",
    "            print(pf_ids)\n",
    "            print(graph.functions)\n",
    "            raise RuntimeError    \n",
    "        graph.chains = lig_flag[lig_flag!=0]\n",
    "        # print(item['complex_id'])\n",
    "\n",
    "        graph.lig_flag = lig_flag\n",
    "        if len(chain_ids) != len(graph.functions):\n",
    "            print(item['complex_id'])\n",
    "            print(chain_ids)\n",
    "            print(len(chain_ids), len(graph.functions))\n",
    "        graph.prot_id = item[\"complex_id\"]\n",
    "        graph.type = self.task\n",
    "        # print(\"Task Type:\", graph.type)\n",
    "        return graph\n",
    "\n",
    "\n",
    "\n",
    "class GNNTransformMultiTask(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cutoff: float = 4.5,\n",
    "        remove_hydrogens: bool = True,\n",
    "        max_num_neighbors: int = 32,\n",
    "        supernode: bool = False,\n",
    "        offset_strategy: int = 0,\n",
    "        gearnet = False\n",
    "    ):\n",
    "        self.cutoff = cutoff\n",
    "        self.remove_hydrogens = remove_hydrogens\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        self.supernode = supernode\n",
    "        self.offset_strategy = offset_strategy\n",
    "        self.gearnet = gearnet\n",
    "\n",
    "    def __call__(self, item: Dict) -> MyData:\n",
    "        # print(\"Using Transform LBA\")\n",
    "        info_root = './output_info/uniprot_dict_all.json'\n",
    "        with open(info_root, 'r') as f:\n",
    "            chain_uniprot_info = json.load(f)\n",
    "        \n",
    "        ligand_df = item[\"atoms_ligand\"]\n",
    "        protein_df = item[\"atoms_protein\"]\n",
    "\n",
    "        if isinstance(ligand_df, pd.DataFrame):\n",
    "            atom_df = pd.concat([protein_df, ligand_df], axis=0)\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "            lig_flag[-len(ligand_df):] = 0\n",
    "        else:\n",
    "            atom_df = protein_df\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "        chain_ids = list(set(protein_df['chain']))\n",
    "        uniprot_ids = []\n",
    "        labels = item[\"labels\"]\n",
    "        pf_ids = []\n",
    "        #目前是按照肽链来区分不同的蛋白，为了便于Unprot分类\n",
    "        for i, id in enumerate(chain_ids):\n",
    "            lig_flag[torch.tensor(list(atom_df['chain'] == id))] = i + 1\n",
    "            if '-' in item['complex_id']:\n",
    "                pf_ids.append(0)\n",
    "                break\n",
    "            if id in chain_uniprot_info[item['complex_id']]:\n",
    "                uniprot_id = chain_uniprot_info[item['complex_id']][id]\n",
    "                uniprot_ids.append(uniprot_id)\n",
    "                labels_uniprot = labels['uniprots']\n",
    "                if uniprot_id in labels_uniprot:\n",
    "                    for idx, u in enumerate(labels_uniprot):\n",
    "                        if uniprot_id == u:\n",
    "                            pf_ids.append(idx)\n",
    "                            break\n",
    "                else:\n",
    "                    pf_ids.append(-1)\n",
    "                    print(\"Error, you shouldn't come here!\")\n",
    "            else:\n",
    "                pf_ids.append(-1)\n",
    "        # ec, mf, bp, cc\n",
    "        # num_classes = 538 + 490 + 1944 + 321\n",
    "        num_classes = [538, 490, 1944, 321]\n",
    "        total_classes = 538 + 490 + 1944 + 321\n",
    "        # 找个办法把chain和Uniprot对应起来，然后就可以查了\n",
    "        if self.gearnet:\n",
    "            graph = hetero_graph_transform(\n",
    "                atom_df=atom_df, super_node=self.supernode, flag=lig_flag, protein_seq=item['protein_seq']\n",
    "            )\n",
    "        else:\n",
    "            graph = prot_graph_transform(\n",
    "                atom_df=atom_df, cutoff=self.cutoff, max_num_neighbors=self.max_num_neighbors, flag=lig_flag, super_node=self.supernode, offset_strategy=self.offset_strategy\n",
    "            )\n",
    "        lba = labels['lba']\n",
    "        ppi = labels['ppi']\n",
    "        ec = labels['ec']\n",
    "        go = labels['go']\n",
    "        graph.affinities = torch.FloatTensor([lba, ppi]).unsqueeze(0)\n",
    "        if lba != -1:\n",
    "            graph.affinity_mask = torch.tensor([1, 0]).unsqueeze(0)\n",
    "        elif ppi != -1:\n",
    "            graph.affinity_mask = torch.tensor([0, 1]).unsqueeze(0)\n",
    "        else:\n",
    "            graph.affinity_mask = torch.tensor([0, 0]).unsqueeze(0)\n",
    "\n",
    "        graph.functions = []\n",
    "        graph.valid_masks = []\n",
    "        for i, pf_id in enumerate(pf_ids):\n",
    "            if pf_id == -1:\n",
    "                valid_mask = torch.zeros(len(num_classes))\n",
    "                prop = torch.zeros(total_classes)\n",
    "                graph.functions.append(prop)\n",
    "                graph.valid_masks.append(valid_mask)\n",
    "                continue\n",
    "            valid_mask = torch.ones(len(num_classes))\n",
    "            annotations = []\n",
    "            ec_annot = ec[pf_id]\n",
    "            go_annot = go[pf_id]\n",
    "            if ec_annot == -1:\n",
    "                valid_mask[0] = 0\n",
    "            else:\n",
    "                annotations = annotations + ec_annot\n",
    "            if go_annot == -1:\n",
    "                valid_mask[1:] = 0\n",
    "            else:\n",
    "                mf_annot = go_annot['molecular_functions'] \n",
    "                mf_annot = [j + 538 for j in mf_annot]\n",
    "                if len(mf_annot) == 0:\n",
    "                    valid_mask[1] = 0\n",
    "                bp_annot = go_annot['biological_process']\n",
    "                bp_annot = [j + 538 + 490 for j in bp_annot]\n",
    "                if len(bp_annot) == 0:\n",
    "                    valid_mask[2] = 0\n",
    "                cc_annot = go_annot['cellular_component']\n",
    "                cc_annot = [j + 538 + 490 + 1944 for j in cc_annot]\n",
    "                if len(cc_annot) == 0:\n",
    "                    valid_mask[3] = 0\n",
    "                annotations = annotations + mf_annot + bp_annot + cc_annot\n",
    "                \n",
    "            prop = torch.zeros(total_classes).scatter_(0,torch.tensor(annotations),1)\n",
    "            graph.functions.append(prop)\n",
    "            graph.valid_masks.append(valid_mask)\n",
    "        try:\n",
    "            graph.functions = torch.vstack(graph.functions)\n",
    "            graph.valid_masks = torch.vstack(graph.valid_masks)\n",
    "        except:\n",
    "            print(\"PF ids:\", pf_ids)\n",
    "            print(item['complex_id'], chain_ids, labels)\n",
    "            print(len(graph.functions))\n",
    "            print(pf_ids)\n",
    "            print(graph.functions)\n",
    "            raise RuntimeError\n",
    "        graph.chains = lig_flag[lig_flag!=0]\n",
    "        # print(item['complex_id'])\n",
    "\n",
    "        graph.lig_flag = lig_flag\n",
    "        if len(chain_ids) != len(graph.functions):\n",
    "            print(item['complex_id'])\n",
    "            print(chain_ids)\n",
    "            print(len(chain_ids), len(graph.functions))\n",
    "        graph.prot_id = item[\"complex_id\"]\n",
    "        graph.type = 'multi'\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMultiTaskDataset(Dataset):\n",
    "    \"\"\"\n",
    "    The Custom MultiTask Dataset with uniform labels\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: str = './datasets/Multi_task', label_dir: str = './datasets/MultiTask/uniformed_labels.json',\n",
    "                remove_hoh = True, remove_hydrogen = False, cutoff = 6, split : str = 'train', task = 'multi', gearnet = False, alpha_only=False):\n",
    "        super(CustomMultiTaskDataset, self).__init__(root_dir)\n",
    "        print(\"Initializing MultiTask Dataset...\")\n",
    "        self.root_dir = root_dir\n",
    "        self.cache_dir = os.path.join(root_dir, \"{}.cache\".format(split))\n",
    "        with open(label_dir, 'r') as f:\n",
    "            self.labels = json.load(f)\n",
    "        self.remove_hoh = remove_hoh\n",
    "        self.remove_hydrogen = remove_hydrogen # 移除氢\n",
    "        self.cutoff = cutoff\n",
    "        self.gearnet = gearnet\n",
    "        self.alpha_only = alpha_only\n",
    "        file_dir = os.path.join(root_dir, split+'.txt')        \n",
    "        self.ec_root = './data/EnzymeCommission/all'\n",
    "        self.go_root = './data/GeneOntology/all'\n",
    "        self.lba_root = './data/PDBbind/refined-set'\n",
    "        self.pp_root = './data/PDBbind/PP'\n",
    "        with open(file_dir, 'r') as f:\n",
    "            self.files = f.readlines()\n",
    "            self.files = [i.strip() for i in self.files]\n",
    "        if split not in ['train', 'val', 'test', 'train_all','train_ec', 'val_ec', 'test_ec']:\n",
    "            print(\"Wrong selected split. Have to choose between ['train', 'val', 'test', 'test_all']\")\n",
    "            print(\"Exiting code\")\n",
    "            exit()\n",
    "        if task not in ['affinity', 'ec', 'cc', 'mf', 'bp', 'multi', 'go', 'ppi', 'lba']:\n",
    "            print(\"Wrong selected task. Have to choose between ['affinity', 'ec', 'cc', 'mf', 'bp', 'multi', 'go']\")\n",
    "            print(\"Exiting code\")\n",
    "            exit()\n",
    "        self.split = split\n",
    "        self.task = task\n",
    "        self.process_complexes()\n",
    "        print('finish init')\n",
    "    def find_structure(self, item):\n",
    "        self.ec_files = os.listdir(self.ec_root)\n",
    "        self.go_files = os.listdir(self.go_root)\n",
    "        self.lba_files = os.listdir(self.lba_root)\n",
    "        self.pp_files = os.listdir(self.pp_root)\n",
    "        if '-' in item:\n",
    "            if item+'.pdb' in self.ec_files:\n",
    "                return os.path.join(self.ec_root, item+'.pdb'), -1\n",
    "            elif item+'.pdb' in self.go_files:\n",
    "                return os.path.join(self.go_root, item+'.pdb'), -1\n",
    "        else:\n",
    "            if item + '.ent.pdb' in self.pp_files:\n",
    "                return os.path.join(self.pp_root, item+'.ent.pdb'), -1\n",
    "            elif item in self.lba_files:\n",
    "                protein_dir = os.path.join(self.lba_root, item, item + \"_protein.pdb\")\n",
    "                ligand_dir = os.path.join(self.lba_root, item, item + '_ligand.mol2')\n",
    "                return protein_dir, ligand_dir\n",
    "        print(item)\n",
    "        return -1, -1\n",
    "    def gen_df(self, coords, elements):\n",
    "        assert len(coords) == len(elements)\n",
    "        unified_elements = []\n",
    "        xs, ys, zs = [coord[0] for coord in coords], [coord[1] for coord in coords], [coord[2] for coord in coords]\n",
    "        for item in elements:\n",
    "            if item in ['CL', 'Cl', 'Br', 'BR', 'AT', 'At', 'F', 'I']:\n",
    "                element = 'Halogen'\n",
    "            elif item in ['FE', 'ZN', 'MG', 'MN', 'K', 'LI', 'Ca', 'Hg']:\n",
    "                element = 'Metal'\n",
    "            else:\n",
    "                element = item\n",
    "            unified_elements.append(element)\n",
    "        df = pd.DataFrame({'element': unified_elements, 'x': xs, 'y': ys, 'z': zs})\n",
    "        return df\n",
    "    def process_complexes(self):\n",
    "        p = PDBParser(QUIET=True)\n",
    "        nmr_files = []\n",
    "        wrong_number = []\n",
    "        self.processed_complexes = []\n",
    "        corrupted = []\n",
    "        # cache_dir = os.path.join(self.root_dir, self.cache_dir)\n",
    "        if os.path.exists(self.cache_dir):\n",
    "            print(\"Start loading cached Multitask files...\")\n",
    "            self.processed_complexes = pickle.load(open(self.cache_dir, 'rb'))\n",
    "            print(\"Complexes Before Checking:\", self.len())\n",
    "            self.check_dataset()\n",
    "            print(\"Complexes Before Task Selection:\", self.len())\n",
    "            self.choose_task_items()\n",
    "            print(\"Dataset size:\", self.len())\n",
    "            if self.alpha_only:\n",
    "                print(\"Only retaining Alpha Carbon atoms for the atom_df\")\n",
    "                self.retain_alpha_carbon()\n",
    "        else:\n",
    "            print(\"Cache not found! Start processing Multitask files...Total Number {}\".format(len(self.files)))\n",
    "            # count = 0\n",
    "            for score_idx, item in enumerate(tqdm(self.files)):\n",
    "                structure_dir, ligand_dir = self.find_structure(item)\n",
    "                if ligand_dir != -1:\n",
    "                    ligand = next(pybel.readfile('mol2', ligand_dir))\n",
    "                    ligand_coords = [atom.coords for atom in ligand]\n",
    "                    atom_map_lig = [atomic_num_dict(atom.atomicnum) for atom in ligand]\n",
    "                    ligand_df = self.gen_df(ligand_coords, atom_map_lig)\n",
    "                else:\n",
    "                    ligand_df = None\n",
    "                try:\n",
    "                    structure = p.get_structure(item, structure_dir)\n",
    "                except:\n",
    "                    corrupted.append(item)\n",
    "                    continue\n",
    "                # structure = p.get_structure(item, structure_dir)\n",
    "                compound_info = structure.header['compound']\n",
    "                protein_numbers = len(compound_info.items())\n",
    "                \n",
    "                if len(structure) > 1:\n",
    "                    nmr_files.append(item)\n",
    "                    continue\n",
    "                if item not in self.labels:\n",
    "                    wrong_number.append(item)\n",
    "                    continue\n",
    "                model = structure[0]\n",
    "                chains = list(model.get_chains())\n",
    "                pattern = re.compile(r'\\d+H.')\n",
    "                processed_complex = {'complex_id': item, 'num_proteins': protein_numbers, 'labels': self.labels[item],\n",
    "                                    'atoms_protein': [], 'protein_seq': [], 'atoms_ligand':ligand_df}\n",
    "                elements = []\n",
    "                xs = []\n",
    "                ys = []\n",
    "                zs = []\n",
    "                chain_ids = []\n",
    "                protein_seq = []\n",
    "                names = []\n",
    "                resnames = []\n",
    "                # chain = chains[0]\n",
    "                for chain in chains:\n",
    "                    if chain.id == ' ':\n",
    "                        continue\n",
    "                    for residue in chain.get_residues():\n",
    "                        # 删除HOH原子\n",
    "                        if self.remove_hoh and residue.get_resname() == 'HOH':\n",
    "                            continue\n",
    "                        protein_seq.append(residue.get_resname())\n",
    "                        for atom in residue:\n",
    "                            # 删除氢原子\n",
    "                            atom_id = atom.get_id()\n",
    "                            if self.remove_hydrogen and atom.get_id().startswith('H') or pattern.match(atom.get_id()) != None:\n",
    "                                continue\n",
    "                            if atom_id.startswith('H') or pattern.match(atom.get_id()) != None:\n",
    "                                element = 'H'\n",
    "                            elif atom_id[0:2] in ['CL', 'Cl', 'Br', 'BR', 'AT', 'At']:\n",
    "                                element = 'Halogen'\n",
    "                            elif atom_id[0:2] in ['FE', 'ZN', 'MG', 'MN', 'K', 'LI']:\n",
    "                                element = 'Metal'\n",
    "                            elif atom_id[0] in ['F', 'I']:\n",
    "                                element = 'Halogen'\n",
    "                            elif atom_id[0] in ['C','N','O','S','P']:\n",
    "                                element = atom_id[0]\n",
    "                            else:\n",
    "                                element = atom_id\n",
    "                            names.append(atom_id)\n",
    "                            elements.append(element)\n",
    "                            chain_ids.append(chain.id)\n",
    "                            resnames.append(residue.get_resname())\n",
    "                            x, y, z = atom.get_vector()\n",
    "                            xs.append(x)\n",
    "                            ys.append(y)\n",
    "                            zs.append(z)\n",
    "                protein_df = pd.DataFrame({'chain': chain_ids, 'resname': resnames, 'element': elements, 'name': names, 'x': xs, 'y': ys, 'z': zs})\n",
    "                processed_complex['atoms_protein'] = protein_df\n",
    "                processed_complex['protein_seq'] = protein_seq\n",
    "                \n",
    "                self.processed_complexes.append(processed_complex)\n",
    "                # count += 1\n",
    "                # if count == 128:\n",
    "                #     break\n",
    "            print(\"Structure processed Done, dumping...\")\n",
    "            print(\"Structures with Wrong numbers:\", len(wrong_number), wrong_number)\n",
    "            print(\"Structures with NMR methods:\", len(nmr_files), nmr_files)\n",
    "            print(\"Corrupted:\", len(corrupted), corrupted)\n",
    "            pickle.dump(self.processed_complexes, open(self.cache_dir, 'wb'))\n",
    "            print(\"Complexes Before Checking:\", self.len())\n",
    "            self.check_dataset()\n",
    "            print(\"Complexes Before Task Selection:\", self.len())\n",
    "            self.choose_task_items()\n",
    "            print(\"Dataset size:\", self.len())\n",
    "            if self.alpha_only:\n",
    "                print(\"Only retaining Alpha Carbon atoms for the atom_df\")\n",
    "                self.retain_alpha_carbon()\n",
    "    def correctness_check(self, chain_uniprot_info, complx):\n",
    "        #Annotation Correctness Check\n",
    "        correct = True\n",
    "        chain_ids = list(set(complx['atoms_protein']['chain']))\n",
    "        if '-' not in complx['complex_id']:\n",
    "            for i, id in enumerate(chain_ids):\n",
    "                if id in chain_uniprot_info[complx['complex_id']]:\n",
    "                    uniprot_id = chain_uniprot_info[complx['complex_id']][id]\n",
    "                    labels_uniprot = complx['labels']['uniprots']\n",
    "                    if uniprot_id not in labels_uniprot:\n",
    "                        print(\"Error, you shouldn't come here!\")\n",
    "                        correct = False\n",
    "                        print(complx['complex_id'], chain_ids, chain_uniprot_info[complx['complex_id']])\n",
    "        return correct\n",
    "    def cal_length_thres(self, complxes):\n",
    "        length_list = []\n",
    "        for complx in complxes:\n",
    "            length = len(complx['atoms_protein']['element'])\n",
    "            length_list.append(length)\n",
    "        sorted_list = sorted(length_list)\n",
    "        thres = sorted_list[int(0.95*len(sorted_list))]\n",
    "        print(\"Cutting Threshold of atom numbers:\", thres)\n",
    "        return thres\n",
    "    def length_check(self, complx, thres):\n",
    "        if len(complx['atoms_protein']['element']) > thres:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    def check_dataset(self):\n",
    "        print(\"Checking the dataset...\")\n",
    "        info_root = './output_info/uniprot_dict_all.json'\n",
    "        with open(info_root, 'r') as f:\n",
    "            chain_uniprot_info = json.load(f)\n",
    "        thres = self.cal_length_thres(self.processed_complexes)\n",
    "        if self.split == 'train':\n",
    "            thres = 6712\n",
    "        self.processed_complexes = [i for i in tqdm(self.processed_complexes) if self.length_check(i, thres) and self.correctness_check(chain_uniprot_info, i)]\n",
    "    \n",
    "    def retain_alpha_carbon(self):\n",
    "        new_complexes = []\n",
    "        for item in self.processed_complexes:\n",
    "            protein_df = item['atoms_protein']\n",
    "            # print(\"Original Nodes:\", len(protein_df))\n",
    "            new_protein_df = protein_df[protein_df.name == 'CA'].reset_index(drop=True)\n",
    "            item['atoms_protein'] = new_protein_df\n",
    "            # print(\"Retaining Alpha Carbons:\", len(new_protein_df))\n",
    "            new_complexes.append(item)\n",
    "        self.processed_complexes = new_complexes\n",
    "    \n",
    "    def choose_task_items(self):\n",
    "        # 根据不同的任务，训练单独的模型\n",
    "        if self.split in ['train', 'val', 'test']:\n",
    "            extra_dir = './datasets/MultiTask/{}.txt'.format(self.split)\n",
    "            with open(extra_dir, 'r') as f:\n",
    "                extra_info = f.readlines()\n",
    "                extra_info = [i.strip() for i in extra_info]\n",
    "        else:\n",
    "            extra_info = []\n",
    "        if self.task == 'ec':\n",
    "            print(\"Using Enzyme Commission Dataset for training:\")\n",
    "            root_dir = './output_info/enzyme_commission_uniprots.json'\n",
    "            with open(root_dir, 'r') as f:\n",
    "                info_dict = json.load(f)\n",
    "            new_complexes = []\n",
    "            for item in self.processed_complexes:\n",
    "                if item['complex_id'] in info_dict or item['complex_id'] in extra_info: #.keys()?\n",
    "                    labels = item['labels']\n",
    "                    annot_number = len(labels['uniprots'])\n",
    "                    for j in range(annot_number):\n",
    "                        labels['go'][j] = -1\n",
    "                    item['labels'] = labels\n",
    "                    new_complexes.append(item)\n",
    "            self.processed_complexes = new_complexes\n",
    "            self.transform_func = GNNTransformEC(task=self.task, gearnet=self.gearnet)\n",
    "            print(\"Using EC dataset and transformation\")\n",
    "        elif self.task in ['bp', 'mf', 'cc', 'go']:\n",
    "            print(\"Using Gene Ontology {} Dataset for training:\".format(self.split))\n",
    "            root_dir = './output_info/gene_ontology_uniprots.json'\n",
    "            with open(root_dir, 'r') as f:\n",
    "                info_dict = json.load(f)\n",
    "            new_complexes = []\n",
    "            for item in self.processed_complexes:\n",
    "                if item['complex_id'] in info_dict or item['complex_id'] in extra_info:\n",
    "                    labels = item['labels']\n",
    "                    annot_number = len(labels['uniprots'])\n",
    "                    for j in range(annot_number):\n",
    "                        labels['ec'][j] = -1\n",
    "                    item['labels'] = labels\n",
    "                    new_complexes.append(item)\n",
    "            self.processed_complexes = new_complexes\n",
    "            self.transform_func = GNNTransformGO(task=self.task, gearnet=self.gearnet)\n",
    "        elif self.task in ['affinity', 'lba', 'ppi']:\n",
    "            print(\"Using Affinity Dataset for training:\")\n",
    "            root_dir = './output_info/protein_protein_uniprots.json'\n",
    "            with open(root_dir, 'r') as f:\n",
    "                info_dict1 = json.load(f)\n",
    "            root_dir2 = './output_info/protein_ligand_uniprots.json'\n",
    "            with open(root_dir2, 'r') as f:\n",
    "                info_dict2 = json.load(f)\n",
    "            info_dict = {**info_dict1, **info_dict2}\n",
    "            new_complexes = []\n",
    "            for item in self.processed_complexes:\n",
    "                if item['complex_id'] in info_dict or item['complex_id'] in extra_info:\n",
    "                    labels = item['labels']\n",
    "                    annot_number = len(labels['uniprots'])\n",
    "                    for j in range(annot_number):\n",
    "                        labels['ec'][j] = -1\n",
    "                        labels['go'][j] = -1\n",
    "                    item['labels'] = labels\n",
    "                    new_complexes.append(item)\n",
    "            self.processed_complexes = new_complexes\n",
    "            self.transform_func = GNNTransformAffinity(task=self.task, gearnet=self.gearnet)\n",
    "        else:\n",
    "            self.transform_func = GNNTransformMultiTask(gearnet=self.gearnet)\n",
    "    def len(self):\n",
    "        return len(self.processed_complexes)\n",
    "    def get(self, idx):\n",
    "        return self.transform_func(self.processed_complexes[idx])\n",
    "    def print_complexes(self, idx):\n",
    "        return self.processed_complexes[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_functions = None\n",
    "class GNNTransformGO(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cutoff: float = 4.5,\n",
    "        remove_hydrogens: bool = True,\n",
    "        max_num_neighbors: int = 32,\n",
    "        supernode: bool = False,\n",
    "        offset_strategy: int = 0,\n",
    "        task='bp', #可能是bp, mf, cc中的一个\n",
    "        gearnet=False\n",
    "    ):\n",
    "        self.cutoff = cutoff\n",
    "        self.remove_hydrogens = remove_hydrogens\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        self.supernode = supernode\n",
    "        self.offset_strategy = offset_strategy\n",
    "        self.task = task\n",
    "        self.gearnet = gearnet\n",
    "        # print(\"GNNtransforgo\")\n",
    "\n",
    "    def __call__(self, item: Dict) -> MyData:\n",
    "        info_root = './output_info/uniprot_dict_all.json'\n",
    "        with open(info_root, 'r') as f:\n",
    "            chain_uniprot_info = json.load(f)\n",
    "        # print(\"Using Transform {}\".format(self.task))\n",
    "        ligand_df = item['atoms_ligand']\n",
    "        protein_df = item[\"atoms_protein\"]\n",
    "        atom_df = protein_df\n",
    "        if isinstance(ligand_df, pd.DataFrame):\n",
    "            atom_df = pd.concat([protein_df, ligand_df], axis=0)\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "            lig_flag[-len(ligand_df):] = 0\n",
    "        else:\n",
    "            atom_df = protein_df\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "        chain_ids = list(set(protein_df['chain']))\n",
    "        uniprot_ids = []\n",
    "        labels = item[\"labels\"]\n",
    "        pf_ids = []\n",
    "        #目前是按照肽链来区分不同的蛋白，为了便于Unprot分类\n",
    "        for i, id in enumerate(chain_ids):\n",
    "            lig_flag[torch.tensor(list(atom_df['chain'] == id))] = i + 1\n",
    "            if '-' in item['complex_id']:\n",
    "                pf_ids.append(0)\n",
    "                break\n",
    "            if id in chain_uniprot_info[item['complex_id']]:\n",
    "                uniprot_id = chain_uniprot_info[item['complex_id']][id]\n",
    "                uniprot_ids.append(uniprot_id)\n",
    "                labels_uniprot = labels['uniprots']\n",
    "                if uniprot_id in labels_uniprot:\n",
    "                    for idx, u in enumerate(labels_uniprot):\n",
    "                        if uniprot_id == u:\n",
    "                            pf_ids.append(idx)\n",
    "                            break\n",
    "                else:\n",
    "                    pf_ids.append(-1)\n",
    "                    print(\"Error, you shouldn't come here!\")\n",
    "            else:\n",
    "                pf_ids.append(-1)\n",
    "        # print(\"pf_ids\", pf_ids)        \n",
    "        if self.task == 'mf':\n",
    "            num_classes = 490\n",
    "        elif self.task == 'bp':\n",
    "            num_classes = 1944\n",
    "        elif self.task == 'cc':\n",
    "            num_classes = 321\n",
    "        elif self.task == 'go':\n",
    "            num_classes = 490 + 1944 + 321\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "        # 找个办法把chain和Uniprot对应起来，然后就可以查了\n",
    "        if self.gearnet:\n",
    "            graph = hetero_graph_transform(\n",
    "                atom_df=atom_df, super_node=self.supernode, flag=lig_flag, protein_seq=item['protein_seq']\n",
    "            )\n",
    "        else:\n",
    "            graph = prot_graph_transform(\n",
    "                atom_df=atom_df, cutoff=self.cutoff, max_num_neighbors=self.max_num_neighbors, flag=lig_flag, super_node=self.supernode, offset_strategy=self.offset_strategy\n",
    "            )\n",
    "        go = labels['go']\n",
    "        # graph.y = torch.zeros(self.num_classes).scatter_(0,torch.tensor(labels),1)\n",
    "        graph.functions = []\n",
    "        graph.valid_masks = []\n",
    "        \n",
    "        for i, pf_id in enumerate(pf_ids):\n",
    "            if pf_id == -1:\n",
    "                valid_mask = torch.zeros(num_classes)\n",
    "                prop = torch.zeros(num_classes)\n",
    "                graph.functions.append(prop)\n",
    "                graph.valid_masks.append(valid_mask)\n",
    "                continue\n",
    "            valid_mask = torch.ones(num_classes)\n",
    "            annotations = []\n",
    "            go_annot = go[pf_id]\n",
    "            if self.task == 'mf':\n",
    "                mf_annot = go_annot['molecular_functions'] \n",
    "                mf_annot = [j for j in mf_annot]\n",
    "                if len(mf_annot) == 0:\n",
    "                    valid_mask[:] = 0\n",
    "                annotations = mf_annot\n",
    "            elif self.task == 'bp':\n",
    "                bp_annot = go_annot['biological_process']\n",
    "                bp_annot = [j for j in bp_annot]\n",
    "                if len(bp_annot) == 0:\n",
    "                    valid_mask[:] = 0\n",
    "                annotations = bp_annot\n",
    "            elif self.task == 'cc':\n",
    "                cc_annot = go_annot['cellular_component']\n",
    "                cc_annot = [j for j in cc_annot]\n",
    "                if len(cc_annot) == 0:\n",
    "                    valid_mask[:] = 0\n",
    "                annotations = cc_annot\n",
    "            elif self.task == 'go':\n",
    "                mf_annot = go_annot['molecular_functions'] \n",
    "                mf_annot = [j for j in mf_annot]\n",
    "                if len(mf_annot) == 0:\n",
    "                    valid_mask[: 490] = 0\n",
    "                bp_annot = go_annot['biological_process']\n",
    "                bp_annot = [j + 490 for j in bp_annot]\n",
    "                if len(bp_annot) == 0:\n",
    "                    valid_mask[490: 490+1944] = 0\n",
    "                cc_annot = go_annot['cellular_component']\n",
    "                cc_annot = [j+490+1944 for j in cc_annot]\n",
    "                if len(cc_annot) == 0:\n",
    "                    valid_mask[490+1944: ] = 0\n",
    "                annotations = mf_annot + bp_annot + cc_annot\n",
    "                \n",
    "            prop = torch.zeros(num_classes).scatter_(0,torch.tensor(annotations),1)\n",
    "            graph.functions.append(prop)\n",
    "            graph.valid_masks.append(valid_mask)\n",
    "        try:\n",
    "            # print(annotations)\n",
    "            \n",
    "            # my_functions = copy.deepcopy(graph.functions)\n",
    "            # print(my_functions)\n",
    "            graph.functions = torch.vstack(graph.functions)\n",
    "            graph.valid_masks = torch.vstack(graph.valid_masks)\n",
    "            # print(\"386\",graph.functions[0,386])\n",
    "        except:\n",
    "            print(\"PF ids:\", pf_ids)\n",
    "            print(item['complex_id'], chain_ids, labels)\n",
    "            print(len(graph.functions))\n",
    "            print(pf_ids)\n",
    "            print(graph.functions)\n",
    "            raise RuntimeError\n",
    "    \n",
    "        graph.chains = lig_flag[lig_flag!=0]\n",
    "\n",
    "        graph.lig_flag = lig_flag\n",
    "        if len(chain_ids) != len(graph.functions):\n",
    "            print(item['complex_id'])\n",
    "            print(chain_ids)\n",
    "            print(len(chain_ids), len(graph.functions))\n",
    "        # print(graph.functions.shape)\n",
    "        graph.prot_id = item[\"complex_id\"]\n",
    "        graph.type = self.task\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MultiTask Dataset...\n",
      "Start loading cached Multitask files...\n",
      "Complexes Before Checking: 263\n",
      "Checking the dataset...\n",
      "Cutting Threshold of atom numbers: 19358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 263/263 [00:00<00:00, 1826.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complexes Before Task Selection: 199\n",
      "Using Affinity Dataset for training:\n",
      "Dataset size: 199\n",
      "finish init\n"
     ]
    }
   ],
   "source": [
    "_ = CustomMultiTaskDataset(split='train', task='ppi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyData(x=[1660], edge_index=[2, 27212], pos=[1660, 3], edge_weights=[27212], affinity_mask=[1], y=[1], chains=[1626], lig_flag=[1660], prot_id='1qbo', type='ppi')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_ = _.print_complexes(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3djq'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_['complex_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uniprots': ['P00669'], 'ec': [-1], 'go': [-1], 'ppi': -1, 'lba': 2.98}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(my_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scatter_() received an invalid combination of arguments - got (int), but expected one of:\n * (int dim, Tensor index, Tensor src)\n * (int dim, Tensor index, Tensor src, *, str reduce)\n * (int dim, Tensor index, Number value)\n * (int dim, Tensor index, Number value, *, str reduce)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prop \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39m2755\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m prop\u001b[39m.\u001b[39;49mscatter_(\u001b[39m0\u001b[39;49m,)\n",
      "\u001b[0;31mTypeError\u001b[0m: scatter_() received an invalid combination of arguments - got (int), but expected one of:\n * (int dim, Tensor index, Tensor src)\n * (int dim, Tensor index, Tensor src, *, str reduce)\n * (int dim, Tensor index, Number value)\n * (int dim, Tensor index, Number value, *, str reduce)\n"
     ]
    }
   ],
   "source": [
    "prop = torch.zeros(2755)\n",
    "prop.scatter_(0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdim= 100,\n",
    "vdim: int = 16,\n",
    "depth: int = 3,\n",
    "r_cutoff: float = 5.0,\n",
    "num_radial: int = 32,\n",
    "model_type: str = \"eqgat\",\n",
    "learning_rate: float = 1e-4,\n",
    "weight_decay: float = 0.0,\n",
    "patience_scheduler: int = 10,\n",
    "factor_scheduler: float = 0.75,\n",
    "max_epochs: int = 30,\n",
    "use_norm: bool = True,\n",
    "aggr: str = \"mean\",\n",
    "enhanced: bool = False,\n",
    "offset_strategy: int = 0,\n",
    "task = 'ppi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readout Strategy: vallina\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import Tensor, nn\n",
    "from gmsl.model import BaseModel, SEGNNModel\n",
    "from torch_geometric.data import Batch\n",
    "from typing import Tuple\n",
    "model = BaseModel(sdim=100,\n",
    "                                   vdim=16,\n",
    "                                   depth=3,\n",
    "                                   r_cutoff=r_cutoff,\n",
    "                                   num_radial=32,\n",
    "                                   model_type=\"gearnet\",\n",
    "                                   graph_level=True,\n",
    "                                   num_elements=10,\n",
    "                                   out_units=1,\n",
    "                                   dropout=0.0,\n",
    "                                   use_norm=use_norm,\n",
    "                                   aggr=aggr,\n",
    "                                   cross_ablate=False,\n",
    "                                   no_feat_attn=False,\n",
    "                                   task=task\n",
    "                                #    protein_function_class_dims = class_dims\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MultiTask Dataset...\n",
      "Start loading cached Multitask files...\n",
      "Complexes Before Checking: 263\n",
      "Checking the dataset...\n",
      "Cutting Threshold of atom numbers: 19358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 263/263 [00:00<00:00, 1517.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complexes Before Task Selection: 199\n",
      "Using Affinity Dataset for training:\n",
      "Dataset size: 199\n",
      "Only retaining Alpha Carbon atoms for the atom_df\n",
      "finish init\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomMultiTaskDataset(split='train', task=task, gearnet=True, \n",
    "                                       alpha_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home-new/rhan21/anaconda3/envs/gmsl/lib/python3.9/site-packages/torch_geometric/data/collate.py:150: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home-new/rhan21/anaconda3/envs/gmsl/lib/python3.9/site-packages/torch_geometric/data/collate.py:150: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home-new/rhan21/anaconda3/envs/gmsl/lib/python3.9/site-packages/torch_geometric/data/collate.py:150: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home-new/rhan21/anaconda3/envs/gmsl/lib/python3.9/site-packages/torch_geometric/data/collate.py:150: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n"
     ]
    }
   ],
   "source": [
    "y = None\n",
    "d = None\n",
    "for idx, data in enumerate(train_loader):\n",
    "    # y = model(data)\n",
    "    d = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[2].shape\n",
    "y_ = torch.hstack(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2755])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.7000,  7.1700, 10.1700,  6.5500,  8.5200,  8.0100,  7.5100])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.y.view(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyData(x=[279, 21], pos=[279, 3], num_residues=[1], chain=0        A\n",
       "1        A\n",
       "2        A\n",
       "3        A\n",
       "4        A\n",
       "      ... \n",
       "274    NaN\n",
       "275    NaN\n",
       "276    NaN\n",
       "277    NaN\n",
       "278    NaN\n",
       "Name: chain, Length: 279, dtype: object, edge_index=[2, 6613], edge_relations=[6613], edge_weights=[6613], num_relation=[1], functions=[1, 2755], valid_masks=[1, 2755], chains=[257], lig_flag=[279], prot_id='2hl4', num_nodes=279)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
