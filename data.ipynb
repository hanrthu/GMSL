{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from argparse import ArgumentParser\n",
    "from typing import Dict, List, Tuple, Union\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from atom3d.datasets import deserialize\n",
    "from Bio.PDB import PDBParser\n",
    "import re\n",
    "import pickle\n",
    "import io\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import lmdb\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import openbabel\n",
    "from openbabel import pybel\n",
    "\n",
    "from utils import MyData, prot_graph_transform, hetero_graph_transform\n",
    "\n",
    "pybel.ob.obErrorLog.SetOutputLevel(0)\n",
    "atomic_num_dict = lambda x: {1: 'H', 2: 'HE', 3: 'LI', 4: 'BE', 5: 'B', 6: 'C', 7: 'N', 8: 'O', 9: 'F', 11: 'NA',\n",
    "                   15: 'P', 16: 'S', 17: 'Cl', 20:'Ca', 25: 'MN', 26: 'FE', 30: 'ZN', 35: 'Br', 53: 'I', 80: 'Hg'}.get(x, 'Others')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class GNNTransformAffinity(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cutoff: float = 4.5,\n",
    "        remove_hydrogens: bool = True,\n",
    "        max_num_neighbors: int = 32,\n",
    "        supernode: bool = False,\n",
    "        offset_strategy: int = 0,\n",
    "        task='affinity', #lba/ppi\n",
    "        gearnet=False\n",
    "    ):\n",
    "        self.cutoff = cutoff\n",
    "        self.remove_hydrogens = remove_hydrogens\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        self.supernode = supernode\n",
    "        self.offset_strategy = offset_strategy\n",
    "        self.task = task\n",
    "        self.gearnet = gearnet\n",
    "\n",
    "    def __call__(self, item: Dict) -> MyData:\n",
    "        # print(\"Using Transform Affinity\")\n",
    "        ligand_df = item[\"atoms_ligand\"]\n",
    "        protein_df = item[\"atoms_protein\"]\n",
    "\n",
    "        if isinstance(ligand_df, pd.DataFrame):\n",
    "            atom_df = pd.concat([protein_df, ligand_df], axis=0)\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "            lig_flag[-len(ligand_df):] = 0\n",
    "        else:\n",
    "            atom_df = protein_df\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "        chain_ids = list(set(protein_df['chain']))\n",
    "        for i, id in enumerate(chain_ids):\n",
    "            lig_flag[torch.tensor(list(atom_df['chain'] == id))] = i + 1\n",
    "        labels = item[\"labels\"]\n",
    "        #目前是按照肽链来区分不同的蛋白，为了便于Unprot分类\n",
    "        if self.gearnet:\n",
    "            graph = hetero_graph_transform(\n",
    "                atom_df=atom_df, super_node=self.supernode, flag=lig_flag, protein_seq=item['protein_seq']\n",
    "            )\n",
    "        else:\n",
    "            graph = prot_graph_transform(\n",
    "                atom_df=atom_df, cutoff=self.cutoff, max_num_neighbors=self.max_num_neighbors, flag=lig_flag, super_node=self.supernode, offset_strategy=self.offset_strategy\n",
    "            )\n",
    "        \n",
    "        lba = labels['lba']\n",
    "        ppi = labels['ppi']\n",
    "        if lba != -1:\n",
    "            affinity = lba\n",
    "            graph.affinity_mask = torch.ones(1)\n",
    "            graph.y = torch.FloatTensor([affinity])\n",
    "        elif ppi != -1:\n",
    "            affinity = ppi\n",
    "            graph.affinity_mask = torch.ones(1)\n",
    "            graph.y = torch.FloatTensor([affinity])\n",
    "        else:\n",
    "            graph.y = torch.FloatTensor([0])\n",
    "            graph.affinity_mask = torch.zeros(1)\n",
    "        graph.chains = lig_flag[lig_flag!=0]\n",
    "\n",
    "        graph.lig_flag = lig_flag\n",
    "        graph.prot_id = item[\"complex_id\"]\n",
    "        graph.type = self.task\n",
    "        return graph\n",
    "    \n",
    "    \n",
    "class GNNTransformEC(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cutoff: float = 4.5,\n",
    "        remove_hydrogens: bool = True,\n",
    "        max_num_neighbors: int = 32,\n",
    "        supernode: bool = False,\n",
    "        offset_strategy: int = 0,\n",
    "        task='ec', #ec\n",
    "        gearnet=False\n",
    "    ):\n",
    "        self.cutoff = cutoff\n",
    "        self.remove_hydrogens = remove_hydrogens\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        self.supernode = supernode\n",
    "        self.offset_strategy = offset_strategy\n",
    "        self.task = task\n",
    "        self.gearnet = gearnet\n",
    "\n",
    "    def __call__(self, item: Dict) -> MyData:\n",
    "        # print(\"Using Transform EC\")\n",
    "        info_root = './output_info/uniprot_dict_all.json'\n",
    "        with open(info_root, 'r') as f:\n",
    "            chain_uniprot_info = json.load(f)\n",
    "        ligand_df = item[\"atoms_ligand\"]\n",
    "        protein_df = item[\"atoms_protein\"]\n",
    "        atom_df = protein_df\n",
    "        if isinstance(ligand_df, pd.DataFrame):\n",
    "            atom_df = pd.concat([protein_df, ligand_df], axis=0)\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "            lig_flag[-len(ligand_df):] = 0\n",
    "        else:\n",
    "            atom_df = protein_df\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "        chain_ids = list(set(protein_df['chain']))\n",
    "        uniprot_ids = []\n",
    "        labels = item[\"labels\"]\n",
    "        pf_ids = []\n",
    "        #目前是按照肽链来区分不同的蛋白，为了便于Unprot分类\n",
    "        for i, id in enumerate(chain_ids):\n",
    "            lig_flag[torch.tensor(list(atom_df['chain'] == id))] = i + 1\n",
    "            if '-' in item['complex_id']:\n",
    "                pf_ids.append(0)\n",
    "                break\n",
    "            if id in chain_uniprot_info[item['complex_id']]:\n",
    "                uniprot_id = chain_uniprot_info[item['complex_id']][id]\n",
    "                uniprot_ids.append(uniprot_id)\n",
    "                labels_uniprot = labels['uniprots']\n",
    "                if uniprot_id in labels_uniprot:\n",
    "                    for idx, u in enumerate(labels_uniprot):\n",
    "                        if uniprot_id == u:\n",
    "                            pf_ids.append(idx)\n",
    "                            break\n",
    "                else:\n",
    "                    pf_ids.append(-1)\n",
    "                    print(\"Error, you shouldn't come here!\")\n",
    "            else:\n",
    "                pf_ids.append(-1)\n",
    "        num_classes =538\n",
    "        if self.gearnet:\n",
    "            graph = hetero_graph_transform(\n",
    "                atom_df=atom_df, super_node=self.supernode, flag=lig_flag, protein_seq=item['protein_seq']\n",
    "            )\n",
    "        else:\n",
    "            graph = prot_graph_transform(\n",
    "                atom_df=atom_df, cutoff=self.cutoff, max_num_neighbors=self.max_num_neighbors, flag=lig_flag, super_node=self.supernode, offset_strategy=self.offset_strategy\n",
    "            )\n",
    "        ec = labels['ec']\n",
    "        graph.functions = []\n",
    "        graph.valid_masks = []\n",
    "        for i, pf_id in enumerate(pf_ids):\n",
    "            if pf_id == -1:\n",
    "                valid_mask = torch.zeros(num_classes)\n",
    "                prop = torch.zeros(num_classes)\n",
    "                graph.functions.append(prop)\n",
    "                graph.valid_masks.append(valid_mask)\n",
    "                continue\n",
    "            valid_mask = torch.ones(num_classes)\n",
    "            annotations = []\n",
    "            ec_annot = ec[pf_id]\n",
    "            if ec_annot == -1:\n",
    "                valid_mask[:] = 0\n",
    "            else:\n",
    "                annotations = ec_annot\n",
    "                \n",
    "            prop = torch.zeros(num_classes).scatter_(0,torch.tensor(annotations),1)\n",
    "            graph.functions.append(prop)\n",
    "            graph.valid_masks.append(valid_mask)\n",
    "        try:\n",
    "            graph.functions = torch.vstack(graph.functions)\n",
    "            graph.valid_masks = torch.vstack(graph.valid_masks)\n",
    "        except:\n",
    "            print(\"PF ids:\", pf_ids)\n",
    "            print(item['complex_id'], chain_ids, labels)\n",
    "            print(len(graph.functions))\n",
    "            print(pf_ids)\n",
    "            print(graph.functions)\n",
    "            raise RuntimeError    \n",
    "        graph.chains = lig_flag[lig_flag!=0]\n",
    "        # print(item['complex_id'])\n",
    "\n",
    "        graph.lig_flag = lig_flag\n",
    "        if len(chain_ids) != len(graph.functions):\n",
    "            print(item['complex_id'])\n",
    "            print(chain_ids)\n",
    "            print(len(chain_ids), len(graph.functions))\n",
    "        graph.prot_id = item[\"complex_id\"]\n",
    "        graph.type = self.task\n",
    "        # print(\"Task Type:\", graph.type)\n",
    "        return graph\n",
    "\n",
    "\n",
    "\n",
    "class GNNTransformMultiTask(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cutoff: float = 4.5,\n",
    "        remove_hydrogens: bool = True,\n",
    "        max_num_neighbors: int = 32,\n",
    "        supernode: bool = False,\n",
    "        offset_strategy: int = 0,\n",
    "        gearnet = False\n",
    "    ):\n",
    "        self.cutoff = cutoff\n",
    "        self.remove_hydrogens = remove_hydrogens\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        self.supernode = supernode\n",
    "        self.offset_strategy = offset_strategy\n",
    "        self.gearnet = gearnet\n",
    "\n",
    "    def __call__(self, item: Dict) -> MyData:\n",
    "        print(\"Using Transform LBA\")\n",
    "        info_root = './output_info/uniprot_dict_all.json'\n",
    "        with open(info_root, 'r') as f:\n",
    "            chain_uniprot_info = json.load(f)\n",
    "        \n",
    "        ligand_df = item[\"atoms_ligand\"]\n",
    "        protein_df = item[\"atoms_protein\"]\n",
    "\n",
    "        if isinstance(ligand_df, pd.DataFrame):\n",
    "            atom_df = pd.concat([protein_df, ligand_df], axis=0)\n",
    "            print('atom_df shape: ',atom_df.shape)\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            print('protein_df shape: ',protein_df.shape)\n",
    "            print('ligand_df shape: ',ligand_df.shape)\n",
    "            print('atom_df shape: ',atom_df.shape)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "            lig_flag[-len(ligand_df):] = 0\n",
    "            print('lig_flag shape: ',lig_flag.shape)\n",
    "            print(atom_df)\n",
    "        else:\n",
    "            atom_df = protein_df\n",
    "            print('protein_df shape: ',protein_df.shape)\n",
    "            # print('ligand_df shape: ',atom_df.shape)\n",
    "            print('atom_df shape: ',atom_df.shape)\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "            print('lig_flag shape: ',lig_flag.shape)\n",
    "        chain_ids = list(set(protein_df['chain']))\n",
    "        uniprot_ids = []\n",
    "        labels = item[\"labels\"]\n",
    "        pf_ids = []\n",
    "        #目前是按照肽链来区分不同的蛋白，为了便于Unprot分类\n",
    "        for i, id in enumerate(chain_ids):\n",
    "            lig_flag[torch.tensor(list(atom_df['chain'] == id))] = i + 1\n",
    "            if '-' in item['complex_id']:\n",
    "                pf_ids.append(0)\n",
    "                break\n",
    "            if id in chain_uniprot_info[item['complex_id']]:\n",
    "                uniprot_id = chain_uniprot_info[item['complex_id']][id]\n",
    "                uniprot_ids.append(uniprot_id)\n",
    "                labels_uniprot = labels['uniprots']\n",
    "                if uniprot_id in labels_uniprot:\n",
    "                    for idx, u in enumerate(labels_uniprot):\n",
    "                        if uniprot_id == u:\n",
    "                            pf_ids.append(idx)\n",
    "                            break\n",
    "                else:\n",
    "                    pf_ids.append(-1)\n",
    "                    print(\"Error, you shouldn't come here!\")\n",
    "            else:\n",
    "                pf_ids.append(-1)\n",
    "        # ec, mf, bp, cc\n",
    "        # num_classes = 538 + 490 + 1944 + 321\n",
    "        num_classes = [538, 490, 1944, 321]\n",
    "        total_classes = 538 + 490 + 1944 + 321\n",
    "        # 找个办法把chain和Uniprot对应起来，然后就可以查了\n",
    "        if self.gearnet:\n",
    "            graph = hetero_graph_transform(\n",
    "                atom_df=atom_df, super_node=self.supernode, flag=lig_flag, protein_seq=item['protein_seq']\n",
    "            )\n",
    "        else:\n",
    "            graph = prot_graph_transform(\n",
    "                atom_df=atom_df, cutoff=self.cutoff, max_num_neighbors=self.max_num_neighbors, flag=lig_flag, super_node=self.supernode, offset_strategy=self.offset_strategy\n",
    "            )\n",
    "        lba = labels['lba']\n",
    "        ppi = labels['ppi']\n",
    "        ec = labels['ec']\n",
    "        go = labels['go']\n",
    "        graph.affinities = torch.FloatTensor([lba, ppi]).unsqueeze(0)\n",
    "        if lba != -1:\n",
    "            graph.affinity_mask = torch.tensor([1, 0]).unsqueeze(0)\n",
    "        elif ppi != -1:\n",
    "            graph.affinity_mask = torch.tensor([0, 1]).unsqueeze(0)\n",
    "        else:\n",
    "            graph.affinity_mask = torch.tensor([0, 0]).unsqueeze(0)\n",
    "\n",
    "        graph.functions = []\n",
    "        graph.valid_masks = []\n",
    "        for i, pf_id in enumerate(pf_ids):\n",
    "            if pf_id == -1:\n",
    "                valid_mask = torch.zeros(len(num_classes))\n",
    "                prop = torch.zeros(total_classes)\n",
    "                graph.functions.append(prop)\n",
    "                graph.valid_masks.append(valid_mask)\n",
    "                continue\n",
    "            valid_mask = torch.ones(len(num_classes))\n",
    "            annotations = []\n",
    "            ec_annot = ec[pf_id]\n",
    "            go_annot = go[pf_id]\n",
    "            if ec_annot == -1:\n",
    "                valid_mask[0] = 0\n",
    "            else:\n",
    "                annotations = annotations + ec_annot\n",
    "            if go_annot == -1:\n",
    "                valid_mask[1:] = 0\n",
    "            else:\n",
    "                mf_annot = go_annot['molecular_functions'] \n",
    "                mf_annot = [j + 538 for j in mf_annot]\n",
    "                if len(mf_annot) == 0:\n",
    "                    valid_mask[1] = 0\n",
    "                bp_annot = go_annot['biological_process']\n",
    "                bp_annot = [j + 538 + 490 for j in bp_annot]\n",
    "                if len(bp_annot) == 0:\n",
    "                    valid_mask[2] = 0\n",
    "                cc_annot = go_annot['cellular_component']\n",
    "                cc_annot = [j + 538 + 490 + 1944 for j in cc_annot]\n",
    "                if len(cc_annot) == 0:\n",
    "                    valid_mask[3] = 0\n",
    "                annotations = annotations + mf_annot + bp_annot + cc_annot\n",
    "                \n",
    "            prop = torch.zeros(total_classes).scatter_(0,torch.tensor(annotations),1)\n",
    "            graph.functions.append(prop)\n",
    "            graph.valid_masks.append(valid_mask)\n",
    "        try:\n",
    "            graph.functions = torch.vstack(graph.functions)\n",
    "            graph.valid_masks = torch.vstack(graph.valid_masks)\n",
    "        except:\n",
    "            print(\"PF ids:\", pf_ids)\n",
    "            print(item['complex_id'], chain_ids, labels)\n",
    "            print(len(graph.functions))\n",
    "            print(pf_ids)\n",
    "            print(graph.functions)\n",
    "            raise RuntimeError\n",
    "        graph.chains = lig_flag[lig_flag!=0]\n",
    "        # print(item['complex_id'])\n",
    "\n",
    "        graph.lig_flag = lig_flag\n",
    "        print(graph.chains.shape)\n",
    "        print(graph.lig_flag.shape)\n",
    "        print(graph.chains.tolist())\n",
    "        print(graph.lig_flag.tolist())\n",
    "        if len(chain_ids) != len(graph.functions):\n",
    "            print(item['complex_id'])\n",
    "            print(chain_ids)\n",
    "            print(len(chain_ids), len(graph.functions))\n",
    "        graph.prot_id = item[\"complex_id\"]\n",
    "        graph.type = 'multi'\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMultiTaskDataset(Dataset):\n",
    "    \"\"\"\n",
    "    The Custom MultiTask Dataset with uniform labels\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: str = './datasets/Multi_task', label_dir: str = './datasets/MultiTask/uniformed_labels.json',\n",
    "                remove_hoh = True, remove_hydrogen = False, cutoff = 6, split : str = 'train', task = 'multi', gearnet = False, alpha_only=False):\n",
    "        super(CustomMultiTaskDataset, self).__init__(root_dir)\n",
    "        print(\"Initializing MultiTask Dataset...\")\n",
    "        self.root_dir = root_dir\n",
    "        self.cache_dir = os.path.join(root_dir, \"{}.cache\".format(split))\n",
    "        with open(label_dir, 'r') as f:\n",
    "            self.labels = json.load(f)\n",
    "        self.remove_hoh = remove_hoh\n",
    "        self.remove_hydrogen = remove_hydrogen # 移除氢\n",
    "        self.cutoff = cutoff\n",
    "        self.gearnet = gearnet\n",
    "        self.alpha_only = alpha_only\n",
    "        file_dir = os.path.join(root_dir, split+'.txt')        \n",
    "        self.ec_root = './data/EnzymeCommission/all'\n",
    "        self.go_root = './data/GeneOntology/all'\n",
    "        self.lba_root = './data/PDBbind/refined-set'\n",
    "        self.pp_root = './data/PDBbind/PP'\n",
    "        with open(file_dir, 'r') as f:\n",
    "            self.files = f.readlines()\n",
    "            self.files = [i.strip() for i in self.files]\n",
    "        if split not in ['train', 'val', 'test', 'train_all','train_ec', 'val_ec', 'test_ec']:\n",
    "            print(\"Wrong selected split. Have to choose between ['train', 'val', 'test', 'test_all']\")\n",
    "            print(\"Exiting code\")\n",
    "            exit()\n",
    "        if task not in ['affinity', 'ec', 'cc', 'mf', 'bp', 'multi', 'go', 'ppi', 'lba']:\n",
    "            print(\"Wrong selected task. Have to choose between ['affinity', 'ec', 'cc', 'mf', 'bp', 'multi', 'go']\")\n",
    "            print(\"Exiting code\")\n",
    "            exit()\n",
    "        self.split = split\n",
    "        self.task = task\n",
    "        self.process_complexes()\n",
    "        print('finish init')\n",
    "    def find_structure(self, item):\n",
    "        self.ec_files = os.listdir(self.ec_root)\n",
    "        self.go_files = os.listdir(self.go_root)\n",
    "        self.lba_files = os.listdir(self.lba_root)\n",
    "        self.pp_files = os.listdir(self.pp_root)\n",
    "        if '-' in item:\n",
    "            if item+'.pdb' in self.ec_files:\n",
    "                return os.path.join(self.ec_root, item+'.pdb'), -1\n",
    "            elif item+'.pdb' in self.go_files:\n",
    "                return os.path.join(self.go_root, item+'.pdb'), -1\n",
    "        else:\n",
    "            if item + '.ent.pdb' in self.pp_files:\n",
    "                return os.path.join(self.pp_root, item+'.ent.pdb'), -1\n",
    "            elif item in self.lba_files:\n",
    "                protein_dir = os.path.join(self.lba_root, item, item + \"_protein.pdb\")\n",
    "                ligand_dir = os.path.join(self.lba_root, item, item + '_ligand.mol2')\n",
    "                return protein_dir, ligand_dir\n",
    "        print(item)\n",
    "        return -1, -1\n",
    "    def gen_df(self, coords, elements):\n",
    "        assert len(coords) == len(elements)\n",
    "        unified_elements = []\n",
    "        xs, ys, zs = [coord[0] for coord in coords], [coord[1] for coord in coords], [coord[2] for coord in coords]\n",
    "        for item in elements:\n",
    "            if item in ['CL', 'Cl', 'Br', 'BR', 'AT', 'At', 'F', 'I']:\n",
    "                element = 'Halogen'\n",
    "            elif item in ['FE', 'ZN', 'MG', 'MN', 'K', 'LI', 'Ca', 'Hg']:\n",
    "                element = 'Metal'\n",
    "            else:\n",
    "                element = item\n",
    "            unified_elements.append(element)\n",
    "        df = pd.DataFrame({'element': unified_elements, 'x': xs, 'y': ys, 'z': zs})\n",
    "        return df\n",
    "    def process_complexes(self):\n",
    "        p = PDBParser(QUIET=True)\n",
    "        nmr_files = []\n",
    "        wrong_number = []\n",
    "        self.processed_complexes = []\n",
    "        corrupted = []\n",
    "        # cache_dir = os.path.join(self.root_dir, self.cache_dir)\n",
    "        if os.path.exists(self.cache_dir):\n",
    "            print(\"Start loading cached Multitask files...\")\n",
    "            self.processed_complexes = pickle.load(open(self.cache_dir, 'rb'))\n",
    "            print(\"Complexes Before Checking:\", self.len())\n",
    "            self.check_dataset()\n",
    "            print(\"Complexes Before Task Selection:\", self.len())\n",
    "            self.choose_task_items()\n",
    "            print(\"Dataset size:\", self.len())\n",
    "            if self.alpha_only:\n",
    "                print(\"Only retaining Alpha Carbon atoms for the atom_df\")\n",
    "                self.retain_alpha_carbon()\n",
    "        else:\n",
    "            print(\"Cache not found! Start processing Multitask files...Total Number {}\".format(len(self.files)))\n",
    "            # count = 0\n",
    "            for score_idx, item in enumerate(tqdm(self.files)):\n",
    "                structure_dir, ligand_dir = self.find_structure(item)\n",
    "                if ligand_dir != -1:\n",
    "                    ligand = next(pybel.readfile('mol2', ligand_dir))\n",
    "                    ligand_coords = [atom.coords for atom in ligand]\n",
    "                    atom_map_lig = [atomic_num_dict(atom.atomicnum) for atom in ligand]\n",
    "                    ligand_df = self.gen_df(ligand_coords, atom_map_lig)\n",
    "                else:\n",
    "                    ligand_df = None\n",
    "                try:\n",
    "                    structure = p.get_structure(item, structure_dir)\n",
    "                except:\n",
    "                    corrupted.append(item)\n",
    "                    continue\n",
    "                # structure = p.get_structure(item, structure_dir)\n",
    "                compound_info = structure.header['compound']\n",
    "                protein_numbers = len(compound_info.items())\n",
    "                \n",
    "                if len(structure) > 1:\n",
    "                    nmr_files.append(item)\n",
    "                    continue\n",
    "                if item not in self.labels:\n",
    "                    wrong_number.append(item)\n",
    "                    continue\n",
    "                model = structure[0]\n",
    "                chains = list(model.get_chains())\n",
    "                pattern = re.compile(r'\\d+H.')\n",
    "                processed_complex = {'complex_id': item, 'num_proteins': protein_numbers, 'labels': self.labels[item],\n",
    "                                    'atoms_protein': [], 'protein_seq': [], 'atoms_ligand':ligand_df}\n",
    "                elements = []\n",
    "                xs = []\n",
    "                ys = []\n",
    "                zs = []\n",
    "                chain_ids = []\n",
    "                protein_seq = []\n",
    "                names = []\n",
    "                resnames = []\n",
    "                # chain = chains[0]\n",
    "                for chain in chains:\n",
    "                    if chain.id == ' ':\n",
    "                        continue\n",
    "                    for residue in chain.get_residues():\n",
    "                        # 删除HOH原子\n",
    "                        if self.remove_hoh and residue.get_resname() == 'HOH':\n",
    "                            continue\n",
    "                        protein_seq.append(residue.get_resname())\n",
    "                        for atom in residue:\n",
    "                            # 删除氢原子\n",
    "                            atom_id = atom.get_id()\n",
    "                            if self.remove_hydrogen and atom.get_id().startswith('H') or pattern.match(atom.get_id()) != None:\n",
    "                                continue\n",
    "                            if atom_id.startswith('H') or pattern.match(atom.get_id()) != None:\n",
    "                                element = 'H'\n",
    "                            elif atom_id[0:2] in ['CL', 'Cl', 'Br', 'BR', 'AT', 'At']:\n",
    "                                element = 'Halogen'\n",
    "                            elif atom_id[0:2] in ['FE', 'ZN', 'MG', 'MN', 'K', 'LI']:\n",
    "                                element = 'Metal'\n",
    "                            elif atom_id[0] in ['F', 'I']:\n",
    "                                element = 'Halogen'\n",
    "                            elif atom_id[0] in ['C','N','O','S','P']:\n",
    "                                element = atom_id[0]\n",
    "                            else:\n",
    "                                element = atom_id\n",
    "                            names.append(atom_id)\n",
    "                            elements.append(element)\n",
    "                            chain_ids.append(chain.id)\n",
    "                            resnames.append(residue.get_resname())\n",
    "                            x, y, z = atom.get_vector()\n",
    "                            xs.append(x)\n",
    "                            ys.append(y)\n",
    "                            zs.append(z)\n",
    "                protein_df = pd.DataFrame({'chain': chain_ids, 'resname': resnames, 'element': elements, 'name': names, 'x': xs, 'y': ys, 'z': zs})\n",
    "                processed_complex['atoms_protein'] = protein_df\n",
    "                processed_complex['protein_seq'] = protein_seq\n",
    "                \n",
    "                self.processed_complexes.append(processed_complex)\n",
    "                # count += 1\n",
    "                # if count == 128:\n",
    "                #     break\n",
    "            print(\"Structure processed Done, dumping...\")\n",
    "            print(\"Structures with Wrong numbers:\", len(wrong_number), wrong_number)\n",
    "            print(\"Structures with NMR methods:\", len(nmr_files), nmr_files)\n",
    "            print(\"Corrupted:\", len(corrupted), corrupted)\n",
    "            pickle.dump(self.processed_complexes, open(self.cache_dir, 'wb'))\n",
    "            print(\"Complexes Before Checking:\", self.len())\n",
    "            self.check_dataset()\n",
    "            print(\"Complexes Before Task Selection:\", self.len())\n",
    "            self.choose_task_items()\n",
    "            print(\"Dataset size:\", self.len())\n",
    "            if self.alpha_only:\n",
    "                print(\"Only retaining Alpha Carbon atoms for the atom_df\")\n",
    "                self.retain_alpha_carbon()\n",
    "    def correctness_check(self, chain_uniprot_info, complx):\n",
    "        #Annotation Correctness Check\n",
    "        correct = True\n",
    "        chain_ids = list(set(complx['atoms_protein']['chain']))\n",
    "        if '-' not in complx['complex_id']:\n",
    "            for i, id in enumerate(chain_ids):\n",
    "                if id in chain_uniprot_info[complx['complex_id']]:\n",
    "                    uniprot_id = chain_uniprot_info[complx['complex_id']][id]\n",
    "                    labels_uniprot = complx['labels']['uniprots']\n",
    "                    if uniprot_id not in labels_uniprot:\n",
    "                        print(\"Error, you shouldn't come here!\")\n",
    "                        correct = False\n",
    "                        print(complx['complex_id'], chain_ids, chain_uniprot_info[complx['complex_id']])\n",
    "        return correct\n",
    "    def cal_length_thres(self, complxes):\n",
    "        length_list = []\n",
    "        for complx in complxes:\n",
    "            length = len(complx['atoms_protein']['element'])\n",
    "            length_list.append(length)\n",
    "        sorted_list = sorted(length_list)\n",
    "        thres = sorted_list[int(0.95*len(sorted_list))]\n",
    "        print(\"Cutting Threshold of atom numbers:\", thres)\n",
    "        return thres\n",
    "    def length_check(self, complx, thres):\n",
    "        if len(complx['atoms_protein']['element']) > thres:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    def check_dataset(self):\n",
    "        print(\"Checking the dataset...\")\n",
    "        info_root = './output_info/uniprot_dict_all.json'\n",
    "        with open(info_root, 'r') as f:\n",
    "            chain_uniprot_info = json.load(f)\n",
    "        thres = self.cal_length_thres(self.processed_complexes)\n",
    "        if self.split == 'train':\n",
    "            thres = 6712\n",
    "        self.processed_complexes = [i for i in tqdm(self.processed_complexes) if self.length_check(i, thres) and self.correctness_check(chain_uniprot_info, i)]\n",
    "    \n",
    "    def retain_alpha_carbon(self):\n",
    "        new_complexes = []\n",
    "        for item in self.processed_complexes:\n",
    "            protein_df = item['atoms_protein']\n",
    "            # print(\"Original Nodes:\", len(protein_df))\n",
    "            new_protein_df = protein_df[protein_df.name == 'CA'].reset_index(drop=True)\n",
    "            item['atoms_protein'] = new_protein_df\n",
    "            # print(\"Retaining Alpha Carbons:\", len(new_protein_df))\n",
    "            new_complexes.append(item)\n",
    "        self.processed_complexes = new_complexes\n",
    "    \n",
    "    def choose_task_items(self):\n",
    "        # 根据不同的任务，训练单独的模型\n",
    "        if self.split in ['train', 'val', 'test']:\n",
    "            extra_dir = './datasets/MultiTask/{}.txt'.format(self.split)\n",
    "            with open(extra_dir, 'r') as f:\n",
    "                extra_info = f.readlines()\n",
    "                extra_info = [i.strip() for i in extra_info]\n",
    "        else:\n",
    "            extra_info = []\n",
    "        if self.task == 'ec':\n",
    "            print(\"Using Enzyme Commission Dataset for training:\")\n",
    "            root_dir = './output_info/enzyme_commission_uniprots.json'\n",
    "            with open(root_dir, 'r') as f:\n",
    "                info_dict = json.load(f)\n",
    "            new_complexes = []\n",
    "            for item in self.processed_complexes:\n",
    "                if item['complex_id'] in info_dict or item['complex_id'] in extra_info: #.keys()?\n",
    "                    labels = item['labels']\n",
    "                    annot_number = len(labels['uniprots'])\n",
    "                    for j in range(annot_number):\n",
    "                        labels['go'][j] = -1\n",
    "                    item['labels'] = labels\n",
    "                    new_complexes.append(item)\n",
    "            self.processed_complexes = new_complexes\n",
    "            self.transform_func = GNNTransformEC(task=self.task, gearnet=self.gearnet)\n",
    "            print(\"Using EC dataset and transformation\")\n",
    "        elif self.task in ['bp', 'mf', 'cc', 'go']:\n",
    "            print(\"Using Gene Ontology {} Dataset for training:\".format(self.split))\n",
    "            root_dir = './output_info/gene_ontology_uniprots.json'\n",
    "            with open(root_dir, 'r') as f:\n",
    "                info_dict = json.load(f)\n",
    "            new_complexes = []\n",
    "            for item in self.processed_complexes:\n",
    "                if item['complex_id'] in info_dict or item['complex_id'] in extra_info:\n",
    "                    labels = item['labels']\n",
    "                    annot_number = len(labels['uniprots'])\n",
    "                    for j in range(annot_number):\n",
    "                        labels['ec'][j] = -1\n",
    "                    item['labels'] = labels\n",
    "                    new_complexes.append(item)\n",
    "            self.processed_complexes = new_complexes\n",
    "            self.transform_func = GNNTransformGO(task=self.task, gearnet=self.gearnet)\n",
    "        elif self.task in ['affinity', 'lba', 'ppi']:\n",
    "            print(\"Using Affinity Dataset for training:\")\n",
    "            root_dir = './output_info/protein_protein_uniprots.json'\n",
    "            with open(root_dir, 'r') as f:\n",
    "                info_dict1 = json.load(f)\n",
    "            root_dir2 = './output_info/protein_ligand_uniprots.json'\n",
    "            with open(root_dir2, 'r') as f:\n",
    "                info_dict2 = json.load(f)\n",
    "            info_dict = {**info_dict1, **info_dict2}\n",
    "            new_complexes = []\n",
    "            for item in self.processed_complexes:\n",
    "                if item['complex_id'] in info_dict or item['complex_id'] in extra_info:\n",
    "                    labels = item['labels']\n",
    "                    annot_number = len(labels['uniprots'])\n",
    "                    for j in range(annot_number):\n",
    "                        labels['ec'][j] = -1\n",
    "                        labels['go'][j] = -1\n",
    "                    item['labels'] = labels\n",
    "                    new_complexes.append(item)\n",
    "            self.processed_complexes = new_complexes\n",
    "            self.transform_func = GNNTransformAffinity(task=self.task, gearnet=self.gearnet)\n",
    "        else:\n",
    "            self.transform_func = GNNTransformMultiTask(gearnet=self.gearnet)\n",
    "    def len(self):\n",
    "        return len(self.processed_complexes)\n",
    "    def get(self, idx):\n",
    "        return self.transform_func(self.processed_complexes[idx])\n",
    "    def print_complexes(self, idx):\n",
    "        return self.processed_complexes[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_functions = None\n",
    "class GNNTransformGO(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cutoff: float = 4.5,\n",
    "        remove_hydrogens: bool = True,\n",
    "        max_num_neighbors: int = 32,\n",
    "        supernode: bool = False,\n",
    "        offset_strategy: int = 0,\n",
    "        task='bp', #可能是bp, mf, cc中的一个\n",
    "        gearnet=False\n",
    "    ):\n",
    "        self.cutoff = cutoff\n",
    "        self.remove_hydrogens = remove_hydrogens\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        self.supernode = supernode\n",
    "        self.offset_strategy = offset_strategy\n",
    "        self.task = task\n",
    "        self.gearnet = gearnet\n",
    "        # print(\"GNNtransforgo\")\n",
    "\n",
    "    def __call__(self, item: Dict) -> MyData:\n",
    "        info_root = './output_info/uniprot_dict_all.json'\n",
    "        with open(info_root, 'r') as f:\n",
    "            chain_uniprot_info = json.load(f)\n",
    "        # print(\"Using Transform {}\".format(self.task))\n",
    "        ligand_df = item['atoms_ligand']\n",
    "        protein_df = item[\"atoms_protein\"]\n",
    "        atom_df = protein_df\n",
    "        if isinstance(ligand_df, pd.DataFrame):\n",
    "            atom_df = pd.concat([protein_df, ligand_df], axis=0)\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "            lig_flag[-len(ligand_df):] = 0\n",
    "        else:\n",
    "            atom_df = protein_df\n",
    "            if self.remove_hydrogens:\n",
    "                # remove hydrogens\n",
    "                atom_df = atom_df[atom_df.element != \"H\"].reset_index(drop=True)\n",
    "            lig_flag = torch.zeros(atom_df.shape[0], dtype=torch.long)\n",
    "        chain_ids = list(set(protein_df['chain']))\n",
    "        uniprot_ids = []\n",
    "        labels = item[\"labels\"]\n",
    "        pf_ids = []\n",
    "        #目前是按照肽链来区分不同的蛋白，为了便于Unprot分类\n",
    "        for i, id in enumerate(chain_ids):\n",
    "            lig_flag[torch.tensor(list(atom_df['chain'] == id))] = i + 1\n",
    "            if '-' in item['complex_id']:\n",
    "                pf_ids.append(0)\n",
    "                break\n",
    "            if id in chain_uniprot_info[item['complex_id']]:\n",
    "                uniprot_id = chain_uniprot_info[item['complex_id']][id]\n",
    "                uniprot_ids.append(uniprot_id)\n",
    "                labels_uniprot = labels['uniprots']\n",
    "                if uniprot_id in labels_uniprot:\n",
    "                    for idx, u in enumerate(labels_uniprot):\n",
    "                        if uniprot_id == u:\n",
    "                            pf_ids.append(idx)\n",
    "                            break\n",
    "                else:\n",
    "                    pf_ids.append(-1)\n",
    "                    print(\"Error, you shouldn't come here!\")\n",
    "            else:\n",
    "                pf_ids.append(-1)\n",
    "        # print(\"pf_ids\", pf_ids)        \n",
    "        if self.task == 'mf':\n",
    "            num_classes = 490\n",
    "        elif self.task == 'bp':\n",
    "            num_classes = 1944\n",
    "        elif self.task == 'cc':\n",
    "            num_classes = 321\n",
    "        elif self.task == 'go':\n",
    "            num_classes = 490 + 1944 + 321\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "        # 找个办法把chain和Uniprot对应起来，然后就可以查了\n",
    "        if self.gearnet:\n",
    "            graph = hetero_graph_transform(\n",
    "                atom_df=atom_df, super_node=self.supernode, flag=lig_flag, protein_seq=item['protein_seq']\n",
    "            )\n",
    "        else:\n",
    "            graph = prot_graph_transform(\n",
    "                atom_df=atom_df, cutoff=self.cutoff, max_num_neighbors=self.max_num_neighbors, flag=lig_flag, super_node=self.supernode, offset_strategy=self.offset_strategy\n",
    "            )\n",
    "        go = labels['go']\n",
    "        # graph.y = torch.zeros(self.num_classes).scatter_(0,torch.tensor(labels),1)\n",
    "        graph.functions = []\n",
    "        graph.valid_masks = []\n",
    "        \n",
    "        for i, pf_id in enumerate(pf_ids):\n",
    "            if pf_id == -1:\n",
    "                valid_mask = torch.zeros(num_classes)\n",
    "                prop = torch.zeros(num_classes)\n",
    "                graph.functions.append(prop)\n",
    "                graph.valid_masks.append(valid_mask)\n",
    "                continue\n",
    "            valid_mask = torch.ones(num_classes)\n",
    "            annotations = []\n",
    "            go_annot = go[pf_id]\n",
    "            if self.task == 'mf':\n",
    "                mf_annot = go_annot['molecular_functions'] \n",
    "                mf_annot = [j for j in mf_annot]\n",
    "                if len(mf_annot) == 0:\n",
    "                    valid_mask[:] = 0\n",
    "                annotations = mf_annot\n",
    "            elif self.task == 'bp':\n",
    "                bp_annot = go_annot['biological_process']\n",
    "                bp_annot = [j for j in bp_annot]\n",
    "                if len(bp_annot) == 0:\n",
    "                    valid_mask[:] = 0\n",
    "                annotations = bp_annot\n",
    "            elif self.task == 'cc':\n",
    "                cc_annot = go_annot['cellular_component']\n",
    "                cc_annot = [j for j in cc_annot]\n",
    "                if len(cc_annot) == 0:\n",
    "                    valid_mask[:] = 0\n",
    "                annotations = cc_annot\n",
    "            elif self.task == 'go':\n",
    "                mf_annot = go_annot['molecular_functions'] \n",
    "                mf_annot = [j for j in mf_annot]\n",
    "                if len(mf_annot) == 0:\n",
    "                    valid_mask[: 490] = 0\n",
    "                bp_annot = go_annot['biological_process']\n",
    "                bp_annot = [j + 490 for j in bp_annot]\n",
    "                if len(bp_annot) == 0:\n",
    "                    valid_mask[490: 490+1944] = 0\n",
    "                cc_annot = go_annot['cellular_component']\n",
    "                cc_annot = [j+490+1944 for j in cc_annot]\n",
    "                if len(cc_annot) == 0:\n",
    "                    valid_mask[490+1944: ] = 0\n",
    "                annotations = mf_annot + bp_annot + cc_annot\n",
    "                \n",
    "            prop = torch.zeros(num_classes).scatter_(0,torch.tensor(annotations),1)\n",
    "            graph.functions.append(prop)\n",
    "            graph.valid_masks.append(valid_mask)\n",
    "        try:\n",
    "            # print(annotations)\n",
    "            \n",
    "            # my_functions = copy.deepcopy(graph.functions)\n",
    "            # print(my_functions)\n",
    "            graph.functions = torch.vstack(graph.functions)\n",
    "            graph.valid_masks = torch.vstack(graph.valid_masks)\n",
    "            # print(\"386\",graph.functions[0,386])\n",
    "        except:\n",
    "            print(\"PF ids:\", pf_ids)\n",
    "            print(item['complex_id'], chain_ids, labels)\n",
    "            print(len(graph.functions))\n",
    "            print(pf_ids)\n",
    "            print(graph.functions)\n",
    "            raise RuntimeError\n",
    "    \n",
    "        graph.chains = lig_flag[lig_flag!=0]\n",
    "\n",
    "        graph.lig_flag = lig_flag\n",
    "        if len(chain_ids) != len(graph.functions):\n",
    "            print(item['complex_id'])\n",
    "            print(chain_ids)\n",
    "            print(len(chain_ids), len(graph.functions))\n",
    "        print(\"functions len\",len(graph.functions))\n",
    "        print(\"functions\", graph.functions)\n",
    "        graph.prot_id = item[\"complex_id\"]\n",
    "        graph.type = self.task\n",
    "        print(graph.lig_flag.tolist())\n",
    "        print(graph.chains.tolist())\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from argparse import ArgumentParser\n",
    "from typing import Dict, List, Tuple, Union\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from atom3d.datasets import deserialize\n",
    "from Bio.PDB import PDBParser\n",
    "import re\n",
    "import pickle\n",
    "import io\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import lmdb\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import openbabel\n",
    "from openbabel import pybel\n",
    "\n",
    "from utils import MyData, prot_graph_transform, hetero_graph_transform\n",
    "\n",
    "pybel.ob.obErrorLog.SetOutputLevel(0)\n",
    "atomic_num_dict = lambda x: {1: 'H', 2: 'HE', 3: 'LI', 4: 'BE', 5: 'B', 6: 'C', 7: 'N', 8: 'O', 9: 'F', 11: 'NA',\n",
    "                   15: 'P', 16: 'S', 17: 'Cl', 20:'Ca', 25: 'MN', 26: 'FE', 30: 'ZN', 35: 'Br', 53: 'I', 80: 'Hg'}.get(x, 'Others')\n",
    "\n",
    "class CustomMultiTaskDataset(Dataset):\n",
    "    \"\"\"\n",
    "    The Custom MultiTask Dataset with uniform labels\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: str = './datasets/MultiTask', label_dir: str = './datasets/MultiTask/uniformed_labels.json',\n",
    "                remove_hoh = True, remove_hydrogen = False, cutoff = 6, split : str = 'train', task = 'multi', gearnet = False, alpha_only=False):\n",
    "        super(CustomMultiTaskDataset, self).__init__(root_dir)\n",
    "        print(\"in\")\n",
    "        print(\"in Initializing MultiTask Dataset...\")\n",
    "        self.root_dir = root_dir\n",
    "        self.cache_dir = os.path.join(root_dir, \"{}.cache\".format(split))\n",
    "        with open(label_dir, 'r') as f:\n",
    "            self.labels = json.load(f)\n",
    "        self.remove_hoh = remove_hoh\n",
    "        self.remove_hydrogen = remove_hydrogen # 移除氢\n",
    "        self.cutoff = cutoff\n",
    "        self.gearnet = gearnet\n",
    "        self.alpha_only = alpha_only\n",
    "        file_dir = os.path.join(root_dir, split+'.txt')        \n",
    "        self.ec_root = './data/EnzymeCommission/all'\n",
    "        # self.ec_root = './data/EC/all'\n",
    "        self.go_root = './data/GeneOntology/all'\n",
    "        # self.go_root = './data/GO/all'\n",
    "        self.lba_root = './data/PDBbind/refined-set'\n",
    "        self.pp_root = './data/PDBbind/PP'\n",
    "        with open(file_dir, 'r') as f:\n",
    "            self.files = f.readlines()\n",
    "            self.files = [i.strip() for i in self.files]\n",
    "        if split not in ['train', 'val', 'test', 'train_all','train_ec', 'val_ec', 'test_ec']:\n",
    "            print(\"Wrong selected split. Have to choose between ['train', 'val', 'test', 'test_all']\")\n",
    "            print(\"Exiting code\")\n",
    "            exit()\n",
    "        if task not in ['affinity', 'ec', 'cc', 'mf', 'bp', 'multi', 'go', 'lba', 'ppi']:\n",
    "            print(\"Wrong selected task. Have to choose between ['affinity', 'ec', 'cc', 'mf', 'bp', 'multi', 'go']\")\n",
    "            print(\"Exiting code\")\n",
    "            exit()\n",
    "        self.split = split\n",
    "        self.task = task\n",
    "        self.process_complexes()\n",
    "    def find_structure(self, item):\n",
    "        self.ec_files = os.listdir(self.ec_root)\n",
    "        self.go_files = os.listdir(self.go_root)\n",
    "        self.lba_files = os.listdir(self.lba_root)\n",
    "        self.pp_files = os.listdir(self.pp_root)\n",
    "        pdb_files = os.listdir('./data/EC/all')\n",
    "        if '-' in item:\n",
    "            if item +'.pdb' in self.ec_files:\n",
    "                return os.path.join(self.ec_root, item+'.pdb'), -1\n",
    "            elif item+'.pdb' in self.go_files:\n",
    "                return os.path.join(self.go_root, item+'.pdb'), -1\n",
    "            elif item.split('-')[0] +'.pdb.gz' in self.ec_files:\n",
    "                return os.path.join(self.ec_root, item.split('-')[0] +'.pdb.gz'), -1\n",
    "            elif item.split('-')[0] +'.pdb' in self.ec_files:\n",
    "                return os.path.join(self.ec_root, item.split('-')[0] +'.pdb'), -1\n",
    "            elif item.split('-')[0] +'.pdb.gz' in self.go_files:\n",
    "                return os.path.join(self.go_root, item.split('-')[0] +'.pdb.gz'), -1\n",
    "            elif item.split('-')[0] +'.pdb' in self.go_files:\n",
    "                return os.path.join(self.go_root, item.split('-')[0] +'.pdb'), -1\n",
    "            elif item.split('-')[0] +'.pdb.gz' in pdb_files:\n",
    "                return os.path.join('./data/EC/all', item.split('-')[0] +'.pdb.gz'), -1\n",
    "            elif item.split('-')[0] +'.pdb' in pdb_files:\n",
    "                return os.path.join('./data/EC/all', item.split('-')[0] +'.pdb'), -1\n",
    "        else:\n",
    "            if item + '.ent.pdb' in self.pp_files:\n",
    "                return os.path.join(self.pp_root, item+'.ent.pdb'), -1\n",
    "            elif item in self.lba_files:\n",
    "                protein_dir = os.path.join(self.lba_root, item, item + \"_protein.pdb\")\n",
    "                ligand_dir = os.path.join(self.lba_root, item, item + '_ligand.mol2')\n",
    "                return protein_dir, ligand_dir\n",
    "        print(\"item\", item)\n",
    "        return -1, -1\n",
    "    def gen_df(self, coords, elements):\n",
    "        assert len(coords) == len(elements)\n",
    "        unified_elements = []\n",
    "        xs, ys, zs = [coord[0] for coord in coords], [coord[1] for coord in coords], [coord[2] for coord in coords]\n",
    "        for item in elements:\n",
    "            if item in ['CL', 'Cl', 'Br', 'BR', 'AT', 'At', 'F', 'I']:\n",
    "                element = 'Halogen'\n",
    "            elif item in ['FE', 'ZN', 'MG', 'MN', 'K', 'LI', 'Ca', 'Hg']:\n",
    "                element = 'Metal'\n",
    "            else:\n",
    "                element = item\n",
    "            unified_elements.append(element)\n",
    "        df = pd.DataFrame({'element': unified_elements, 'x': xs, 'y': ys, 'z': zs})\n",
    "        return df\n",
    "    def process_complexes(self):\n",
    "        p = PDBParser(QUIET=True)\n",
    "        nmr_files = []\n",
    "        wrong_number = []\n",
    "        self.processed_complexes = []\n",
    "        corrupted = []\n",
    "        # cache_dir = os.path.join(self.root_dir, self.cache_dir)\n",
    "        if os.path.exists(self.cache_dir):\n",
    "            print(\"Start loading cached Multitask files...\")\n",
    "            self.processed_complexes = pickle.load(open(self.cache_dir, 'rb'))\n",
    "            print(\"Complexes Before Checking:\", self.len())\n",
    "            self.check_dataset()\n",
    "            print(\"Complexes Before Task Selection:\", self.len())\n",
    "            self.choose_task_items()\n",
    "            print(\"Dataset size:\", self.len())\n",
    "            if self.alpha_only:\n",
    "                print(\"Only retaining Alpha Carbon atoms for the atom_df\")\n",
    "                self.retain_alpha_carbon()\n",
    "        else:\n",
    "            print(\"Cache not found! Start processing Multitask files...Total Number {}\".format(len(self.files)))\n",
    "            count = 0\n",
    "            for score_idx, item in enumerate(tqdm(self.files)):\n",
    "                structure_dir, ligand_dir = self.find_structure(item)\n",
    "                if ligand_dir != -1:\n",
    "                    ligand = next(pybel.readfile('mol2', ligand_dir))\n",
    "                    ligand_coords = [atom.coords for atom in ligand]\n",
    "                    atom_map_lig = [atomic_num_dict(atom.atomicnum) for atom in ligand]\n",
    "                    ligand_df = self.gen_df(ligand_coords, atom_map_lig)\n",
    "                else:\n",
    "                    ligand_df = None\n",
    "                split_chain_flag = False\n",
    "                # print(structure_dir)\n",
    "                file_is_chain = False\n",
    "                have_chain_id = False\n",
    "                pdb_id = None\n",
    "                chain_id = None\n",
    "                try:\n",
    "                    if '-' in structure_dir:\n",
    "                        file_is_chain = True\n",
    "                    if '.gz' in structure_dir:\n",
    "                        if '-' in item:\n",
    "                            pdb_id, chain_id = item.split('-')\n",
    "                            file_is_chain = False\n",
    "                            have_chain_id = True\n",
    "                        file_handle = gzip.open(structure_dir, 'rt')\n",
    "                        split_chain_flag = True\n",
    "                    else:\n",
    "                        file_handle = structure_dir\n",
    "                    structure = p.get_structure(item, file_handle)\n",
    "                except:\n",
    "                    corrupted.append(item)\n",
    "                    continue\n",
    "                # structure = p.get_structure(item, structure_dir)\n",
    "                compound_info = structure.header['compound']\n",
    "                protein_numbers = len(compound_info.items())\n",
    "                \n",
    "                if len(structure) > 1:\n",
    "                    nmr_files.append(item)\n",
    "                    continue\n",
    "                if item not in self.labels:\n",
    "                    wrong_number.append(item)\n",
    "                    continue\n",
    "                model = structure[0]\n",
    "                chains = list(model.get_chains())\n",
    "                if file_is_chain:\n",
    "                    chain = chains[0]\n",
    "                else:\n",
    "                    if have_chain_id:\n",
    "                        for chain in chains:\n",
    "                            if chain.id == chain_id:\n",
    "                                break\n",
    "                if have_chain_id:\n",
    "                    if chain.id != chain_id:\n",
    "                        print('cannot find chain:', pdb_id, chain_id)\n",
    "                chains = [chain]\n",
    "                pattern = re.compile(r'\\d+H.')\n",
    "                processed_complex = {'complex_id': item, 'num_proteins': protein_numbers, 'labels': self.labels[item],\n",
    "                                    'atoms_protein': [], 'protein_seq': [], 'atoms_ligand':ligand_df}\n",
    "                elements = []\n",
    "                xs = []\n",
    "                ys = []\n",
    "                zs = []\n",
    "                chain_ids = []\n",
    "                protein_seq = []\n",
    "                names = []\n",
    "                resnames = []\n",
    "                # chain = chains[0]\n",
    "                for chain in chains:\n",
    "                    if chain.id == ' ':\n",
    "                        continue\n",
    "                    for residue in chain.get_residues():\n",
    "                        # 删除HOH原子\n",
    "                        if self.remove_hoh and residue.get_resname() == 'HOH':\n",
    "                            continue\n",
    "                        protein_seq.append(residue.get_resname())\n",
    "                        for atom in residue:\n",
    "                            # 删除氢原子\n",
    "                            atom_id = atom.get_id()\n",
    "                            if self.remove_hydrogen and atom.get_id().startswith('H') or pattern.match(atom.get_id()) != None:\n",
    "                                continue\n",
    "                            if atom_id.startswith('H') or pattern.match(atom.get_id()) != None:\n",
    "                                element = 'H'\n",
    "                            elif atom_id[0:2] in ['CL', 'Cl', 'Br', 'BR', 'AT', 'At']:\n",
    "                                element = 'Halogen'\n",
    "                            elif atom_id[0:2] in ['FE', 'ZN', 'MG', 'MN', 'K', 'LI']:\n",
    "                                element = 'Metal'\n",
    "                            elif atom_id[0] in ['F', 'I']:\n",
    "                                element = 'Halogen'\n",
    "                            elif atom_id[0] in ['C','N','O','S','P']:\n",
    "                                element = atom_id[0]\n",
    "                            else:\n",
    "                                element = atom_id\n",
    "                            names.append(atom_id)\n",
    "                            elements.append(element)\n",
    "                            chain_ids.append(chain.id)\n",
    "                            resnames.append(residue.get_resname())\n",
    "                            x, y, z = atom.get_vector()\n",
    "                            xs.append(x)\n",
    "                            ys.append(y)\n",
    "                            zs.append(z)\n",
    "                protein_df = pd.DataFrame({'chain': chain_ids, 'resname': resnames, 'element': elements, 'name': names, 'x': xs, 'y': ys, 'z': zs})\n",
    "                processed_complex['atoms_protein'] = protein_df\n",
    "                processed_complex['protein_seq'] = protein_seq\n",
    "                # print(processed_complex)\n",
    "                self.processed_complexes.append(processed_complex)\n",
    "                count += 1\n",
    "                # print(\"count:\", count)\n",
    "                # print(len(processed_complexes))\n",
    "                # if count == 128:\n",
    "                #     break\n",
    "            print(len(self.processed_complexes))\n",
    "            print(\"Structure processed Done, dumping...\")\n",
    "            print(\"Structures with Wrong numbers:\", len(wrong_number), wrong_number)\n",
    "            print(\"Structures with NMR methods:\", len(nmr_files), nmr_files)\n",
    "            print(\"Corrupted:\", len(corrupted), corrupted)\n",
    "            pickle.dump(self.processed_complexes, open(self.cache_dir, 'wb'))\n",
    "            print(\"Complexes Before Checking:\", self.len())\n",
    "            self.check_dataset()\n",
    "            print(\"Complexes Before Task Selection:\", self.len())\n",
    "            self.choose_task_items()\n",
    "            print(\"Dataset size:\", self.len())\n",
    "            if self.alpha_only:\n",
    "                print(\"Only retaining Alpha Carbon atoms for the atom_df\")\n",
    "                self.retain_alpha_carbon()\n",
    "    def correctness_check(self, chain_uniprot_info, complx):\n",
    "        #Annotation Correctness Check\n",
    "        correct = True\n",
    "        chain_ids = list(set(complx['atoms_protein']['chain']))\n",
    "        if '-' not in complx['complex_id']:\n",
    "            # if complx['complex_id'] == '4p3y':\n",
    "            #     print(complx['complex_id'], id, uniprot_id, chain_ids, chain_uniprot_info[complx['complex_id']])\n",
    "            #     print(labels_uniprot)\n",
    "            for i, id in enumerate(chain_ids):\n",
    "                if id in chain_uniprot_info[complx['complex_id']]:\n",
    "                    uniprot_id = chain_uniprot_info[complx['complex_id']][id]\n",
    "                    labels_uniprot = complx['labels']['uniprots']\n",
    "                    if uniprot_id not in labels_uniprot:\n",
    "                        print(\"Error, you shouldn't come here!\")\n",
    "                        correct = False\n",
    "                        print(complx['complex_id'],id, uniprot_id, chain_ids, chain_uniprot_info[complx['complex_id']])\n",
    "                        print(labels_uniprot)\n",
    "        return correct\n",
    "    def cal_length_thres(self, complxes):\n",
    "        length_list = []\n",
    "        for complx in complxes:\n",
    "            length = len(complx['atoms_protein']['element'])\n",
    "            length_list.append(length)\n",
    "        sorted_list = sorted(length_list)\n",
    "        thres = sorted_list[int(0.95*len(sorted_list))]\n",
    "        print(\"Cutting Threshold of atom numbers:\", thres)\n",
    "        return thres\n",
    "    def length_check(self, complx, thres):\n",
    "        if len(complx['atoms_protein']['element']) > thres:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    def check_dataset(self):\n",
    "        print(\"Checking the dataset...\")\n",
    "        info_root = './output_info/uniprot_dict_all_go.json'\n",
    "        with open(info_root, 'r') as f:\n",
    "            chain_uniprot_info = json.load(f)\n",
    "        thres = self.cal_length_thres(self.processed_complexes)\n",
    "        if self.split == 'train':\n",
    "            thres = 6712\n",
    "\n",
    "        self.processed_complexes = [i for i in tqdm(self.processed_complexes) if self.length_check(i, thres) and self.correctness_check(chain_uniprot_info, i)]\n",
    "    \n",
    "    def retain_alpha_carbon(self):\n",
    "        new_complexes = []\n",
    "        for item in self.processed_complexes:\n",
    "            protein_df = item['atoms_protein']\n",
    "            # print(\"Original Nodes:\", len(protein_df))\n",
    "            new_protein_df = protein_df[protein_df.name == 'CA'].reset_index(drop=True)\n",
    "            item['atoms_protein'] = new_protein_df\n",
    "            # print(\"Retaining Alpha Carbons:\", len(new_protein_df))\n",
    "            new_complexes.append(item)\n",
    "        self.processed_complexes = new_complexes\n",
    "    \n",
    "    def choose_task_items(self):\n",
    "        # 根据不同的任务，训练单独的模型\n",
    "        if self.split in ['train', 'val', 'test']:\n",
    "            extra_dir = './datasets/MultiTask/{}.txt'.format(self.split)\n",
    "            with open(extra_dir, 'r') as f:\n",
    "                extra_info = f.readlines()\n",
    "                extra_info = [i.strip() for i in extra_info]\n",
    "        else:\n",
    "            extra_info = []\n",
    "        if self.task == 'ec':\n",
    "            print(\"Using Enzyme Commission Dataset for training:\")\n",
    "            root_dir = './output_info/enzyme_commission_uniprots.json'\n",
    "            with open(root_dir, 'r') as f:\n",
    "                info_dict = json.load(f)\n",
    "            new_complexes = []\n",
    "            for item in self.processed_complexes:\n",
    "                if item['complex_id'] in info_dict or item['complex_id'] in extra_info: #.keys()?\n",
    "                    labels = item['labels']\n",
    "                    annot_number = len(labels['uniprots'])\n",
    "                    for j in range(annot_number):\n",
    "                        labels['go'][j] = -1\n",
    "                    item['labels'] = labels\n",
    "                    new_complexes.append(item)\n",
    "            self.processed_complexes = new_complexes\n",
    "            self.transform_func = GNNTransformEC(task=self.task, gearnet=self.gearnet)\n",
    "            print(\"Using EC dataset and transformation\")\n",
    "        elif self.task in ['bp', 'mf', 'cc', 'go']:\n",
    "            print(\"Using Gene Ontology {} Dataset for training:\".format(self.split))\n",
    "            # root_dir = './output_info/gene_ontology_uniprots.json'\n",
    "            root_dir = './output_info/go_task_uniprots.json'\n",
    "            with open(root_dir, 'r') as f:\n",
    "                info_dict = json.load(f)\n",
    "            new_complexes = []\n",
    "            print(\"ok\")\n",
    "            for item in self.processed_complexes:\n",
    "                if item['complex_id'] in info_dict or item['complex_id'] in extra_info:\n",
    "                    labels = item['labels']\n",
    "                    annot_number = len(labels['uniprots'])\n",
    "                    for j in range(annot_number):\n",
    "                        labels['ec'][j] = -1\n",
    "                    item['labels'] = labels\n",
    "                    new_complexes.append(item)\n",
    "            self.processed_complexes = new_complexes\n",
    "            print([complx['complex_id'] for complx in new_complexes])\n",
    "            self.transform_func = GNNTransformGO(task=self.task, gearnet=self.gearnet)\n",
    "        elif self.task in ['affinity', 'lba', 'ppi']:\n",
    "            print(\"Using Affinity Dataset for trainisng:\")\n",
    "            root_dir = './output_info/protein_protein_uniprots.json'\n",
    "            with open(root_dir, 'r') as f:\n",
    "                info_dict1 = json.load(f)\n",
    "            root_dir2 = './output_info/protein_ligand_uniprots.json'\n",
    "            with open(root_dir2, 'r') as f:\n",
    "                info_dict2 = json.load(f)\n",
    "            info_dict = {**info_dict1, **info_dict2}\n",
    "            new_complexes = []\n",
    "            for item in self.processed_complexes:\n",
    "                if item['complex_id'] in info_dict or item['complex_id'] in extra_info:\n",
    "                    labels = item['labels']\n",
    "                    annot_number = len(labels['uniprots'])\n",
    "                    for j in range(annot_number):\n",
    "                        labels['ec'][j] = -1\n",
    "                        labels['go'][j] = -1\n",
    "                    item['labels'] = labels\n",
    "                    new_complexes.append(item)\n",
    "            self.processed_complexes = new_complexes\n",
    "            # print(new_complexes)\n",
    "            self.transform_func = GNNTransformAffinity(task=self.task, gearnet=self.gearnet)\n",
    "        else:\n",
    "            print([complx['complex_id'] for complx in self.processed_complexes])\n",
    "            self.transform_func = GNNTransformMultiTask(gearnet=self.gearnet)\n",
    "    def len(self):\n",
    "        return len(self.processed_complexes)\n",
    "    def get(self, idx):\n",
    "        # print(type(self.processed_complexes))\n",
    "        return self.transform_func(self.processed_complexes[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "in Initializing MultiTask Dataset...\n",
      "Start loading cached Multitask files...\n",
      "Complexes Before Checking: 283\n",
      "Checking the dataset...\n",
      "Cutting Threshold of atom numbers: 4873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:00<00:00, 1518.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complexes Before Task Selection: 271\n",
      "['5e13', '4m2v', '4q81', '3ffp', '1hn4', '3mmf', '3mna', '1f0t', '1o35', '3bgq', '1xq0', '4q7w', '1z6s', '4km0', '1o0h', '1qb6', '5u0g', '2zq2', '3dd8', '3ikd', '3axz', '1tng', '4hp0', '6got', '2zdn', '4tun', '6guk', '3myq', '3aas', '1zge', '3ml2', '1jys', '4k0y', '3rz1', '5u0d', '1o2q', '4aba', '2c3i', '3ibn', '3mhm', '1g36', '5fng', '1c5s', '4qgd', '1g53', '5o07', '1eb2', '5tya', '5uln', '2x7u', '2v2c', '1gi1', '1k1o', '1o0n', '3ibi', '4zx0', '1v2j', '4z1k', '1o2h', '2o4z', '6gxb', '6no9', '4qy3', '4jsa', '3ljo', '1bty', '5dwr', '1bnv', '1g45', '3k8q', '1v2o', '2x7t', '4abf', '6q4e', '3iog', '4u5s', '6dpx', '6h34', '1jsv', '2xog', '1gj6', '5sz7', '6ql2', '5e2r', '4ase', '1okl', '4bf6', '3t82', '6c7x', '6n0k', '5dpx', '1i9n', '1v2t', '3m1k', '6hgr', '5e2k', '1fki', '1bnn', '3n86', '4m2u', '2bza', '1fv0', '4z1e', '5eh8', '1o3j', '5n0e', '3b4f', '3mho', '4u5o', '1mrs', '6b59', '4yyt', '1o3f', '5ipj', '3d6o', '6ugr', '3gy3', '1s5z', '1v2w', '4tmk', '1o0f', '1k1n', '5wgp', '1ndz', '1y6r', '1bnu', '3f8e', '3oaf', '1mrn', '6oe1', '1o2j', '5evk', '4rfd', '1o2o', '4k18', '3n7a', '2fvd', '1lvu', '4asd', '3lp4', '3s72', '3p4v', '3t83', '1j4r', '1g54', '4ab9', '1o2r', '6h33', '1tx7', '1c5p', '5llg', '1b57', '4yxi', '1k1j', '2j27', '1ctu', '1jn4', '1tni', '4abg', '3h1x', '6h29', '6dpy', '1zfq', '5txy', '3ni5', '5j0d', '5llh', '4r5a', '5evb', '4kz3', '5mpn', '3f80', '10gs', '3m3x', '1g3e', '1bju', '1ndy', '3gst', '4abb', '5amd', '2r9x', '1pph', '4mo8', '1a9q', '3hkn', '4eb8', '2pow', '3r17', '6g3q', '4abe', '4qf7', '5n1r', '1k1l', '1jvu', '5nlk', '1hi5', '2fx6', '2gst', '5ti0', '4jsz', '6dpz', '3hs4', '1o3d', '5wex', '2xp7', '4q83', '2weh', '6ebe', '4qf8', '1t5f', '2w5g', '4iwz', '3d9z', '4yx4', '3qlm', '1hi3', '1bn4', '3s71', '3jy0', '2zdm', '4r59', '3n4b', '3nb5', '4q6d', '4rn4', '1oxr', '1o2n', '1o3i', '1bnt', '5u0f', '4ht0', '3eft', '4hpi', '3oyq', '4k9y', '3vbd', '4z0q', '1f0u', '1b38', '5mnn', '4zx1', '5mpk', '3aau', '4kz6', '6g6t', '5nxp', '3m5e', '5l2s', '3iof', '3mhl', '5x74', '3ryz', '6ibk', '1vfn', '4bf1', '4x8o', '2ot1', '1o2z', '6jb2', '1rjc', '2p45', '1kxv', '4pgj', '2p42', '3ukx', '3a6c', '4pou', '2p44', '2p47', '4gn3', '4gn4']\n",
      "Dataset size: 271\n"
     ]
    }
   ],
   "source": [
    "# from utils.multitask_data import CustomMultiTaskDataset\n",
    "r_d = './datasets/MultiTask_new/'\n",
    "_ = CustomMultiTaskDataset(split='val', task='multi', root_dir=r_d, label_dir='./datasets/MultiTask_new/uniformed_labels.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       A\n",
       "1       A\n",
       "2       A\n",
       "3       A\n",
       "4       A\n",
       "       ..\n",
       "2803    A\n",
       "2804    A\n",
       "2805    A\n",
       "2806    A\n",
       "2807    A\n",
       "Name: chain, Length: 2808, dtype: object"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.processed_complexes[17]['atoms_protein']['chain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45873/45873 [00:00<00:00, 215609.65it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "test_list = ['1qbn', '1qhc', '4rfc', '3mhc', '5ty9', '5vgy', '2xj1', '4b6o', '5sz2', '4ag8', '2gkl', '3jya', '4m2w', '5mmg', '4q8x', '5ep7', '4gu6', '1f74', '1v2k', '2gss', '1bnq', '3s75', '1fkg', '3tmk', '3mfv', '4kiu', '3a1e', '1k1m', '1kpm', '3ryj', '3oku', '5th4', '1bn1', '5ngz', '5j41', '4e3g', '4q09', '5mme', '1w4p', '4g8v', '4cp5', '1xug', '3s76', '1u1b', '6h2z', '4agc', '3p58', '1rpf', '1bzy', '2on6', '3d6p', '5flt', '1cny', '4g90', '6ic2', '3n8k', '4qer', '4q7s', '3kv2', '1f73', '3mjl', '3d8z', '3daz', '4q9y', '1qbo', '6hgs', '6ecz', '1c5t', '1fkh', '1fkb', '1drv', '3vhk', '2arm', '3v7x', '1rnm', '1jq8', '4q99', '4kao', '5tb6', '1ydk', '4bqs', '6gxe', '4r5b', '5f2u', '2hoc', '3gy4', '5nya', '4z1j', '3dbu', '5sz0', '5sz5', '5ljt', '5fdi', '3a1d', '6pl1', '2q1q', '1g52', '5lle', '3acl', '3fuc', '1rr6', '1lbk', '2q38', '6pvv', '3ibl', '1xgi', '4unp', '3n0n', '1uto', '1tnh', '2zdl', '1ctt', '1xpz', '2ez7', '3m40', '5amg', '1cnx', '4g95', '1w4o', '3n76', '5dbm', '3hkt', '2hnc', '1o36', '3nkk', '3bl1', '1gar', '4abd', '3dd0', '2hl4', '2xj2', '1afk', '1gi4', '3rz7', '6q4g', '5ny1', '4ruy', '2zfs', '1g4o', '2zdk', '3m67', '5nxw', '3p5l', '1uho', '5n25', '2weg', '1ttm', '1v2l', '1m2x', '6guh', '3n2p', '2euk', '3n3j', '5eh5', '5j8z', '1pxn', '1hi4', '3ibu', '4rsk', '5ny3', '5nxo', '5evd', '5tmp', '5n24', '5llc', '1g48', '1if8', '6oe0', '3m96', '3bgz', '4ciw', '3t5u', '2vxn', '1c5q', '1ppc', '1e1v', '1b8n', '6n0j', '1ydb', '2xb8', '5ekm', '4yk0', '1afl', '2pov', '1g3d', '1w4q', '4rux', '1a1e', '2qdt', '1avn', '1yda', '4gu9', '1kv5', '1if7', '3oim', '5h85', '3kwa', '2fu8', '5ehv', '5mn1', '4abh', '2i6b', '4pmm', '1utn', '3bgs', '6cjv', '4q08', '5u0e', '4hj2', '3ryv', '6ugp', '1o2w', '4kz4', '4yml', '2bes', '1bcd', '4q7p', '1o38', '1b8o', '1c1r', '1y3v', '1amk', '1pxo', '3s78', '1y3x', '1iih', '2weo', '6of5', '6equ', '5ljq', '2v2h', '5e2s', '5fdc', '5nee', '3dcc', '4kni', '5etj', '1o33', '6ugn', '1qb9', '3ryx', '5flo', '3oil', '3d8w', '4n6z', '1dud', '1ql7', '1o0m', '6c7w', '6uh0', '2exm', '5sz3', '1i9p', '4q7v', '1pxp', '5mpz', '1zvy', '3ukw', '1bvk', '1kxt', '2iff', '1zmy', '3a6b', '2p48', '2p46', '3qsk', '4glv', '1zv5', '2p43', '4gn5']\n",
    "train_list = ['1g52', '4rfc', '3mfv', '1bju', '1sv3', '3t85', '1o2o', '2weo', '1hi3', '4abg', '2x7t', '1jvu', '1okl', '3iog', '4n6z', '3n8k', '6uh0', '1eb2', '4hj2', '1o3d', '5evd', '1ucn', '5jsq', '1b38', '2gkl', '1uho', '5ny1', '3oyq', '2weg', '4n8q', '5u0f', '3s73', '2p48', '3a6b']\n",
    "val_list = ['5e13', '4m2v', '4q81', '3ffp', '1hn4', '3mmf', '3mna', '1f0t', '1o35', '3bgq', '1xq0', '4q7w', '1z6s', '4km0', '1o0h', '1qb6', '5u0g', '2zq2', '3dd8', '3ikd', '3axz', '1tng', '4hp0', '6got', '2zdn', '4tun', '6guk', '3myq', '3aas', '1zge', '3ml2', '1jys', '4k0y', '3rz1', '5u0d', '1o2q', '4aba', '2c3i', '3ibn', '3mhm', '1g36', '5fng', '1c5s', '4qgd', '1g53', '5o07', '1eb2', '5tya', '5uln', '2x7u', '2v2c', '1gi1', '1k1o', '1o0n', '3ibi', '4zx0', '1v2j', '4z1k', '1o2h', '2o4z', '6gxb', '6no9', '4qy3', '4jsa', '3ljo', '1bty', '5dwr', '1bnv', '1g45', '3k8q', '1v2o', '2x7t', '4abf', '6q4e', '3iog', '4u5s', '6dpx', '6h34', '1jsv', '2xog', '1gj6', '5sz7', '6ql2', '5e2r', '4ase', '1okl', '4bf6', '3t82', '6c7x', '6n0k', '5dpx', '1i9n', '1v2t', '3m1k', '6hgr', '5e2k', '1fki', '1bnn', '3n86', '4m2u', '2bza', '1fv0', '4z1e', '5eh8', '1o3j', '5n0e', '3b4f', '3mho', '4u5o', '1mrs', '6b59', '4yyt', '1o3f', '5ipj', '3d6o', '6ugr', '3gy3', '1s5z', '1v2w', '4tmk', '1o0f', '1k1n', '5wgp', '1ndz', '1y6r', '1bnu', '3f8e', '3oaf', '1mrn', '6oe1', '1o2j', '5evk', '4rfd', '1o2o', '4k18', '3n7a', '2fvd', '1lvu', '4asd', '3lp4', '3s72', '3p4v', '3t83', '1j4r', '1g54', '4ab9', '1o2r', '6h33', '1tx7', '1c5p', '5llg', '1b57', '4yxi', '1k1j', '2j27', '1ctu', '1jn4', '1tni', '4abg', '3h1x', '6h29', '6dpy', '1zfq', '5txy', '3ni5', '5j0d', '5llh', '4r5a', '5evb', '4kz3', '5mpn', '3f80', '10gs', '3m3x', '1g3e', '1bju', '1ndy', '3gst', '4abb', '5amd', '2r9x', '1pph', '4mo8', '1a9q', '3hkn', '4eb8', '2pow', '3r17', '6g3q', '4abe', '4qf7', '5n1r', '1k1l', '1jvu', '5nlk', '1hi5', '2fx6', '2gst', '5ti0', '4jsz', '6dpz', '3hs4', '1o3d', '5wex', '2xp7', '4q83', '2weh', '6ebe', '4qf8', '1t5f', '2w5g', '4iwz', '3d9z', '4yx4', '3qlm', '1hi3', '1bn4', '3s71', '3jy0', '2zdm', '4r59', '3n4b', '3nb5', '4q6d', '4rn4', '1oxr', '1o2n', '1o3i', '1bnt', '5u0f', '4ht0', '3eft', '4hpi', '3oyq', '4k9y', '3vbd', '4z0q', '1f0u', '1b38', '5mnn', '4zx1', '5mpk', '3aau', '4kz6', '6g6t', '5nxp', '3m5e', '5l2s', '3iof', '3mhl', '5x74', '3ryz', '6ibk', '1vfn', '4bf1', '4x8o', '2ot1', '1o2z', '6jb2', '1rjc', '2p45', '1kxv', '4pgj', '2p42', '3ukx', '3a6c', '4pou', '2p44', '2p47', '4gn3', '4gn4']\n",
    "train_all_list = [cpl['complex_id'] for cpl in _.processed_complexes]\n",
    "lines = []\n",
    "with open('./datasets/MultiTask_new/test.txt') as f:\n",
    "    lines.extend(f.readlines())\n",
    "with open('./datasets/MultiTask_new/train.txt') as f:\n",
    "    lines.extend(f.readlines())\n",
    "with open('./datasets/MultiTask_new/val.txt') as f:\n",
    "    lines.extend(f.readlines())\n",
    "with open('./datasets/MultiTask_new/train_all.txt') as f:\n",
    "    lines.extend(f.readlines())\n",
    "with open('./datasets/MultiTask_new/uniformed_labels.json') as f:\n",
    "    labels = json.load(f)\n",
    "for line in tqdm(lines):\n",
    "    if len(labels[line[:-1]]['uniprots']) > 1 and line[:-1] in test_list+ val_list + train_list + train_all_list:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, pdb in enumerate(test_list):\n",
    "    if pdb == '3po1':\n",
    "        print(idx, pdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5e13', '4m2v', '4q81', '3ffp', '1hn4', '3mmf', '3mna', '1f0t', '1o35', '3bgq', '1xq0', '4q7w', '1z6s', '4km0', '1o0h', '1qb6', '5u0g', '2zq2', '3dd8', '3ikd', '3axz', '1tng', '4hp0', '6got', '2zdn', '4tun', '6guk', '3myq', '3aas', '1zge', '3ml2', '1jys', '4k0y', '3rz1', '5u0d', '1o2q', '4aba', '2c3i', '3ibn', '3mhm', '1g36', '5fng', '1c5s', '4qgd', '1g53', '5o07', '1eb2', '5tya', '5uln', '2x7u', '2v2c', '1gi1', '1k1o', '1o0n', '3ibi', '4zx0', '1v2j', '4z1k', '1o2h', '2o4z', '6gxb', '6no9', '4qy3', '4jsa', '3ljo', '1bty', '5dwr', '1bnv', '1g45', '3k8q', '1v2o', '2x7t', '4abf', '6q4e', '3iog', '4u5s', '6dpx', '6h34', '1jsv', '2xog', '1gj6', '5sz7', '6ql2', '5e2r', '4ase', '1okl', '4bf6', '3t82', '6c7x', '6n0k', '5dpx', '1i9n', '1v2t', '3m1k', '6hgr', '5e2k', '1fki', '1bnn', '3n86', '4m2u', '2bza', '1fv0', '4z1e', '5eh8', '1o3j', '5n0e', '3b4f', '3mho', '4u5o', '1mrs', '6b59', '4yyt', '1o3f', '5ipj', '3d6o', '6ugr', '3gy3', '1s5z', '1v2w', '4tmk', '1o0f', '1k1n', '5wgp', '1ndz', '1y6r', '1bnu', '3f8e', '3oaf', '1mrn', '6oe1', '1o2j', '5evk', '4rfd', '1o2o', '4k18', '3n7a', '2fvd', '1lvu', '4asd', '3lp4', '3s72', '3p4v', '3t83', '1j4r', '1g54', '4ab9', '1o2r', '6h33', '1tx7', '1c5p', '5llg', '1b57', '4yxi', '1k1j', '2j27', '1ctu', '1jn4', '1tni', '4abg', '3h1x', '6h29', '6dpy', '1zfq', '5txy', '3ni5', '5j0d', '5llh', '4r5a', '5evb', '4kz3', '5mpn', '3f80', '10gs', '3m3x', '1g3e', '1bju', '1ndy', '3gst', '4abb', '5amd', '2r9x', '1pph', '4mo8', '1a9q', '3hkn', '4eb8', '2pow', '3r17', '6g3q', '4abe', '4qf7', '5n1r', '1k1l', '1jvu', '5nlk', '1hi5', '2fx6', '2gst', '5ti0', '4jsz', '6dpz', '3hs4', '1o3d', '5wex', '2xp7', '4q83', '2weh', '6ebe', '4qf8', '1t5f', '2w5g', '4iwz', '3d9z', '4yx4', '3qlm', '1hi3', '1bn4', '3s71', '3jy0', '2zdm', '4r59', '3n4b', '3nb5', '4q6d', '4rn4', '1oxr', '1o2n', '1o3i', '1bnt', '5u0f', '4ht0', '3eft', '4hpi', '3oyq', '4k9y', '3vbd', '4z0q', '1f0u', '1b38', '5mnn', '4zx1', '5mpk', '3aau', '4kz6', '6g6t', '5nxp', '3m5e', '5l2s', '3iof', '3mhl', '5x74', '3ryz', '6ibk', '1vfn', '4bf1', '4x8o', '2ot1', '1o2z', '6jb2', '1rjc', '2p45', '1kxv', '4pgj', '2p42', '3ukx', '3a6c', '4pou', '2p44', '2p47', '4gn3', '4gn4']\n"
     ]
    }
   ],
   "source": [
    "_.choose_task_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Transform LBA\n",
      "atom_df shape:  (2875, 7)\n",
      "protein_df shape:  (2808, 7)\n",
      "ligand_df shape:  (67, 4)\n",
      "atom_df shape:  (1655, 7)\n",
      "lig_flag shape:  torch.Size([1655])\n",
      "     chain resname element name          x      y      z\n",
      "0        A     ILE       N    N  18.568001  5.023  8.831\n",
      "1        A     ILE       C   CA  18.843000  5.109  7.372\n",
      "2        A     ILE       C    C  20.315001  5.484  7.189\n",
      "3        A     ILE       O    O  20.705000  6.540  7.715\n",
      "4        A     ILE       C   CB  17.920000  6.103  6.679\n",
      "...    ...     ...     ...  ...        ...    ...    ...\n",
      "1650   NaN     NaN       C  NaN  19.008000 -3.505  6.891\n",
      "1651   NaN     NaN       N  NaN  18.194000 -3.133  5.902\n",
      "1652   NaN     NaN       N  NaN  20.318000 -3.639  6.717\n",
      "1653   NaN     NaN       C  NaN  17.222000 -3.065  8.612\n",
      "1654   NaN     NaN       C  NaN  16.620000 -3.292  9.843\n",
      "\n",
      "[1655 rows x 7 columns]\n",
      "torch.Size([1625])\n",
      "torch.Size([1655])\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "complex_ = _.get(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyData(x=[242], edge_index=[2, 3450], pos=[242, 3], edge_weights=[3450], affinities=[1, 2], affinity_mask=[1, 2], functions=[1, 3293], valid_masks=[1, 4], chains=[222], lig_flag=[242], prot_id='3po1', type='multi')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(my_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scatter_() received an invalid combination of arguments - got (int), but expected one of:\n * (int dim, Tensor index, Tensor src)\n * (int dim, Tensor index, Tensor src, *, str reduce)\n * (int dim, Tensor index, Number value)\n * (int dim, Tensor index, Number value, *, str reduce)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prop \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39m2755\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m prop\u001b[39m.\u001b[39;49mscatter_(\u001b[39m0\u001b[39;49m,)\n",
      "\u001b[0;31mTypeError\u001b[0m: scatter_() received an invalid combination of arguments - got (int), but expected one of:\n * (int dim, Tensor index, Tensor src)\n * (int dim, Tensor index, Tensor src, *, str reduce)\n * (int dim, Tensor index, Number value)\n * (int dim, Tensor index, Number value, *, str reduce)\n"
     ]
    }
   ],
   "source": [
    "prop = torch.zeros(2755)\n",
    "prop.scatter_(0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdim= 100,\n",
    "vdim: int = 16,\n",
    "depth: int = 3,\n",
    "r_cutoff: float = 5.0,\n",
    "num_radial: int = 32,\n",
    "model_type: str = \"eqgat\",\n",
    "learning_rate: float = 1e-4,\n",
    "weight_decay: float = 0.0,\n",
    "patience_scheduler: int = 10,\n",
    "factor_scheduler: float = 0.75,\n",
    "max_epochs: int = 30,\n",
    "use_norm: bool = True,\n",
    "aggr: str = \"mean\",\n",
    "enhanced: bool = False,\n",
    "offset_strategy: int = 0,\n",
    "task = 'multi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.task: multi\n",
      "Readout Strategy: vanilla\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import Tensor, nn\n",
    "from gmsl.model import BaseModel, SEGNNModel\n",
    "from torch_geometric.data import Batch\n",
    "from typing import Tuple\n",
    "model = BaseModel(sdim=100,\n",
    "                                   vdim=16,\n",
    "                                   depth=3,\n",
    "                                   r_cutoff=r_cutoff,\n",
    "                                   num_radial=32,\n",
    "                                   model_type=\"gearnet\",\n",
    "                                   graph_level=True,\n",
    "                                   num_elements=10,\n",
    "                                   out_units=1,\n",
    "                                   dropout=0.0,\n",
    "                                   use_norm=use_norm,\n",
    "                                   aggr=aggr,\n",
    "                                   cross_ablate=False,\n",
    "                                   no_feat_attn=False,\n",
    "                                   task=task,\n",
    "                                   readout='vanilla'\n",
    "                                #    protein_function_class_dims = class_dims\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "in Initializing MultiTask Dataset...\n",
      "Start loading cached Multitask files...\n",
      "Complexes Before Checking: 264\n",
      "Checking the dataset...\n",
      "Cutting Threshold of atom numbers: 6087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [00:00<00:00, 1377.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complexes Before Task Selection: 254\n",
      "['1o2o', '1g52', '6pvv', '1ppm', '2zdm', '4jsa', '3lxe', '5i8g', '3jdw', '1o2z', '1hfs', '6ql1', '4k0y', '1pme', '1jsv', '3huc', '2bal', '4myd', '2qrk', '4n8q', '4i71', '1o3l', '3tmk', '1ciz', '5gmn', '4e4n', '3twp', '1uho', '2nn7', '5j7q', '2c97', '4kiu', '5v0n', '1c5q', '5ndf', '4rfc', '6ebe', '2xj2', '5jsq', '3lpi', '4oc0', '3ljo', '1apv', '2uz9', '2x7t', '2hu6', '1o3i', '4e4l', '3m35', '4fai', '5flo', '5aml', '3lk8', '1kyv', '4en4', '1ado', '5ia1', '1dud', '5ny1', '1mrn', '5eng', '3ckz', '6dpz', '3f5j', '3ldp', '6g3q', '6got', '4wop', '3s76', '3bl1', '3mho', '4oc2', '5fck', '6dz0', '1bnu', '6ftz', '3hs4', '4hj2', '2v59', '6p86', '3buf', '4gfm', '6gjm', '2weg', '6h37', '4qem', '3cd5', '4pv5', '4h7q', '3pbb', '3gcs', '2weo', '6nw3', '5sz4', '1jvu', '1ogd', '3d8z', '3ryj', '2a14', '3bgz', '4efs', '3s73', '4k18', '1if8', '5i3v', '4wrb', '5n1r', '1o0n', '3mfv', '1lbk', '1d7j', '2bes', '1bju', '3hub', '6h34', '5sz2', '4qlk', '1eb2', '4e3g', '4gr3', '4ehz', '5ftg', '4q6d', '1ql7', '1f73', '6oe1', '1w4q', '4q90', '3axz', '1moq', '4do5', '1bnq', '4abg', '3gy4', '2x7u', '1v2t', '3hkq', '3mhc', '4q19', '3hkt', '1bxq', '6dz2', '5u0f', '5vr8', '5ia2', '5ta4', '6ugr', '5ey4', '3n8k', '3ml5', '4dhl', '3c2f', '1qhc', '1o3d', '3rv4', '1mmr', '3dd0', '5i9z', '4abd', '4qij', '1bjv', '5mme', '1t32', '2e94', '4q09', '1ydk', '5t9u', '1fkn', '5upf', '4n6z', '1ydd', '4unp', '4in9', '3exh', '3s77', '1q7a', '4qll', '5lz5', '1tx7', '1pdz', '6dyz', '4std', '1bn1', '1uml', '3oyq', '5e2r', '2qpu', '5evd', '3c2r', '1phw', '5nkb', '1bnw', '3bra', '2e92', '3lka', '6hgr', '5th4', '4tmk', '4r5b', '5yjm', '1v48', '4q81', '1z6s', '1cnx', '4eoh', '6pl1', '6q4e', '3dcc', '3tvc', '6uh0', '3t85', '5eij', '6d1b', '1bxo', '1ik4', '4kyk', '2q8m', '1sv3', '1ceb', '3d6p', '3iog', '6h29', '1gj6', '1okl', '3cct', '6nv7', '3gst', '1ucn', '1b38', '1hi3', '4abb', '4agc', '5nk6', '5wij', '5jox', '2qu6', '2gkl', '1qin', '5t9w', '2weh', '3tsk', '4h3g', '3roc', '6guk', '5ock', '4afz', '1g6v', '2p48', '3a6b', '6jb2', '5szh', '2a78', '2p43', '2p42']\n",
      "Dataset size: 254\n",
      "Only retaining Alpha Carbon atoms for the atom_df\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomMultiTaskDataset(split='train', task='multi', gearnet=True, \n",
    "                                       alpha_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home-new/rhan21/mambaforge/envs/gmsl/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home-new/rhan21/mambaforge/envs/gmsl/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home-new/rhan21/mambaforge/envs/gmsl/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home-new/rhan21/mambaforge/envs/gmsl/lib/python3.9/site-packages/torch_geometric/data/dataset.py\", line 258, in __getitem__\n    data = self.get(self.indices()[idx])\n  File \"/tmp/ipykernel_2170304/83550311.py\", line 389, in get\n    return self.transform_func(self.processed_complexes[idx])\n  File \"/tmp/ipykernel_2170304/1926787363.py\", line 295, in __call__\n    graph = hetero_graph_transform(\nTypeError: hetero_graph_transform() missing 1 required positional argument: 'item_name'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rhan21/sjm/GMSL/data.ipynb 单元格 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254696d6c61622d643034227d/home/rhan21/sjm/GMSL/data.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254696d6c61622d643034227d/home/rhan21/sjm/GMSL/data.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m d \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254696d6c61622d643034227d/home/rhan21/sjm/GMSL/data.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254696d6c61622d643034227d/home/rhan21/sjm/GMSL/data.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     y \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254696d6c61622d643034227d/home/rhan21/sjm/GMSL/data.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# d = data\u001b[39;00m\n",
      "File \u001b[0;32m/home-new/rhan21/mambaforge/envs/gmsl/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/home-new/rhan21/mambaforge/envs/gmsl/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m/home-new/rhan21/mambaforge/envs/gmsl/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/home-new/rhan21/mambaforge/envs/gmsl/lib/python3.9/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home-new/rhan21/mambaforge/envs/gmsl/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home-new/rhan21/mambaforge/envs/gmsl/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home-new/rhan21/mambaforge/envs/gmsl/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home-new/rhan21/mambaforge/envs/gmsl/lib/python3.9/site-packages/torch_geometric/data/dataset.py\", line 258, in __getitem__\n    data = self.get(self.indices()[idx])\n  File \"/tmp/ipykernel_2170304/83550311.py\", line 389, in get\n    return self.transform_func(self.processed_complexes[idx])\n  File \"/tmp/ipykernel_2170304/1926787363.py\", line 295, in __call__\n    graph = hetero_graph_transform(\nTypeError: hetero_graph_transform() missing 1 required positional argument: 'item_name'\n"
     ]
    }
   ],
   "source": [
    "y = None\n",
    "d = None\n",
    "for idx, data in enumerate(train_loader):\n",
    "    y = model(data)\n",
    "    # d = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[2].shape\n",
    "y_ = torch.hstack(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2755])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.7000,  7.1700, 10.1700,  6.5500,  8.5200,  8.0100,  7.5100])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.y.view(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyData(x=[279, 21], pos=[279, 3], num_residues=[1], chain=0        A\n",
       "1        A\n",
       "2        A\n",
       "3        A\n",
       "4        A\n",
       "      ... \n",
       "274    NaN\n",
       "275    NaN\n",
       "276    NaN\n",
       "277    NaN\n",
       "278    NaN\n",
       "Name: chain, Length: 279, dtype: object, edge_index=[2, 6613], edge_relations=[6613], edge_weights=[6613], num_relation=[1], functions=[1, 2755], valid_masks=[1, 2755], chains=[257], lig_flag=[279], prot_id='2hl4', num_nodes=279)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "root_dir = './data/PDBbind/refined-set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.listdir(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5318"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323404\n"
     ]
    }
   ],
   "source": [
    "csv_dir = './data/EC'\n",
    "rows = []\n",
    "with codecs.open('./data/EC/pdb_chain_enzyme.csv', encoding='utf-8') as f:\n",
    "    for row in csv.DictReader(f, skipinitialspace=True):\n",
    "        rows.append(row)\n",
    "print(len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdbid = []\n",
    "for row in rows:\n",
    "    pdbid.append(row['PDB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdbid = list(set(pdbid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110225"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdbid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19198"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec_dir = os.listdir('./data/EnzymeCommission/all')\n",
    "len(ec_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18571"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdb_ec = []\n",
    "for id in ec_dir:\n",
    "    pdb_ec.append(id[0:4])\n",
    "pdb_ec = list(set(pdb_ec))\n",
    "len(pdb_ec)\n",
    "ec_dic = {}\n",
    "for id in ec_rows:\n",
    "    if id[0:4] not in ec_dic.keys():\n",
    "        ec_dic[id[0:4]] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19201\n"
     ]
    }
   ],
   "source": [
    "ec_rows = []\n",
    "with codecs.open('./data/EnzymeCommission/nrPDB-EC_annot_copy.tsv', encoding='utf-8') as f:\n",
    "    for ec_row in csv.DictReader(f, skipinitialspace=True, delimiter=\"\\t\"):\n",
    "        ec_rows.append(ec_row)\n",
    "print(len(ec_rows))\n",
    "for row in ec_rows:\n",
    "    row['EC-numbers'] =  row['EC-numbers'].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PDB-chain': '4PR3-A', 'EC-numbers': ['3.2.2.9', '3.2.2.-']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec_rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6H8K ['7.1.1.-', '7.1.1.2'] ['1.6.99.-', '1.6.99.3']\n",
      "3FQD ['3.6.1.-'] ['3.1.13.-']\n",
      "5GM6 ['3.6.4.13', '3.6.4.-'] ['5.2.1.-', '5.2.1.8']\n",
      "6F5D ['3.6.3.14', '3.6.3.-'] ['7.1.2.-', '7.1.2.2']\n",
      "6FGA ['2.3.2.23', '2.3.2.-', '2.3.2.24'] ['2.3.2.27', '2.3.2.-']\n",
      "5FL7 ['3.6.3.14', '3.6.3.-'] ['7.1.2.-', '7.1.2.2']\n",
      "5MPC ['3.4.19.12', '3.4.19.-'] ['3.4.25.1', '3.4.25.-']\n",
      "6RFL ['2.7.7.6', '2.7.7.-'] ['3.6.1.15', '3.6.1.-']\n",
      "5B8I ['5.2.1.-', '5.2.1.8'] ['3.1.3.16', '3.1.3.-']\n",
      "6RFQ ['7.1.1.-', '7.1.1.2'] ['1.6.99.3', '1.6.99.-', '7.1.1.-', '7.1.1.2']\n",
      "2ISS ['3.5.1.-', '4.3.3.6', '3.5.1.2', '4.3.3.-'] ['4.3.3.6', '4.3.3.-']\n",
      "5XTD ['7.1.1.-', '7.1.1.2'] ['1.6.99.3', '1.6.99.-', '7.1.1.-', '7.1.1.2']\n",
      "6RFL ['2.7.7.6', '2.7.7.-'] ['2.1.1.-', '2.1.1.56']\n",
      "6G72 ['7.1.1.-', '7.1.1.2'] ['1.6.99.-', '1.6.99.3']\n",
      "6RFQ ['7.1.1.-', '7.1.1.2'] ['1.6.99.-', '1.6.99.3']\n",
      "6H8K ['7.1.1.-', '7.1.1.2'] ['1.6.99.-', '1.6.99.3']\n",
      "4B3J ['2.3.1.-'] ['1.1.1.35', '1.1.1.-']\n",
      "4PBW ['2.7.10.-', '2.7.10.1'] ['3.1.3.48', '3.1.3.-']\n",
      "6H8K ['7.1.1.-', '7.1.1.2'] ['1.6.99.-', '1.6.99.3']\n",
      "1Y8X ['2.3.2.-'] ['6.2.1.-']\n",
      "4U4C ['2.7.7.19', '2.7.7.-'] ['3.6.4.13', '3.6.4.-']\n",
      "2YEV ['7.1.1.9', '7.1.1.-'] ['1.9.3.1', '1.9.3.-']\n",
      "6F5D ['3.6.3.14', '3.6.3.-'] ['3.6.3.-']\n",
      "6DRE ['2.4.2.-'] ['3.2.2.-']\n",
      "6H8K ['7.1.1.-', '7.1.1.2'] ['1.6.99.3', '1.6.99.-', '7.1.1.-', '7.1.1.2']\n",
      "6H8K ['7.1.1.-', '7.1.1.2'] ['1.6.99.-', '1.6.99.3']\n",
      "4WJ3 ['6.3.5.-'] ['6.1.1.-']\n",
      "5L9T ['2.3.2.23', '2.3.2.-'] ['2.7.11.-', '2.7.11.1']\n",
      "5XTI ['7.1.1.-', '7.1.1.8'] ['7.1.1.-', '7.1.1.2']\n",
      "5XTH ['7.1.1.-', '7.1.1.8'] ['1.6.99.3', '1.6.99.-', '7.1.1.-', '7.1.1.2']\n",
      "6RFS ['1.6.99.-', '1.6.99.3'] ['7.1.1.-', '7.1.1.2']\n",
      "4WZ7 ['1.6.99.-', '1.6.99.3'] ['7.1.1.-', '7.1.1.2']\n",
      "6G72 ['7.1.1.-', '7.1.1.2'] ['1.6.99.-', '1.6.99.3']\n",
      "5LDW ['1.6.99.3', '1.6.99.-', '7.1.1.-', '7.1.1.2'] ['7.1.1.-', '7.1.1.2']\n",
      "5O31 ['1.6.99.3', '1.6.99.-', '7.1.1.-', '7.1.1.2'] ['7.1.1.-', '7.1.1.2']\n",
      "3D54 ['6.3.5.-'] ['3.5.1.-', '3.5.1.2', '6.3.5.-']\n",
      "2NVU ['2.3.2.-'] ['6.2.1.-']\n",
      "6Y3Z ['3.6.1.-'] ['6.3.2.-']\n",
      "6ESQ ['2.3.3.-'] ['2.3.1.-', '2.3.1.9']\n",
      "5LDX ['7.1.1.-', '7.1.1.2'] ['1.6.99.3', '1.6.99.-', '7.1.1.-', '7.1.1.2']\n",
      "1I1Q ['4.1.3.-'] ['2.4.2.-', '4.1.3.-']\n",
      "2O2V ['2.7.12.2', '2.7.12.-'] ['2.7.11.25', '2.7.11.-']\n",
      "6HXI ['2.3.3.-'] ['6.2.1.-']\n",
      "3ASN ['1.9.3.1', '1.9.3.-'] ['7.1.1.9', '7.1.1.-']\n",
      "1KA9 ['4.3.2.-', '4.3.2.10'] ['3.5.1.-', '4.3.2.-', '4.3.2.10', '3.5.1.2']\n",
      "2W6I ['7.1.2.-', '7.1.2.2'] ['3.6.3.14', '3.6.3.-']\n",
      "6G72 ['7.1.1.-', '7.1.1.2'] ['1.6.99.-', '1.6.99.3']\n",
      "5FL7 ['3.6.3.14', '3.6.3.-'] ['3.6.1.-', '3.6.1.34']\n",
      "3AL0 ['6.3.5.-'] ['6.1.1.-', '6.1.1.17']\n",
      "6H8K ['7.1.1.-', '7.1.1.2'] ['1.6.99.-', '1.6.99.3']\n",
      "2WP8 ['3.1.13.-'] ['3.1.13.-', '3.1.26.-']\n",
      "6U42 ['2.7.1.-'] ['2.7.4.6', '2.7.4.-']\n",
      "4LRT ['4.1.3.-'] ['1.2.1.-']\n",
      "6H5E ['3.5.1.-', '3.5.1.2', '6.3.5.-'] ['6.3.5.-']\n",
      "1NVM ['1.2.1.-'] ['4.1.3.-']\n",
      "5X3F ['2.7.11.-'] ['2.6.1.-']\n",
      "4DFC ['3.6.4.-'] ['3.6.1.3', '3.6.1.-']\n",
      "1PYT ['3.4.21.-'] ['3.4.17.-']\n",
      "6G2J ['1.6.99.-', '1.6.99.3'] ['7.1.1.-', '7.1.1.2']\n",
      "5GUW ['1.7.2.-', '1.7.2.1'] ['1.7.2.-']\n",
      "3JB9 ['5.2.1.-', '5.2.1.8'] ['2.3.2.27', '2.3.2.-']\n",
      "6RFQ ['7.1.1.-', '7.1.1.2'] ['1.6.99.-', '1.6.99.3']\n",
      "6RFR ['1.6.99.-', '1.6.99.3'] ['7.1.1.-', '7.1.1.2']\n",
      "5FL7 ['3.6.3.14', '3.6.3.-'] ['3.6.1.-', '3.6.1.34']\n",
      "6ERG ['4.2.99.-', '3.6.4.-'] ['3.6.4.-']\n",
      "1M56 ['1.9.3.1', '1.9.3.-'] ['7.1.1.9', '7.1.1.-']\n",
      "6TZ8 ['3.1.3.16', '3.1.3.-'] ['5.2.1.-', '5.2.1.8']\n",
      "2D0V ['1.1.99.-'] ['1.1.2.-']\n",
      "6ADQ ['1.9.3.1', '1.9.3.-'] ['1.15.1.1', '1.15.1.-']\n",
      "5W21 ['2.7.10.-', '2.7.10.1'] ['3.2.1.-', '3.2.1.31']\n",
      "5HPT ['2.3.2.26', '2.3.2.-'] ['2.3.2.23', '2.3.2.-']\n",
      "6RFR ['1.6.99.-', '1.6.99.3'] ['7.1.1.-', '7.1.1.2']\n",
      "6NYB ['2.7.12.2', '2.7.12.-'] ['2.7.11.-', '2.7.11.1']\n",
      "6FQB ['3.5.1.-', '3.5.1.2', '6.3.5.-'] ['6.3.5.-']\n",
      "5O31 ['1.6.99.3', '1.6.99.-', '7.1.1.-', '7.1.1.2'] ['7.1.1.-', '7.1.1.2']\n",
      "4JN6 ['4.1.3.-'] ['1.2.1.-']\n",
      "4WHV ['2.3.2.27', '2.3.2.-'] ['2.3.2.23', '2.3.2.-']\n",
      "2OZN ['3.2.1.-'] ['3.2.1.169', '3.2.1.-']\n",
      "2KSO ['2.7.10.-', '2.7.10.1'] ['3.1.3.-', '3.1.3.86']\n",
      "5XTI ['7.1.1.-', '7.1.1.8'] ['7.1.1.-', '7.1.1.2']\n",
      "3ASN ['1.9.3.1', '1.9.3.-'] ['7.1.1.9', '7.1.1.-']\n",
      "4FZW ['5.3.3.-'] ['4.2.1.-', '4.2.1.17']\n"
     ]
    }
   ],
   "source": [
    "ec_dic = {}\n",
    "for row in ec_rows:\n",
    "    if row['PDB-chain'][0:4] not in ec_dic.keys():\n",
    "        ec_dic[row['PDB-chain'][0:4]] = row['EC-numbers']\n",
    "    else:\n",
    "        if ec_dic[row['PDB-chain'][0:4]] != row['EC-numbers']:\n",
    "            print(row['PDB-chain'][0:4], ec_dic[row['PDB-chain'][0:4]], row['EC-numbers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1],\n",
    "                  [2],\n",
    "                  [3]])\n",
    "a.view(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my = []\n",
    "for i in range(3):\n",
    "    p = torch.tensor([i])\n",
    "    my.append(p)\n",
    "torch.vstack(my)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
