{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from process_multi_task_dataset import *\n",
    "from utils.multitask_data import CustomMultiTaskDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MultiTask Dataset...\n",
      "Start loading cached Multitask files...\n",
      "Complexes Before Checking: 6\n",
      "Checking the dataset...\n",
      "Cutting Threshold of atom numbers: 24306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 614.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complexes Before Task Selection: 6\n",
      "Using Fold val Dataset for training:\n",
      "now\n",
      "new_complexes 6\n",
      "Dataset size: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_dataset = CustomMultiTaskDataset(split='val', task='fold', gearnet=True, alpha_only=False, root_dir = './datasets/MultiTask_fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'complex_id': '1de4',\n",
       " 'num_proteins': 3,\n",
       " 'labels': {'uniprots': ['P61769', 'Q30201', 'P02786'],\n",
       "  'ec': [-1, -1, -1],\n",
       "  'go': [-1, -1, -1],\n",
       "  'reaction': [-1, -1, -1],\n",
       "  'domains': ['d1de4c1', 'd1de4c2', 'd1de4a1', 'd1de4a2'],\n",
       "  'fold': [[46], [465], [284], [623]],\n",
       "  'ppi': 7.62,\n",
       "  'lba': -1},\n",
       " 'atoms_protein':       chain resname element name          x          y           z domain_name\n",
       " 0         A     ARG       N    N  59.716999  59.879002  247.330002       other\n",
       " 1         A     ARG       C   CA  60.457001  61.174999  247.408997       other\n",
       " 2         A     ARG       C    C  61.854000  61.002998  248.013000       other\n",
       " 3         A     ARG       O    O  62.028000  60.966999  249.235001       other\n",
       " 4         A     ARG       C   CB  59.650002  62.205002  248.222000       other\n",
       " ...     ...     ...     ...  ...        ...        ...         ...         ...\n",
       " 24301     I     NAG       O   O4  51.841000  72.031998  133.104004       other\n",
       " 24302     I     NAG       O   O5  49.580002  71.544998  135.994995       other\n",
       " 24303     I     NAG       O   O6  50.793999  74.183998  134.738998       other\n",
       " 24304     I     NAG       O   O7  46.188999  69.619003  133.772003       other\n",
       " 24305     I      CA       C   CA  53.095001  70.755997  149.559006       other\n",
       " \n",
       " [24306 rows x 8 columns],\n",
       " 'protein_seq': ['ARG',\n",
       "  'SER',\n",
       "  'HIS',\n",
       "  'SER',\n",
       "  'LEU',\n",
       "  'HIS',\n",
       "  'TYR',\n",
       "  'LEU',\n",
       "  'PHE',\n",
       "  'MET',\n",
       "  'GLY',\n",
       "  'ALA',\n",
       "  'SER',\n",
       "  'GLU',\n",
       "  'GLN',\n",
       "  'ASP',\n",
       "  'LEU',\n",
       "  'GLY',\n",
       "  'LEU',\n",
       "  'SER',\n",
       "  'LEU',\n",
       "  'PHE',\n",
       "  'GLU',\n",
       "  'ALA',\n",
       "  'LEU',\n",
       "  'GLY',\n",
       "  'TYR',\n",
       "  'VAL',\n",
       "  'ASP',\n",
       "  'ASP',\n",
       "  'GLN',\n",
       "  'LEU',\n",
       "  'PHE',\n",
       "  'VAL',\n",
       "  'PHE',\n",
       "  'TYR',\n",
       "  'ASP',\n",
       "  'HIS',\n",
       "  'GLU',\n",
       "  'SER',\n",
       "  'ARG',\n",
       "  'ARG',\n",
       "  'VAL',\n",
       "  'GLU',\n",
       "  'PRO',\n",
       "  'ARG',\n",
       "  'THR',\n",
       "  'PRO',\n",
       "  'TRP',\n",
       "  'VAL',\n",
       "  'SER',\n",
       "  'SER',\n",
       "  'ARG',\n",
       "  'ILE',\n",
       "  'SER',\n",
       "  'SER',\n",
       "  'GLN',\n",
       "  'MET',\n",
       "  'TRP',\n",
       "  'LEU',\n",
       "  'GLN',\n",
       "  'LEU',\n",
       "  'SER',\n",
       "  'GLN',\n",
       "  'SER',\n",
       "  'LEU',\n",
       "  'LYS',\n",
       "  'GLY',\n",
       "  'TRP',\n",
       "  'ASP',\n",
       "  'HIS',\n",
       "  'MET',\n",
       "  'PHE',\n",
       "  'THR',\n",
       "  'VAL',\n",
       "  'ASP',\n",
       "  'PHE',\n",
       "  'TRP',\n",
       "  'THR',\n",
       "  'ILE',\n",
       "  'MET',\n",
       "  'GLU',\n",
       "  'ASN',\n",
       "  'HIS',\n",
       "  'ASN',\n",
       "  'HIS',\n",
       "  'SER',\n",
       "  'LYS',\n",
       "  'GLU',\n",
       "  'SER',\n",
       "  'HIS',\n",
       "  'THR',\n",
       "  'LEU',\n",
       "  'GLN',\n",
       "  'VAL',\n",
       "  'ILE',\n",
       "  'LEU',\n",
       "  'GLY',\n",
       "  'CYS',\n",
       "  'GLU',\n",
       "  'MET',\n",
       "  'GLN',\n",
       "  'GLU',\n",
       "  'ASP',\n",
       "  'ASN',\n",
       "  'SER',\n",
       "  'THR',\n",
       "  'GLU',\n",
       "  'GLY',\n",
       "  'TYR',\n",
       "  'TRP',\n",
       "  'LYS',\n",
       "  'TYR',\n",
       "  'GLY',\n",
       "  'TYR',\n",
       "  'ASP',\n",
       "  'GLY',\n",
       "  'GLN',\n",
       "  'ASP',\n",
       "  'HIS',\n",
       "  'LEU',\n",
       "  'GLU',\n",
       "  'PHE',\n",
       "  'CYS',\n",
       "  'PRO',\n",
       "  'ASP',\n",
       "  'THR',\n",
       "  'LEU',\n",
       "  'ASP',\n",
       "  'TRP',\n",
       "  'ARG',\n",
       "  'ALA',\n",
       "  'ALA',\n",
       "  'GLU',\n",
       "  'PRO',\n",
       "  'ARG',\n",
       "  'ALA',\n",
       "  'TRP',\n",
       "  'PRO',\n",
       "  'THR',\n",
       "  'LYS',\n",
       "  'LEU',\n",
       "  'GLU',\n",
       "  'TRP',\n",
       "  'GLU',\n",
       "  'ARG',\n",
       "  'HIS',\n",
       "  'LYS',\n",
       "  'ILE',\n",
       "  'ARG',\n",
       "  'ALA',\n",
       "  'ARG',\n",
       "  'GLN',\n",
       "  'ASN',\n",
       "  'ARG',\n",
       "  'ALA',\n",
       "  'TYR',\n",
       "  'LEU',\n",
       "  'GLU',\n",
       "  'ARG',\n",
       "  'ASP',\n",
       "  'CYS',\n",
       "  'PRO',\n",
       "  'ALA',\n",
       "  'GLN',\n",
       "  'LEU',\n",
       "  'GLN',\n",
       "  'GLN',\n",
       "  'LEU',\n",
       "  'LEU',\n",
       "  'GLU',\n",
       "  'LEU',\n",
       "  'GLY',\n",
       "  'ARG',\n",
       "  'GLY',\n",
       "  'VAL',\n",
       "  'LEU',\n",
       "  'ASP',\n",
       "  'GLN',\n",
       "  'GLN',\n",
       "  'VAL',\n",
       "  'PRO',\n",
       "  'PRO',\n",
       "  'LEU',\n",
       "  'VAL',\n",
       "  'LYS',\n",
       "  'VAL',\n",
       "  'THR',\n",
       "  'HIS',\n",
       "  'HIS',\n",
       "  'VAL',\n",
       "  'THR',\n",
       "  'SER',\n",
       "  'SER',\n",
       "  'VAL',\n",
       "  'THR',\n",
       "  'THR',\n",
       "  'LEU',\n",
       "  'ARG',\n",
       "  'CYS',\n",
       "  'ARG',\n",
       "  'ALA',\n",
       "  'LEU',\n",
       "  'ASN',\n",
       "  'TYR',\n",
       "  'TYR',\n",
       "  'PRO',\n",
       "  'GLN',\n",
       "  'ASN',\n",
       "  'ILE',\n",
       "  'THR',\n",
       "  'MET',\n",
       "  'LYS',\n",
       "  'TRP',\n",
       "  'LEU',\n",
       "  'LYS',\n",
       "  'ASP',\n",
       "  'LYS',\n",
       "  'GLN',\n",
       "  'PRO',\n",
       "  'MET',\n",
       "  'ASP',\n",
       "  'ALA',\n",
       "  'LYS',\n",
       "  'GLU',\n",
       "  'PHE',\n",
       "  'GLU',\n",
       "  'PRO',\n",
       "  'LYS',\n",
       "  'ASP',\n",
       "  'VAL',\n",
       "  'LEU',\n",
       "  'PRO',\n",
       "  'ASN',\n",
       "  'GLY',\n",
       "  'ASP',\n",
       "  'GLY',\n",
       "  'THR',\n",
       "  'TYR',\n",
       "  'GLN',\n",
       "  'GLY',\n",
       "  'TRP',\n",
       "  'ILE',\n",
       "  'THR',\n",
       "  'LEU',\n",
       "  'ALA',\n",
       "  'VAL',\n",
       "  'PRO',\n",
       "  'PRO',\n",
       "  'GLY',\n",
       "  'GLU',\n",
       "  'GLU',\n",
       "  'GLN',\n",
       "  'ARG',\n",
       "  'TYR',\n",
       "  'THR',\n",
       "  'CYS',\n",
       "  'GLN',\n",
       "  'VAL',\n",
       "  'GLU',\n",
       "  'HIS',\n",
       "  'PRO',\n",
       "  'GLY',\n",
       "  'LEU',\n",
       "  'ASP',\n",
       "  'GLN',\n",
       "  'PRO',\n",
       "  'LEU',\n",
       "  'ILE',\n",
       "  'VAL',\n",
       "  'ILE',\n",
       "  'TRP',\n",
       "  'ILE',\n",
       "  'GLN',\n",
       "  'ARG',\n",
       "  'THR',\n",
       "  'PRO',\n",
       "  'LYS',\n",
       "  'ILE',\n",
       "  'GLN',\n",
       "  'VAL',\n",
       "  'TYR',\n",
       "  'SER',\n",
       "  'ARG',\n",
       "  'HIS',\n",
       "  'PRO',\n",
       "  'ALA',\n",
       "  'GLU',\n",
       "  'ASN',\n",
       "  'GLY',\n",
       "  'LYS',\n",
       "  'SER',\n",
       "  'ASN',\n",
       "  'PHE',\n",
       "  'LEU',\n",
       "  'ASN',\n",
       "  'CYS',\n",
       "  'TYR',\n",
       "  'VAL',\n",
       "  'SER',\n",
       "  'GLY',\n",
       "  'PHE',\n",
       "  'HIS',\n",
       "  'PRO',\n",
       "  'SER',\n",
       "  'ASP',\n",
       "  'ILE',\n",
       "  'GLU',\n",
       "  'VAL',\n",
       "  'ASP',\n",
       "  'LEU',\n",
       "  'LEU',\n",
       "  'LYS',\n",
       "  'ASN',\n",
       "  'GLY',\n",
       "  'GLU',\n",
       "  'ARG',\n",
       "  'ILE',\n",
       "  'GLU',\n",
       "  'LYS',\n",
       "  'VAL',\n",
       "  'GLU',\n",
       "  'HIS',\n",
       "  'SER',\n",
       "  'ASP',\n",
       "  'LEU',\n",
       "  'SER',\n",
       "  'PHE',\n",
       "  'SER',\n",
       "  'LYS',\n",
       "  'ASP',\n",
       "  'TRP',\n",
       "  'SER',\n",
       "  'PHE',\n",
       "  'TYR',\n",
       "  'LEU',\n",
       "  'LEU',\n",
       "  'TYR',\n",
       "  'TYR',\n",
       "  'THR',\n",
       "  'GLU',\n",
       "  'PHE',\n",
       "  'THR',\n",
       "  'PRO',\n",
       "  'THR',\n",
       "  'GLU',\n",
       "  'LYS',\n",
       "  'ASP',\n",
       "  'GLU',\n",
       "  'TYR',\n",
       "  'ALA',\n",
       "  'CYS',\n",
       "  'ARG',\n",
       "  'VAL',\n",
       "  'ASN',\n",
       "  'HIS',\n",
       "  'VAL',\n",
       "  'THR',\n",
       "  'LEU',\n",
       "  'SER',\n",
       "  'GLN',\n",
       "  'PRO',\n",
       "  'LYS',\n",
       "  'ILE',\n",
       "  'VAL',\n",
       "  'LYS',\n",
       "  'TRP',\n",
       "  'ASP',\n",
       "  'ARG',\n",
       "  'ASP',\n",
       "  'MET',\n",
       "  'LEU',\n",
       "  'TYR',\n",
       "  'TRP',\n",
       "  'ASP',\n",
       "  'ASP',\n",
       "  'LEU',\n",
       "  'LYS',\n",
       "  'ARG',\n",
       "  'LYS',\n",
       "  'LEU',\n",
       "  'SER',\n",
       "  'GLU',\n",
       "  'LYS',\n",
       "  'LEU',\n",
       "  'ASP',\n",
       "  'SER',\n",
       "  'THR',\n",
       "  'ASP',\n",
       "  'PHE',\n",
       "  'THR',\n",
       "  'SER',\n",
       "  'THR',\n",
       "  'ILE',\n",
       "  'LYS',\n",
       "  'LEU',\n",
       "  'LEU',\n",
       "  'ASN',\n",
       "  'GLU',\n",
       "  'ASN',\n",
       "  'SER',\n",
       "  'TYR',\n",
       "  'VAL',\n",
       "  'PRO',\n",
       "  'ARG',\n",
       "  'GLU',\n",
       "  'ALA',\n",
       "  'GLY',\n",
       "  'SER',\n",
       "  'GLN',\n",
       "  'LYS',\n",
       "  'ASP',\n",
       "  'GLU',\n",
       "  'ASN',\n",
       "  'LEU',\n",
       "  'ALA',\n",
       "  'LEU',\n",
       "  'TYR',\n",
       "  'VAL',\n",
       "  'GLU',\n",
       "  'ASN',\n",
       "  'GLN',\n",
       "  'PHE',\n",
       "  'ARG',\n",
       "  'GLU',\n",
       "  'PHE',\n",
       "  'LYS',\n",
       "  'LEU',\n",
       "  'SER',\n",
       "  'LYS',\n",
       "  'VAL',\n",
       "  'TRP',\n",
       "  'ARG',\n",
       "  'ASP',\n",
       "  'GLN',\n",
       "  'HIS',\n",
       "  'PHE',\n",
       "  'VAL',\n",
       "  'LYS',\n",
       "  'ILE',\n",
       "  'GLN',\n",
       "  'VAL',\n",
       "  'LYS',\n",
       "  'ASP',\n",
       "  'SER',\n",
       "  'ALA',\n",
       "  'GLN',\n",
       "  'ASN',\n",
       "  'SER',\n",
       "  'VAL',\n",
       "  'ILE',\n",
       "  'ILE',\n",
       "  'VAL',\n",
       "  'ASP',\n",
       "  'LYS',\n",
       "  'ASN',\n",
       "  'GLY',\n",
       "  'ARG',\n",
       "  'LEU',\n",
       "  'VAL',\n",
       "  'TYR',\n",
       "  'LEU',\n",
       "  'VAL',\n",
       "  'GLU',\n",
       "  'ASN',\n",
       "  'PRO',\n",
       "  'GLY',\n",
       "  'GLY',\n",
       "  'TYR',\n",
       "  'VAL',\n",
       "  'ALA',\n",
       "  'TYR',\n",
       "  'SER',\n",
       "  'LYS',\n",
       "  'ALA',\n",
       "  'ALA',\n",
       "  'THR',\n",
       "  'VAL',\n",
       "  'THR',\n",
       "  'GLY',\n",
       "  'LYS',\n",
       "  'LEU',\n",
       "  'VAL',\n",
       "  'HIS',\n",
       "  'ALA',\n",
       "  'ASN',\n",
       "  'PHE',\n",
       "  'GLY',\n",
       "  'THR',\n",
       "  'LYS',\n",
       "  'LYS',\n",
       "  'ASP',\n",
       "  'PHE',\n",
       "  'GLU',\n",
       "  'ASP',\n",
       "  'LEU',\n",
       "  'TYR',\n",
       "  'THR',\n",
       "  'PRO',\n",
       "  'VAL',\n",
       "  'ASN',\n",
       "  'GLY',\n",
       "  'SER',\n",
       "  'ILE',\n",
       "  'VAL',\n",
       "  'ILE',\n",
       "  'VAL',\n",
       "  'ARG',\n",
       "  'ALA',\n",
       "  'GLY',\n",
       "  'LYS',\n",
       "  'ILE',\n",
       "  'THR',\n",
       "  'PHE',\n",
       "  'ALA',\n",
       "  'GLU',\n",
       "  'LYS',\n",
       "  'VAL',\n",
       "  'ALA',\n",
       "  'ASN',\n",
       "  'ALA',\n",
       "  'GLU',\n",
       "  'SER',\n",
       "  'LEU',\n",
       "  'ASN',\n",
       "  'ALA',\n",
       "  'ILE',\n",
       "  'GLY',\n",
       "  'VAL',\n",
       "  'LEU',\n",
       "  'ILE',\n",
       "  'TYR',\n",
       "  'MET',\n",
       "  'ASP',\n",
       "  'GLN',\n",
       "  'THR',\n",
       "  'LYS',\n",
       "  'PHE',\n",
       "  'PRO',\n",
       "  'ILE',\n",
       "  'VAL',\n",
       "  'ASN',\n",
       "  'ALA',\n",
       "  'GLU',\n",
       "  'LEU',\n",
       "  'SER',\n",
       "  'PHE',\n",
       "  'PHE',\n",
       "  'GLY',\n",
       "  'HIS',\n",
       "  'ALA',\n",
       "  'HIS',\n",
       "  'LEU',\n",
       "  'GLY',\n",
       "  'THR',\n",
       "  'GLY',\n",
       "  'ASP',\n",
       "  'PRO',\n",
       "  'TYR',\n",
       "  'THR',\n",
       "  'PRO',\n",
       "  'GLY',\n",
       "  'PHE',\n",
       "  'PRO',\n",
       "  'SER',\n",
       "  'PHE',\n",
       "  'ASN',\n",
       "  'HIS',\n",
       "  'THR',\n",
       "  'GLN',\n",
       "  'PHE',\n",
       "  'PRO',\n",
       "  'PRO',\n",
       "  'SER',\n",
       "  'ARG',\n",
       "  'SER',\n",
       "  'SER',\n",
       "  'GLY',\n",
       "  'LEU',\n",
       "  'PRO',\n",
       "  'ASN',\n",
       "  'ILE',\n",
       "  'PRO',\n",
       "  'VAL',\n",
       "  'GLN',\n",
       "  'THR',\n",
       "  'ILE',\n",
       "  'SER',\n",
       "  'ARG',\n",
       "  'ALA',\n",
       "  'ALA',\n",
       "  'ALA',\n",
       "  'GLU',\n",
       "  'LYS',\n",
       "  'LEU',\n",
       "  'PHE',\n",
       "  'GLY',\n",
       "  'ASN',\n",
       "  'MET',\n",
       "  'GLU',\n",
       "  'GLY',\n",
       "  'ASP',\n",
       "  'CYS',\n",
       "  'PRO',\n",
       "  'SER',\n",
       "  'ASP',\n",
       "  'TRP',\n",
       "  'LYS',\n",
       "  'THR',\n",
       "  'ASP',\n",
       "  'SER',\n",
       "  'THR',\n",
       "  'CYS',\n",
       "  'ARG',\n",
       "  'MET',\n",
       "  'VAL',\n",
       "  'THR',\n",
       "  'SER',\n",
       "  'GLU',\n",
       "  'SER',\n",
       "  'LYS',\n",
       "  'ASN',\n",
       "  'VAL',\n",
       "  'LYS',\n",
       "  'LEU',\n",
       "  'THR',\n",
       "  'VAL',\n",
       "  'SER',\n",
       "  'ASN',\n",
       "  'VAL',\n",
       "  'LEU',\n",
       "  'LYS',\n",
       "  'GLU',\n",
       "  'ILE',\n",
       "  'LYS',\n",
       "  'ILE',\n",
       "  'LEU',\n",
       "  'ASN',\n",
       "  'ILE',\n",
       "  'PHE',\n",
       "  'GLY',\n",
       "  'VAL',\n",
       "  'ILE',\n",
       "  'LYS',\n",
       "  'GLY',\n",
       "  'PHE',\n",
       "  'VAL',\n",
       "  'GLU',\n",
       "  'PRO',\n",
       "  'ASP',\n",
       "  'HIS',\n",
       "  'TYR',\n",
       "  'VAL',\n",
       "  'VAL',\n",
       "  'VAL',\n",
       "  'GLY',\n",
       "  'ALA',\n",
       "  'GLN',\n",
       "  'ARG',\n",
       "  'ASP',\n",
       "  'ALA',\n",
       "  'TRP',\n",
       "  'GLY',\n",
       "  'PRO',\n",
       "  'GLY',\n",
       "  'ALA',\n",
       "  'ALA',\n",
       "  'LYS',\n",
       "  'SER',\n",
       "  'GLY',\n",
       "  'VAL',\n",
       "  'GLY',\n",
       "  'THR',\n",
       "  'ALA',\n",
       "  'LEU',\n",
       "  'LEU',\n",
       "  'LEU',\n",
       "  'LYS',\n",
       "  'LEU',\n",
       "  'ALA',\n",
       "  'GLN',\n",
       "  'MET',\n",
       "  'PHE',\n",
       "  'SER',\n",
       "  'ASP',\n",
       "  'MET',\n",
       "  'VAL',\n",
       "  'LEU',\n",
       "  'LYS',\n",
       "  'ASP',\n",
       "  'GLY',\n",
       "  'PHE',\n",
       "  'GLN',\n",
       "  'PRO',\n",
       "  'SER',\n",
       "  'ARG',\n",
       "  'SER',\n",
       "  'ILE',\n",
       "  'ILE',\n",
       "  'PHE',\n",
       "  'ALA',\n",
       "  'SER',\n",
       "  'TRP',\n",
       "  'SER',\n",
       "  'ALA',\n",
       "  'GLY',\n",
       "  'ASP',\n",
       "  'PHE',\n",
       "  'GLY',\n",
       "  'SER',\n",
       "  'VAL',\n",
       "  'GLY',\n",
       "  'ALA',\n",
       "  'THR',\n",
       "  'GLU',\n",
       "  'TRP',\n",
       "  'LEU',\n",
       "  'GLU',\n",
       "  'GLY',\n",
       "  'TYR',\n",
       "  'LEU',\n",
       "  'SER',\n",
       "  'SER',\n",
       "  'LEU',\n",
       "  'HIS',\n",
       "  'LEU',\n",
       "  'LYS',\n",
       "  'ALA',\n",
       "  'PHE',\n",
       "  'THR',\n",
       "  'TYR',\n",
       "  'ILE',\n",
       "  'ASN',\n",
       "  'LEU',\n",
       "  'ASP',\n",
       "  'LYS',\n",
       "  'ALA',\n",
       "  'VAL',\n",
       "  'LEU',\n",
       "  'GLY',\n",
       "  'THR',\n",
       "  'SER',\n",
       "  'ASN',\n",
       "  'PHE',\n",
       "  'LYS',\n",
       "  'VAL',\n",
       "  'SER',\n",
       "  'ALA',\n",
       "  'SER',\n",
       "  'PRO',\n",
       "  'LEU',\n",
       "  'LEU',\n",
       "  'TYR',\n",
       "  'THR',\n",
       "  'LEU',\n",
       "  'ILE',\n",
       "  'GLU',\n",
       "  'LYS',\n",
       "  'THR',\n",
       "  'MET',\n",
       "  'GLN',\n",
       "  'ASN',\n",
       "  'VAL',\n",
       "  'LYS',\n",
       "  'HIS',\n",
       "  'PRO',\n",
       "  'VAL',\n",
       "  'THR',\n",
       "  'GLY',\n",
       "  'GLN',\n",
       "  'PHE',\n",
       "  'LEU',\n",
       "  'TYR',\n",
       "  'GLN',\n",
       "  'ASP',\n",
       "  'SER',\n",
       "  'ASN',\n",
       "  'TRP',\n",
       "  'ALA',\n",
       "  'SER',\n",
       "  'LYS',\n",
       "  'VAL',\n",
       "  'GLU',\n",
       "  'LYS',\n",
       "  'LEU',\n",
       "  'THR',\n",
       "  'LEU',\n",
       "  'ASP',\n",
       "  'ASN',\n",
       "  'ALA',\n",
       "  'ALA',\n",
       "  'PHE',\n",
       "  'PRO',\n",
       "  'PHE',\n",
       "  'LEU',\n",
       "  'ALA',\n",
       "  'TYR',\n",
       "  'SER',\n",
       "  'GLY',\n",
       "  'ILE',\n",
       "  'PRO',\n",
       "  'ALA',\n",
       "  'VAL',\n",
       "  'SER',\n",
       "  'PHE',\n",
       "  'CYS',\n",
       "  'PHE',\n",
       "  'CYS',\n",
       "  'GLU',\n",
       "  'ASP',\n",
       "  'THR',\n",
       "  'ASP',\n",
       "  'TYR',\n",
       "  'PRO',\n",
       "  'TYR',\n",
       "  'LEU',\n",
       "  'GLY',\n",
       "  'THR',\n",
       "  'THR',\n",
       "  'MET',\n",
       "  'ASP',\n",
       "  'THR',\n",
       "  'TYR',\n",
       "  'LYS',\n",
       "  'GLU',\n",
       "  'LEU',\n",
       "  'ILE',\n",
       "  'GLU',\n",
       "  'ARG',\n",
       "  'ILE',\n",
       "  'PRO',\n",
       "  'GLU',\n",
       "  'LEU',\n",
       "  'ASN',\n",
       "  'LYS',\n",
       "  'VAL',\n",
       "  'ALA',\n",
       "  'ARG',\n",
       "  'ALA',\n",
       "  'ALA',\n",
       "  'ALA',\n",
       "  'GLU',\n",
       "  'VAL',\n",
       "  'ALA',\n",
       "  'GLY',\n",
       "  'GLN',\n",
       "  'PHE',\n",
       "  'VAL',\n",
       "  'ILE',\n",
       "  'LYS',\n",
       "  'LEU',\n",
       "  'THR',\n",
       "  'HIS',\n",
       "  'ASP',\n",
       "  'VAL',\n",
       "  'GLU',\n",
       "  'LEU',\n",
       "  'ASN',\n",
       "  'LEU',\n",
       "  'ASP',\n",
       "  'TYR',\n",
       "  'GLU',\n",
       "  'ARG',\n",
       "  'TYR',\n",
       "  'ASN',\n",
       "  'SER',\n",
       "  'GLN',\n",
       "  'LEU',\n",
       "  'LEU',\n",
       "  'SER',\n",
       "  'PHE',\n",
       "  'VAL',\n",
       "  'ARG',\n",
       "  'ASP',\n",
       "  'LEU',\n",
       "  'ASN',\n",
       "  'GLN',\n",
       "  'TYR',\n",
       "  'ARG',\n",
       "  'ALA',\n",
       "  'ASP',\n",
       "  'ILE',\n",
       "  'LYS',\n",
       "  'GLU',\n",
       "  'MET',\n",
       "  'GLY',\n",
       "  'LEU',\n",
       "  'SER',\n",
       "  'LEU',\n",
       "  'GLN',\n",
       "  'TRP',\n",
       "  'LEU',\n",
       "  'TYR',\n",
       "  'SER',\n",
       "  'ALA',\n",
       "  'ARG',\n",
       "  'GLY',\n",
       "  'ASP',\n",
       "  'PHE',\n",
       "  'PHE',\n",
       "  'ARG',\n",
       "  'ALA',\n",
       "  'THR',\n",
       "  'SER',\n",
       "  'ARG',\n",
       "  'LEU',\n",
       "  'THR',\n",
       "  'THR',\n",
       "  'ASP',\n",
       "  'PHE',\n",
       "  'GLY',\n",
       "  'ASN',\n",
       "  'ALA',\n",
       "  'GLU',\n",
       "  'LYS',\n",
       "  'THR',\n",
       "  'ASP',\n",
       "  'ARG',\n",
       "  'PHE',\n",
       "  'VAL',\n",
       "  'MET',\n",
       "  'LYS',\n",
       "  'LYS',\n",
       "  'LEU',\n",
       "  'ASN',\n",
       "  'ASP',\n",
       "  'ARG',\n",
       "  'VAL',\n",
       "  'MET',\n",
       "  'ARG',\n",
       "  'VAL',\n",
       "  'GLU',\n",
       "  'TYR',\n",
       "  'HIS',\n",
       "  'PHE',\n",
       "  'LEU',\n",
       "  'SER',\n",
       "  'PRO',\n",
       "  'TYR',\n",
       "  'VAL',\n",
       "  'SER',\n",
       "  'PRO',\n",
       "  'LYS',\n",
       "  'GLU',\n",
       "  'SER',\n",
       "  'PRO',\n",
       "  'PHE',\n",
       "  'ARG',\n",
       "  'HIS',\n",
       "  'VAL',\n",
       "  'PHE',\n",
       "  'TRP',\n",
       "  'GLY',\n",
       "  'SER',\n",
       "  'GLY',\n",
       "  'SER',\n",
       "  'HIS',\n",
       "  'THR',\n",
       "  'LEU',\n",
       "  'PRO',\n",
       "  'ALA',\n",
       "  'LEU',\n",
       "  'LEU',\n",
       "  'GLU',\n",
       "  'ASN',\n",
       "  'LEU',\n",
       "  'LYS',\n",
       "  'LEU',\n",
       "  'ARG',\n",
       "  'LYS',\n",
       "  'GLN',\n",
       "  'ASN',\n",
       "  'ASN',\n",
       "  'GLY',\n",
       "  'ALA',\n",
       "  'PHE',\n",
       "  'ASN',\n",
       "  'GLU',\n",
       "  'THR',\n",
       "  'LEU',\n",
       "  'PHE',\n",
       "  'ARG',\n",
       "  'ASN',\n",
       "  'GLN',\n",
       "  'LEU',\n",
       "  'ALA',\n",
       "  'LEU',\n",
       "  'ALA',\n",
       "  'THR',\n",
       "  'TRP',\n",
       "  'THR',\n",
       "  'ILE',\n",
       "  'GLN',\n",
       "  'GLY',\n",
       "  'ALA',\n",
       "  'ALA',\n",
       "  'ASN',\n",
       "  'ALA',\n",
       "  'LEU',\n",
       "  'SER',\n",
       "  ...],\n",
       " 'atoms_ligand': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.processed_complexes[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chain</th>\n",
       "      <th>resname</th>\n",
       "      <th>element</th>\n",
       "      <th>name</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>domain_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>ARG</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>59.716999</td>\n",
       "      <td>59.879002</td>\n",
       "      <td>247.330002</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>ARG</td>\n",
       "      <td>C</td>\n",
       "      <td>CA</td>\n",
       "      <td>60.457001</td>\n",
       "      <td>61.174999</td>\n",
       "      <td>247.408997</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>ARG</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>61.854000</td>\n",
       "      <td>61.002998</td>\n",
       "      <td>248.013000</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>ARG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>62.028000</td>\n",
       "      <td>60.966999</td>\n",
       "      <td>249.235001</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>ARG</td>\n",
       "      <td>C</td>\n",
       "      <td>CB</td>\n",
       "      <td>59.650002</td>\n",
       "      <td>62.205002</td>\n",
       "      <td>248.222000</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24301</th>\n",
       "      <td>I</td>\n",
       "      <td>NAG</td>\n",
       "      <td>O</td>\n",
       "      <td>O4</td>\n",
       "      <td>51.841000</td>\n",
       "      <td>72.031998</td>\n",
       "      <td>133.104004</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24302</th>\n",
       "      <td>I</td>\n",
       "      <td>NAG</td>\n",
       "      <td>O</td>\n",
       "      <td>O5</td>\n",
       "      <td>49.580002</td>\n",
       "      <td>71.544998</td>\n",
       "      <td>135.994995</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24303</th>\n",
       "      <td>I</td>\n",
       "      <td>NAG</td>\n",
       "      <td>O</td>\n",
       "      <td>O6</td>\n",
       "      <td>50.793999</td>\n",
       "      <td>74.183998</td>\n",
       "      <td>134.738998</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24304</th>\n",
       "      <td>I</td>\n",
       "      <td>NAG</td>\n",
       "      <td>O</td>\n",
       "      <td>O7</td>\n",
       "      <td>46.188999</td>\n",
       "      <td>69.619003</td>\n",
       "      <td>133.772003</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24305</th>\n",
       "      <td>I</td>\n",
       "      <td>CA</td>\n",
       "      <td>C</td>\n",
       "      <td>CA</td>\n",
       "      <td>53.095001</td>\n",
       "      <td>70.755997</td>\n",
       "      <td>149.559006</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24306 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      chain resname element name          x          y           z domain_name\n",
       "0         A     ARG       N    N  59.716999  59.879002  247.330002       other\n",
       "1         A     ARG       C   CA  60.457001  61.174999  247.408997       other\n",
       "2         A     ARG       C    C  61.854000  61.002998  248.013000       other\n",
       "3         A     ARG       O    O  62.028000  60.966999  249.235001       other\n",
       "4         A     ARG       C   CB  59.650002  62.205002  248.222000       other\n",
       "...     ...     ...     ...  ...        ...        ...         ...         ...\n",
       "24301     I     NAG       O   O4  51.841000  72.031998  133.104004       other\n",
       "24302     I     NAG       O   O5  49.580002  71.544998  135.994995       other\n",
       "24303     I     NAG       O   O6  50.793999  74.183998  134.738998       other\n",
       "24304     I     NAG       O   O7  46.188999  69.619003  133.772003       other\n",
       "24305     I      CA       C   CA  53.095001  70.755997  149.559006       other\n",
       "\n",
       "[24306 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(val_dataset.processed_complexes[5]['atoms_protein'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1lbk\n",
      "['A']\n",
      "1 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyData(x=[1656, 21], pos=[1656, 3], num_nodes=1656, num_residues=208, chain=0         A\n",
       "1         A\n",
       "2         A\n",
       "3         A\n",
       "4         A\n",
       "       ... \n",
       "1651    NaN\n",
       "1652    NaN\n",
       "1653    NaN\n",
       "1654    NaN\n",
       "1655    NaN\n",
       "Name: chain, Length: 1656, dtype: object, edge_index=[2, 60788], edge_relations=[60788], edge_weights=[60788], num_relation=7, fold_functions=[2, 1], fold_masks=[2, 1195], chains=[1636], domains=[1636], domain_flag=[1656], lig_flag=[1656], prot_id='1lbk', type='fold')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = CustomMultiTaskDataset(split='test', task='ec', gearnet=True, alpha_only=False, root_dir = './datasets/MultiTask')\n",
    "train_dataset = CustomMultiTaskDataset(split='train_ec', task='ec', gearnet=True, alpha_only=False, root_dir = './datasets/MultiTask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_annot_dict = gen_ec_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_annot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './output_info/'\n",
    "json_files = ['enzyme_commission_uniprots.json', 'gene_ontology_uniprots.json', 'protein_protein_uniprots.json', 'protein_ligand_uniprots.json']\n",
    "json_dirs = [os.path.join(root_dir, json_file) for json_file in json_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_uniprot_dict, go_info_dict = gen_protein_property_uniprots(json_dirs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_labels, go_full_uniprot_dict = gen_go_labels(go_uniprot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "seed = int(torch.empty((), dtype=torch.int64).random_().item())\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_train = torch.randperm(24, generator=generator).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is for processing the dataset to form a new multi-task dataset with a unform labeling code\n",
    "# Datasets PDBBind(Protein-Ligand, Protein-Protein), EnzymeCommission, GeneOntology\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def cal_complex_all(json_dirs):\n",
    "    complex_list = []\n",
    "    for json_dir in json_dirs:\n",
    "        with open(json_dir, 'r') as f:\n",
    "            info_dict = json.load(f)\n",
    "        for i in info_dict:\n",
    "            if '-' in i:\n",
    "                id = i.split('-')[0].lower()\n",
    "            complex_list.append(id)\n",
    "    print(len(list(set(complex_list))))\n",
    "        \n",
    "\n",
    "def gen_train_test_ids(complex_dict, ec_dict, go_dict):\n",
    "    train_list = []\n",
    "    # print(complex_dict)\n",
    "    all_list = []\n",
    "    test_uniprots = []\n",
    "    test_list_tmp = []\n",
    "    for k, v in complex_dict.items():\n",
    "        all_list.extend(v)\n",
    "    all_list = list(set(all_list))\n",
    "    for k, v in complex_dict.items():\n",
    "        # print(k, v)\n",
    "        # 由于每个蛋白可能有多个Uniprot，只能根据某一个Uniprot不在的情况找出训练集的样本\n",
    "        if k not in ec_dict or k not in go_dict:\n",
    "            train_list.extend(v)\n",
    "    #取反即可获得测试集\n",
    "    test_list = [i for i in all_list if i not in train_list]\n",
    "    test_uniprots = []\n",
    "    for k, v in complex_dict.items():\n",
    "        for id in v:\n",
    "            if id in test_list and k not in test_uniprots:\n",
    "                test_uniprots.append(k)\n",
    "    # print(\"Uniprots:\", len(test_uniprots))\n",
    "    # print(\"Train:\", len(train_list), len(list(set(train_list))))\n",
    "    # print(\"Test:\", len(test_list), len(list(set(test_list))))\n",
    "    # print(\"All:\", len(list(set(test_list + train_list))))\n",
    "    # print(list(set(train_list)))\n",
    "    return list(set(train_list)), list(set(test_list)), list(set(test_uniprots))\n",
    "        \n",
    "            \n",
    "def gen_protein_property_uniprots(json_dir: str, single=True):\n",
    "    with open(json_dir, 'r') as f:\n",
    "        info_dict = json.load(f)\n",
    "    uniprot_dict = {} # 蛋白质 dictionary\n",
    "    # print(len(info_dict))\n",
    "    info_dict_new = {}\n",
    "    for k, v in info_dict.items():\n",
    "        if single:\n",
    "            if len(v) != 1:\n",
    "                continue\n",
    "            else:\n",
    "                uniprot_id = v[0]\n",
    "                info_dict_new[k] = [uniprot_id]   \n",
    "                # 只记录同一个uniprot id的所有pdb_id\n",
    "                if uniprot_id not in uniprot_dict:     \n",
    "                    uniprot_dict[uniprot_id] = k\n",
    "                # else:\n",
    "                #     uniprot_dict[uniprot_id].append(k)\n",
    "        else:\n",
    "            if len(v) == 0 or len(v) > 3:\n",
    "                continue\n",
    "            else:\n",
    "                info_dict_new[k] = v\n",
    "                for uniprot_id in v:\n",
    "                    if uniprot_id not in uniprot_dict:\n",
    "                        uniprot_dict[uniprot_id] = [k]\n",
    "                    else:\n",
    "                        uniprot_dict[uniprot_id].append(k)\n",
    "    # print(\"Unitest:\", len(uniprot_dict), len(info_dict_new))\n",
    "    return uniprot_dict, info_dict_new\n",
    "    \n",
    "def save_dataset_info(dir, complex_list):\n",
    "    with open(dir, 'w') as f:\n",
    "        for complex in complex_list:\n",
    "            f.write(complex + '\\n')\n",
    "        f.close()\n",
    "\n",
    "def gen_ec_labels():\n",
    "    root_dir = './data/EnzymeCommission/nrPDB-EC_annot.tsv'\n",
    "    # root_dir = '/home/rhan21/sjm/GMSL/data/EnzymeCommission/nrPDB-EC_annot.tsv'\n",
    "    with open(root_dir, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    ec_classes = lines[1].strip().split('\\t')\n",
    "    label_dict = {}\n",
    "    pdb_annot_dict = {}\n",
    "    label_id = 0\n",
    "    for label in ec_classes:\n",
    "        if label not in label_dict:\n",
    "            label_dict[label] = label_id\n",
    "            label_id += 1\n",
    "    for item in lines[3:]:\n",
    "        pdb_id, annotations = item.split('\\t')\n",
    "        annotations_list = annotations.strip().split(',')\n",
    "        pdb_annot_dict[pdb_id] = [label_dict[annot] for annot in annotations_list]\n",
    "    return pdb_annot_dict\n",
    "    # print(\"Number of classes in task {} is {}\".format('EnzymeCommission', label_id))\n",
    "\n",
    "# def get_full_annotation(go_uniprot_dict):\n",
    "#     root_dir = './data/GeneOntology/nrPDB-GO_annot.tsv'\n",
    "#     with open(root_dir, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#     go_classes_molecular_functions = lines[1].strip().split('\\t')\n",
    "#     go_classes_biological_process = lines[5].strip().split('\\t')\n",
    "#     go_classes_cellular_component = lines[9].strip().split('\\t')\n",
    "#     for k, v in go_uniprot_dict.items():\n",
    "        \n",
    "\n",
    "def gen_go_labels(go_uniprot_dict):\n",
    "    root_dir = './data/GeneOntology/nrPDB-GO_annot.tsv'\n",
    "    root_dir = '/home/rhan21/sjm/GMSL/data/GeneOntology/nrPDB-GO_annot.tsv'\n",
    "    with open(root_dir, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    go_classes_molecular_functions = lines[1].strip().split('\\t')\n",
    "    go_classes_biological_process = lines[5].strip().split('\\t')\n",
    "    go_classes_cellular_component = lines[9].strip().split('\\t')\n",
    "    \n",
    "    label_dict = {'molecular_functions':{}, 'biological_process':{}, 'cellular_component':{}}\n",
    "    pdb_annot_dict = {}\n",
    "    label_id_molecular = 0\n",
    "    label_id_biological = 0\n",
    "    label_id_cellular = 0\n",
    "    full_ids = []\n",
    "    full_uniprot_dict = {}\n",
    "    for label in go_classes_molecular_functions:\n",
    "        if label not in label_dict['molecular_functions']:\n",
    "            label_dict['molecular_functions'][label] = label_id_molecular\n",
    "            label_id_molecular += 1\n",
    "    for label in go_classes_biological_process:\n",
    "        if label not in label_dict['biological_process']:\n",
    "            label_dict['biological_process'][label] = label_id_biological\n",
    "            label_id_biological += 1\n",
    "    for label in go_classes_cellular_component:\n",
    "        if label not in label_dict['cellular_component']:\n",
    "            label_dict['cellular_component'][label] = label_id_cellular\n",
    "            label_id_cellular += 1\n",
    "    for item in lines[13:]:\n",
    "        pdb_id, molecular, biological, cellular = item.split('\\t')\n",
    "        molecular_list  = molecular.strip().split(',')\n",
    "        biological_list = biological.strip().split(',')\n",
    "        cellular_list = cellular.strip().split(',')\n",
    "        # print(molecular_list)\n",
    "        # 列表中会包含一些空的信息\n",
    "        pdb_annot_dict[pdb_id] = {'molecular_functions':[label_dict['molecular_functions'][annot] for annot in molecular_list if annot != ''],\n",
    "                                  'biological_process':[label_dict['biological_process'][annot] for annot in biological_list if annot != ''],\n",
    "                                  'cellular_component':[label_dict['cellular_component'][annot] for annot in cellular_list if annot != '']}\n",
    "        if pdb_annot_dict[pdb_id]['molecular_functions'] != [] and pdb_annot_dict[pdb_id]['biological_process'] != [] and pdb_annot_dict[pdb_id]['cellular_component'] != []:\n",
    "            full_ids.append(pdb_id)\n",
    "    for k, v in go_uniprot_dict.items():\n",
    "        if v in full_ids:\n",
    "            full_uniprot_dict[k] = v\n",
    "    print(\"Number of classes in task {} is {}\".format('molecular_functions', label_id_molecular+1))\n",
    "    print(\"Number of classes in task {} is {}\".format('biological_process', label_id_biological+1))\n",
    "    print(\"Number of classes in task {} is {}\".format('cellular_component', label_id_cellular+1))\n",
    "    return pdb_annot_dict, full_uniprot_dict\n",
    "    # print(pdb_annot_dict)\n",
    "\n",
    "\n",
    "def gen_lba_labels():\n",
    "    root_dir = './data/PDBbind/refined-set/index/INDEX_general_PL_data.2020'\n",
    "    root_dir = '/home/rhan21/sjm/GMSL/data/PDBbind/refined-set/index/INDEX_general_PL_data.2020'\n",
    "    res = {}\n",
    "    with open(root_dir) as f:\n",
    "        for line in f:\n",
    "            if '#' in line:\n",
    "                continue\n",
    "            cont = line.strip().split()\n",
    "            if len(cont) < 5:\n",
    "                continue\n",
    "            code, pk = cont[0], cont[3]\n",
    "            res[code] = float(pk)\n",
    "    print(\"LBA:\", len(res))\n",
    "    return res\n",
    "\n",
    "\n",
    "def gen_ppi_labels():\n",
    "    root_dir = './data/protein_protein/pp_affinity.xlsx'\n",
    "    pp_info = pd.read_excel(root_dir, header=1)\n",
    "    pdb_codes = pp_info['PDB code']\n",
    "    res = {}\n",
    "    for i, code in enumerate(pdb_codes):\n",
    "        res[code] = pp_info['pKd pKi pIC50'][i]\n",
    "    # print(\"PPI:\", len(res))\n",
    "    return res\n",
    "\n",
    "\n",
    "def gen_label(pdb_ids, \n",
    "            ec_labels, go_labels, ppi_labels, lba_labels, \n",
    "            ec_uniprot_dict, go_uniprot_dict, \n",
    "            ec_info_dict, go_info_dict, pp_info_dict, pl_info_dict):\n",
    "    uniform_labels = {}\n",
    "    for pdb_id in pdb_ids:\n",
    "        if '-' in pdb_id:\n",
    "            if pdb_id in ec_labels:\n",
    "                uniprots = ec_info_dict[pdb_id]\n",
    "                single_ec_label = [ec_labels[pdb_id]]\n",
    "            else:\n",
    "                single_ec_label = [-1]\n",
    "            if pdb_id in go_labels:\n",
    "                uniprots = go_info_dict[pdb_id]\n",
    "                single_go_label = [go_labels[pdb_id]]\n",
    "            else:\n",
    "                single_go_label = [-1]\n",
    "        else:\n",
    "            single_ec_label = []\n",
    "            single_go_label = []\n",
    "            if pdb_id in pp_info_dict:\n",
    "                uniprots = pp_info_dict[pdb_id]\n",
    "            elif pdb_id in pl_info_dict:\n",
    "                uniprots = pl_info_dict[pdb_id]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            for uniprot_id in uniprots:\n",
    "                if uniprot_id in ec_uniprot_dict:\n",
    "                    ec_label = ec_labels[ec_uniprot_dict[uniprot_id]]\n",
    "                else:\n",
    "                    ec_label = -1\n",
    "                if uniprot_id in go_uniprot_dict:\n",
    "                    go_label = go_labels[go_uniprot_dict[uniprot_id]]\n",
    "                else:\n",
    "                    go_label = -1\n",
    "                single_ec_label.append(ec_label)\n",
    "                single_go_label.append(go_label)\n",
    "        if pdb_id in ppi_labels:\n",
    "            ppi_label = ppi_labels[pdb_id]\n",
    "        else:\n",
    "            ppi_label = -1\n",
    "        if pdb_id in lba_labels:\n",
    "            lba_label = lba_labels[pdb_id]\n",
    "        else:\n",
    "            lba_label = -1\n",
    "        uniform_labels[pdb_id] = {\"uniprots\":uniprots, \"ec\": single_ec_label, \"go\": single_go_label, \"ppi\": ppi_label, \"lba\": lba_label}\n",
    "    return uniform_labels\n",
    "    # print(uniform_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_lba_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ec():\n",
    "    root_dir = './output_info/'\n",
    "    json_files = ['enzyme_commission_uniprots.json']\n",
    "    json_dirs = [os.path.join(root_dir, json_file) for json_file in json_files]\n",
    "    # cal_complex_all(json_dirs)\n",
    "    ec_uniprot_dict, ec_info_dict = gen_protein_property_uniprots(json_dirs[0])\n",
    "    # go_uniprot_dict, go_info_dict = gen_protein_property_uniprots(json_dirs[1])\n",
    "    # pp_uniprot_dict, pp_info_dict = gen_protein_property_uniprots(json_dirs[2], single=False)\n",
    "    # pl_uniprot_dict, pl_info_dict = gen_protein_property_uniprots(json_dirs[3], single=False)\n",
    "    # print(\"pl, pp, ec, go:\", len(pl_info_dict), len(pp_info_dict), len(ec_info_dict), len(go_info_dict))\n",
    "    print(\"ec:\",len(ec_info_dict))\n",
    "    ec_labels = gen_ec_labels()\n",
    "\n",
    "    \n",
    "    full_test_list = test_list_pl[int(0.6*len(test_list_pl)):] + test_list_pp[int(0.6*len(test_list_pp)):]\n",
    "    full_val_list = test_list_pl[int(0.2*len(test_list_pl)): int(0.6*(len(test_list_pl)))] + test_list_pp[int(0.2*len(test_list_pp)): int(0.6*len(test_list_pp))]\n",
    "    full_train_list = test_list_pl[: int(0.2*len(test_list_pl))] + test_list_pp[: int(0.2*len(test_list_pp))]\n",
    "\n",
    "    train_list_ec = [ec_uniprot_dict[i] for i in ec_uniprot_dict if i not in test_uniprots_all]\n",
    "    train_list_go = [go_uniprot_dict[i] for i in go_uniprot_dict if i not in test_uniprots_all]\n",
    "    print(\"Train List:\", len(train_list_ec), len(train_list_go))\n",
    "    train_list_all = list(set(train_list_pl + train_list_pp + train_list_ec + train_list_go + full_train_list))\n",
    "    # print(train_list_all)\n",
    "    print(len(full_test_list), len(full_val_list), len(train_list_all))\n",
    "    print(\"save_info\")\n",
    "    save_dataset_info('./data/MultiTask/train_list_all.txt', train_list_all)\n",
    "    save_dataset_info('./data/MultiTask/full_train.txt', full_train_list)\n",
    "    save_dataset_info('./data/MultiTask/full_val.txt', full_val_list)\n",
    "    save_dataset_info('./data/MultiTask/full_test.txt', full_test_list)\n",
    "    \n",
    "    print(len(ec_info_dict), len(ec_uniprot_dict), len(ec_labels))\n",
    "    print(len(go_info_dict), len(go_uniprot_dict), len(go_labels))\n",
    "    uniformed_label_dict = gen_label(train_list_all+full_test_list+full_val_list, ec_labels, go_labels, ppi_labels, lba_labels, ec_uniprot_dict, go_uniprot_dict, ec_info_dict, go_info_dict, pp_info_dict, pl_info_dict)    \n",
    "    print(full_test_list[0], \": \", uniformed_label_dict[full_test_list[0]])\n",
    "    with open('./data/MultiTask/uniformed_labels.json', 'w') as f:\n",
    "        json.dump(uniformed_label_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_info('/home/rhan21/sjm/GMSL/data/MultiTask/train_list_all.txt', train_list_all)\n",
    "save_dataset_info('/home/rhan21/sjm/GMSL/data/MultiTask/full_train.txt', full_train_list)\n",
    "save_dataset_info('/home/rhan21/sjm/GMSL/data/MultiTask/full_val.txt', full_val_list)\n",
    "save_dataset_info('/home/rhan21/sjm/GMSL/data/MultiTask/full_test.txt', full_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ec_info_dict), len(ec_uniprot_dict), len(ec_labels))\n",
    "print(len(go_info_dict), len(go_uniprot_dict), len(go_labels))\n",
    "uniformed_label_dict = gen_label(train_list_all+full_test_list+full_val_list, ec_labels, go_labels, ppi_labels, lba_labels, ec_uniprot_dict, go_uniprot_dict, ec_info_dict, go_info_dict, pp_info_dict, pl_info_dict)    \n",
    "print(full_test_list[0], \": \", uniformed_label_dict[full_test_list[0]])\n",
    "with open('./data/MultiTask/uniformed_labels.json', 'w') as f:\n",
    "    json.dump(uniformed_label_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    # Multiprocess Setting to speedup dataloader\n",
    "    torch.multiprocessing.set_start_method('forkserver')\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    args = get_argparse()\n",
    "\n",
    "    \n",
    "\n",
    "def go_task(args):\n",
    "    device = args.device\n",
    "    # if device != \"cpu\":\n",
    "    #     if ',' not in device:\n",
    "    #         device = [int(device)]\n",
    "    #     else:\n",
    "    #         device = [int(i) for i in device.split(',')]\n",
    "    if args.wandb:\n",
    "        name = args.run_name + time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        wandb.init(project='gmsl', name=name)\n",
    "        wandb_logger = WandbLogger()\n",
    "    else:\n",
    "        wandb_logger = None\n",
    "    model = PropertyModel(\n",
    "            args=args,\n",
    "            sdim=args.sdim,\n",
    "            vdim=args.vdim,\n",
    "            depth=args.depth,\n",
    "            model_type=args.model_type,\n",
    "            learning_rate=args.learning_rate,\n",
    "            weight_decay=args.weight_decay,\n",
    "            r_cutoff=4.5,\n",
    "            num_radial=args.num_radial,\n",
    "            max_epochs=args.max_epochs,\n",
    "            factor_scheduler=0.75,\n",
    "            enhanced=args.enhanced,\n",
    "            offset_strategy = args.offset_strategy,\n",
    "            task=args.train_task\n",
    "        )\n",
    "    model_cls = PropertyModel\n",
    "    print(\n",
    "            f\"Model consists of {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable params.\"\n",
    "        )\n",
    "    model_dir = osp.join(MODEL_DIR, args.save_dir)\n",
    "    if not osp.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    run_results = []\n",
    "    seed = args.seed\n",
    "    for run in range(args.nruns):\n",
    "        pl.seed_everything(seed, workers=True)\n",
    "        seed += run\n",
    "        print(f\"Starting run {run} with seed {seed}\")\n",
    "        datamodule = LBADataLightning(\n",
    "            batch_size=args.batch_size,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=args.drop_last,\n",
    "            # sample_strategy = args.sample_strategy,\n",
    "            train_task=args.train_task,\n",
    "            train_split=args.train_split,\n",
    "            val_split=args.val_split,\n",
    "            test_split=args.test_split,\n",
    "            gearnet=True if args.model_type=='gearnet' else False,\n",
    "            alpha_only=args.alpha_only\n",
    "            # auxiliary=None\n",
    "        )\n",
    "        model = model_cls(\n",
    "            args=args,\n",
    "            sdim=args.sdim,\n",
    "            vdim=args.vdim,\n",
    "            depth=args.depth,\n",
    "            model_type=args.model_type,\n",
    "            learning_rate=args.learning_rate,\n",
    "            weight_decay=args.weight_decay,\n",
    "            r_cutoff=4.5,\n",
    "            num_radial=args.num_radial,\n",
    "            max_epochs=args.max_epochs,\n",
    "            factor_scheduler=0.75,\n",
    "            aggr=\"mean\",\n",
    "            use_norm=False,\n",
    "            enhanced=args.enhanced,\n",
    "            offset_strategy = args.offset_strategy,\n",
    "            task=args.train_task,\n",
    "            # readout=args.readout\n",
    "        )\n",
    "        # 根据不同任务设置选择最优模型的方法\n",
    "        monitor, mode = choose_monitor(args.train_task)\n",
    "        \n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor=monitor,\n",
    "            filename=\"model-{epoch:02d}-{val_eloss:.4f}\",\n",
    "            mode=mode,\n",
    "            save_last=False,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            devices=device if device != \"cpu\" else None,\n",
    "            accelerator=\"gpu\" if device != \"cpu\" else \"cpu\",\n",
    "            max_epochs=args.max_epochs,\n",
    "            precision=32,\n",
    "            # amp_backend=\"native\",\n",
    "            callbacks=[\n",
    "                checkpoint_callback,\n",
    "                LearningRateMonitor(),\n",
    "                ModelSummary(max_depth=2)\n",
    "            ],\n",
    "            default_root_dir=model_dir,\n",
    "            gradient_clip_val=args.gradient_clip_val,\n",
    "            accumulate_grad_batches=args.batch_accum_grad,\n",
    "            logger=wandb_logger,\n",
    "            strategy=DDPStrategy(find_unused_parameters=True)\n",
    "        )\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        trainer.fit(model, datamodule=datamodule, ckpt_path=args.load_ckpt)\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        time_diff = end_time - start_time\n",
    "        print(f\"Training time: {time_diff}\")\n",
    "\n",
    "        # running test set\n",
    "        _ = trainer.test(ckpt_path=\"best\", datamodule=datamodule)\n",
    "        res = model.res\n",
    "        run_results.append(res)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"res.json\"), \"w\") as f:\n",
    "        json.dump(run_results, f)\n",
    "\n",
    "    # aggregate over runs..\n",
    "    results_df = pd.DataFrame(run_results)\n",
    "    print(results_df.describe())\n",
    "    # for result in run_results:\n",
    "    #     wandb_logger.log_metrics(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
